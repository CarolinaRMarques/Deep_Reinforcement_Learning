{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNOptmiza.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMlpaG2dh7Ww",
        "outputId": "d50d4126-e3a0-4fcd-e5eb-6b2713f32bcb"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!pip install atari_py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLRxdl2ch8w4",
        "outputId": "09dd933a-fc15-41a1-c9ac-5d386d5dbd7f"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VGHCtJFgRax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cceada9b-107c-4452-ce57-f72f2ca2f335"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\n",
        "#https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import skimage as skimage\n",
        "from skimage import transform, color, exposure\n",
        "from skimage.transform import rotate\n",
        "from skimage.viewer import ImageViewer\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import json\n",
        "from tensorflow.keras.initializers import identity\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD , Adam\n",
        "import tensorflow as tf\n",
        "#import agent\n",
        "# Import the gym module\n",
        "import gym\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Viewer requires Qt\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBORqwFUgWzs",
        "outputId": "4b7d3cf2-3114-4ef1-e094-3c0e38d6d95e"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f9ebbd97a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1hqnsP3c8uA",
        "outputId": "d4dbe62c-43ca-4748-8825-46cb7ac92289"
      },
      "source": [
        "!python -m atari_py.import_roms '/content/drive/My Drive/Comp_Natural/ROMS'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying mr_do.bin from /content/drive/My Drive/Comp_Natural/ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying lost_luggage.bin from /content/drive/My Drive/Comp_Natural/ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying elevator_action.bin from /content/drive/My Drive/Comp_Natural/ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying asterix.bin from /content/drive/My Drive/Comp_Natural/ROMS/Asterix (AKA Taz) (1984) (Atari, Jerome Domurat, Steve Woita) (CX2696).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying riverraid.bin from /content/drive/My Drive/Comp_Natural/ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying video_pinball.bin from /content/drive/My Drive/Comp_Natural/ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying road_runner.bin from patched version of /content/drive/My Drive/Comp_Natural/ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying qbert.bin from /content/drive/My Drive/Comp_Natural/ROMS/Q. Bert (1983) (CCE) (C-822).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying surround.bin from /content/drive/My Drive/Comp_Natural/ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying ms_pacman.bin from /content/drive/My Drive/Comp_Natural/ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying up_n_down.bin from /content/drive/My Drive/Comp_Natural/ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying kung_fu_master.bin from /content/drive/My Drive/Comp_Natural/ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying atlantis.bin from /content/drive/My Drive/Comp_Natural/ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying pitfall.bin from /content/drive/My Drive/Comp_Natural/ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying chopper_command.bin from /content/drive/My Drive/Comp_Natural/ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying ice_hockey.bin from /content/drive/My Drive/Comp_Natural/ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying breakout.bin from /content/drive/My Drive/Comp_Natural/ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying boxing.bin from /content/drive/My Drive/Comp_Natural/ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying freeway.bin from /content/drive/My Drive/Comp_Natural/ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying kaboom.bin from /content/drive/My Drive/Comp_Natural/ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying skiing.bin from /content/drive/My Drive/Comp_Natural/ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying tennis.bin from /content/drive/My Drive/Comp_Natural/ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying bank_heist.bin from /content/drive/My Drive/Comp_Natural/ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying demon_attack.bin from /content/drive/My Drive/Comp_Natural/ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying koolaid.bin from /content/drive/My Drive/Comp_Natural/ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying star_gunner.bin from /content/drive/My Drive/Comp_Natural/ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying trondead.bin from /content/drive/My Drive/Comp_Natural/ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying robotank.bin from /content/drive/My Drive/Comp_Natural/ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying space_invaders.bin from /content/drive/My Drive/Comp_Natural/ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying hero.bin from /content/drive/My Drive/Comp_Natural/ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying montezuma_revenge.bin from /content/drive/My Drive/Comp_Natural/ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying assault.bin from /content/drive/My Drive/Comp_Natural/ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying air_raid.bin from /content/drive/My Drive/Comp_Natural/ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from /content/drive/My Drive/Comp_Natural/ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from /content/drive/My Drive/Comp_Natural/ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying asteroids.bin from /content/drive/My Drive/Comp_Natural/ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying beam_rider.bin from /content/drive/My Drive/Comp_Natural/ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from /content/drive/My Drive/Comp_Natural/ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from /content/drive/My Drive/Comp_Natural/ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying battle_zone.bin from /content/drive/My Drive/Comp_Natural/ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying carnival.bin from /content/drive/My Drive/Comp_Natural/ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying crazy_climber.bin from /content/drive/My Drive/Comp_Natural/ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying centipede.bin from /content/drive/My Drive/Comp_Natural/ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying defender.bin from /content/drive/My Drive/Comp_Natural/ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying double_dunk.bin from /content/drive/My Drive/Comp_Natural/ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying donkey_kong.bin from /content/drive/My Drive/Comp_Natural/ROMS/Donkey Kong (1987) (Atari) (CX26143).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying frostbite.bin from /content/drive/My Drive/Comp_Natural/ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying fishing_derby.bin from /content/drive/My Drive/Comp_Natural/ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying frogger.bin from /content/drive/My Drive/Comp_Natural/ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying enduro.bin from /content/drive/My Drive/Comp_Natural/ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying gopher.bin from /content/drive/My Drive/Comp_Natural/ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from /content/drive/My Drive/Comp_Natural/ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying galaxian.bin from /content/drive/My Drive/Comp_Natural/ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying krull.bin from /content/drive/My Drive/Comp_Natural/ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying journey_escape.bin from /content/drive/My Drive/Comp_Natural/ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kangaroo.bin from /content/drive/My Drive/Comp_Natural/ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying name_this_game.bin from /content/drive/My Drive/Comp_Natural/ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pooyan.bin from /content/drive/My Drive/Comp_Natural/ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying phoenix.bin from /content/drive/My Drive/Comp_Natural/ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying private_eye.bin from /content/drive/My Drive/Comp_Natural/ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying solaris.bin from /content/drive/My Drive/Comp_Natural/ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying seaquest.bin from /content/drive/My Drive/Comp_Natural/ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying venture.bin from /content/drive/My Drive/Comp_Natural/ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying zaxxon.bin from /content/drive/My Drive/Comp_Natural/ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying wizard_of_wor.bin from /content/drive/My Drive/Comp_Natural/ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying pong.bin from /content/drive/My Drive/Comp_Natural/ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying yars_revenge.bin from /content/drive/My Drive/Comp_Natural/ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying time_pilot.bin from /content/drive/My Drive/Comp_Natural/ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying tutankham.bin from /content/drive/My Drive/Comp_Natural/ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying jamesbond.bin from /content/drive/My Drive/Comp_Natural/ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying adventure.bin from /content/drive/My Drive/Comp_Natural/ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying pacman.bin from /content/drive/My Drive/Comp_Natural/ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying keystone_kapers.bin from /content/drive/My Drive/Comp_Natural/ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from /content/drive/My Drive/Comp_Natural/ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from /content/drive/My Drive/Comp_Natural/ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying sir_lancelot.bin from /content/drive/My Drive/Comp_Natural/ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T2jR9Ukwf5km",
        "outputId": "02528db2-ff21-4db5-85f5-7fc712dfc490"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "GAME = 'atari' # the name of the game being played for log files\n",
        "CONFIG = 'nothreshold'\n",
        "ACTIONS = 3 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVATION = 200. # timesteps to observe before training. de cada 3200 frames, vamos ao nosso buffer e selecionamos de forma aleatoria um batch size. Neste caso, 32 frames. em numpy arrays\n",
        "EXPLORE = 3000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.01 # starting value of epsilon EPSILON é para ver o exploration vs exploitation\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "LEARNING_RATE = 1e-3\n",
        "#MAX_STEPS_PER_EPISODE = 1000\n",
        "EPISODES = 10000\n",
        "q_max_list = []\n",
        "loss_list = []\n",
        "reward_list = []\n",
        "accuracy_list=[]\n",
        "img_rows, img_cols = 84, 84\n",
        "#Convert image into Black and white\n",
        "img_channels = 4 #We stack 4 frames\n",
        "\n",
        "def buildmodel():\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32,kernel_size=(8,8),strides=(2, 2),input_shape=(84,84,4), activation='elu', padding='valid'))\n",
        "    model.add(BatchNormalization(axis=1, epsilon=1e-5))\n",
        "    model.add(Conv2D(64,kernel_size=(4,4), strides=(2, 2), activation='elu', padding='valid'))\n",
        "    model.add(BatchNormalization(axis=1, epsilon=1e-5))\n",
        "    model.add(Conv2D(128,kernel_size=(4,4), strides=(1, 1), activation='elu', padding='valid'))\n",
        "    model.add(BatchNormalization(axis=1, epsilon=1e-5))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='elu'))\n",
        "    model.add(Dense(ACTIONS))\n",
        "\n",
        "    opt = Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(opt,'mean_squared_error',['accuracy'])\n",
        "\n",
        "    print(\"We finish building the model\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "def reshape(x_t):\n",
        "   x_t = skimage.color.rgb2gray(x_t)\n",
        "   x_t = skimage.transform.resize(x_t, (84,84))\n",
        "   x_t = skimage.exposure.rescale_intensity(x_t, out_range = (0,255))\n",
        "\n",
        "   x_t = x_t / 255.0\n",
        "   return x_t \n",
        "\n",
        "def writeFiles(q_max, loss, reward):\n",
        "    with open(\"/content/drive/My Drive/Comp_Natural/q_max.txt\", \"a\") as f_q_max:\n",
        "      for e in q_max:\n",
        "        f_q_max.write(str(e) + \"\\n\")\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Comp_Natural/loss.txt\", \"a\") as f_loss:\n",
        "      for e in loss:\n",
        "        f_loss.write(str(e) + \"\\n\")\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Comp_Natural/reward.txt\", \"a\") as f_rewards:\n",
        "      for e in reward:\n",
        "        f_rewards.write(str(e) + \"\\n\")\n",
        "\n",
        "def trainNetwork(model,args):\n",
        "    # open up a game state to communicate with emulator\n",
        "    env = wrap_env(gym.make('BreakoutDeterministic-v4'))\n",
        "    env.reset()\n",
        "    # store the previous observations in replay memory\n",
        "    \n",
        "    #----------------------------------------\n",
        "    #PARA OBTER O SIGNIFICADO DAS AÇÕES POSSíVEIS\n",
        "    #print(env.unwrapped.get_action_meanings())\n",
        "    #----------------------------------------\n",
        "    \n",
        "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        \n",
        "    x_t, r_0, terminal, info = env.step(1) #COMEÇAR O JOGO COM A AÇÃO \"FIRE\"\n",
        "    \n",
        "    env.render()\n",
        "\n",
        "    D = args['D']\n",
        "\n",
        "    x_t = reshape(x_t)\n",
        "\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2) #colocar a sequência de frames. 4 frames sequenciais, que vamos aplicar à nossa lista. Para conseguir a estabilidade de imagens sequenciais\n",
        "    \n",
        "    #print (s_t.shape)\n",
        "\n",
        "    #In Keras, need to reshape\n",
        "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
        "\n",
        "    t = args['t']\n",
        "\n",
        "    if args['mode'] == 'Run':\n",
        "        OBSERVE = 999999999\t#We keep observe, never train\n",
        "        epsilon = FINAL_EPSILON # higher epsilon, more timestamps?\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/My Drive/Comp_Natural/models/model.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "\n",
        "    elif args['mode'] == 'CTrain': #Continue previous train\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = 0.07823368810419994 #0.08811709480229288\n",
        "        epsilon = args['epsilon']\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/MyDrive/Comp_Natural/models/model.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        \n",
        "\n",
        "    else:\t\t\t\t\t   #We go to training mode -> -m \"Train\"\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = INITIAL_EPSILON #o EPSILON é o que divide a parte de exploration vs exploitation. se for abaixo de um dado valor é exploration. Caso contrário é exploitation\n",
        "        epsilon = args['epsilon']\n",
        "\n",
        "\n",
        "    lives = 5\n",
        "    r_total = 0\n",
        "    while (lives > 0):\n",
        "    #for i in range(MAX_STEPS_PER_EPISODE):\n",
        "        loss = 0.0\n",
        "        Q_sa = 0 # Q(s, a) representing the maximum discounted future reward when we perform action a in state s.\n",
        "        action_index = 0\n",
        "        r_t = 0 #reward\n",
        "        a_t = np.zeros([ACTIONS]) #action\n",
        "      \n",
        "        #choose an action epsilon greedy\n",
        "        if t % FRAME_PER_ACTION == 0:\n",
        "            if random.random() <= epsilon:\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
        "                a_t[action_index] = 1\n",
        "                \n",
        "            else:\n",
        "                q = model.predict(s_t)\t   #input a stack of 4 images, get the prediction\n",
        "                max_Q = np.argmax(q)\n",
        "                action_index = max_Q\n",
        "                a_t[max_Q] = 1\n",
        "\n",
        "        #We reduce the epsilon gradually\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        #run the selected action and observed next state and reward. DEPOIS DE UM \"STEP\" correr sempre o \"RENDER\"\n",
        "        x_t1_colored, r_t, terminal, info = env.step(list(a_t).index(1) + 1) #FUNÇÂO \"WHERE\" para obter o índice do valor do array que está a 1\n",
        "\n",
        "        if info['ale.lives'] < lives:\n",
        "          lives -= 1\n",
        "          r_t = -1.0\n",
        "          if info['ale.lives'] > 0:\n",
        "            env.step(1)\n",
        "\n",
        "        env.render()\n",
        "        \n",
        "        x_t1 = reshape(x_t1_colored)\n",
        "\n",
        "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis = 3)\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        #only train if done observing\n",
        "        if t > OBSERVE: #train ou update da nossa rede. de quantas em quantas frames vamos precisar para fazer um treino. se replay_mem começar a ficar mto cheio retira a última entrada. e fazemos append das novas decisoes que foram sendo adquiridas\n",
        "            #sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "\n",
        "            #Now we do the experience replay\n",
        "            state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "            state_t = np.concatenate(state_t)\n",
        "            state_t1 = np.concatenate(state_t1)\n",
        "            targets = model.predict(state_t)\n",
        "            Q_sa = model.predict(state_t1)\n",
        "            targets[range(BATCH), action_t] = reward_t + GAMMA * np.max(Q_sa, axis = 1) * np.invert(terminal) #qual o target associado\n",
        "            \n",
        "            lossAcc = model.train_on_batch(state_t, targets, return_dict=True)\n",
        "            #print(lossAcc)\n",
        "            loss += lossAcc['loss'] #quanto mais proximo de zero, mais proximo está de convergir para conseguir estimar o key value de acordo com o par (estado, ação)\n",
        "            #accuracy_list.append(lossAcc['accuracy'])\n",
        "        s_t = s_t1\n",
        "        t = t + 1\n",
        "\n",
        "        # save progress every 1000 iterations\n",
        "        if t % 100 == 0:\n",
        "            print(\"Now we save model\")\n",
        "            model.save_weights(\"/content/drive/My Drive/Comp_Natural/models/model_save3.h5\", overwrite = True)\n",
        "            with open(\"model.json\", \"w\") as outfile:\n",
        "                json.dump(model.to_json(), outfile)\n",
        "\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
        "\n",
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
        "        \n",
        "        q_max_list.append(np.max(Q_sa))\n",
        "        loss_list.append(loss)\n",
        "        reward_list.append(r_t)\n",
        "\n",
        "    env.close()\n",
        "    writeFiles(q_max_list, loss_list, reward_list)\n",
        "    \n",
        "        \n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")\n",
        "    return t, epsilon, D\n",
        "\n",
        "\n",
        "def playGame(args):\n",
        "    model = buildmodel()\n",
        "    t, epsilon, D = trainNetwork(model,args)\n",
        "    return t, epsilon, D\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description = 'Description of your program')\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True)\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True) adicionar o argumento de número de episódios\n",
        "    #args = vars(parser.parse_args())\n",
        "    t = 0\n",
        "    epsilon = INITIAL_EPSILON\n",
        "    D = deque()\n",
        "    for i in range(EPISODES):\n",
        "        print(\"EPISODE\", i)\n",
        "        tp, epsilonp, Dp = playGame({'mode': 'Run', 't': t, 'epsilon': epsilon, 'D': D})\n",
        "        t = tp\n",
        "        epsilon = epsilonp\n",
        "        D = Dp\n",
        "        show_video()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPISODE 0\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "Now we save model\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "Episode finished!\n",
            "************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "EPISODE 1\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "Now we save model\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "Episode finished!\n",
            "************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAFdVtZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiByMjg1NCBlOWE1OTAzIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxNyAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAACLWWIhAA3//728P4FNlYEUGa7Q91nCgDAQZ/NTMClgclA4pYaytdZh+dVJuV432kRnWAs5rNIwVgDq82rBm7oYytwVrbMx0s0deIjDqA1k01uG4ANrMfO1grOJwZga71GmymuARhsR4WZ3pZaGRcKmQ1AKWNgP+g0jF9vZyFT5gpot86JTbMFc0ZcdazcAe48r5fEfvVute9rB6tUebo7LSDmgk0Yk5SccnTIC7OKXa0z9mVrKEExQtVw/h9gGhuwMIFSbYTBIps6sOwginkaQahoy3YpigJeHW1QeapzVRURgWt3Cb/fABnsQJUbQBnzSOFXARJncJs9Egfvxb8tPmLx1saqciKlnYZZ3TBjw5BDKy2pLMIQlUrsVKYTYX9hV+lrvVOAfbYkUg1VgiZyygVNBBkK0cr2V9Z9KA4o9WtvG9L3J2bT1I7ByLycHnVVT6CxO8EXVlpG+CGjAAHXgWgnJa5SvEPAl+X3OxVbIlNDcTwrogczuBZURzjl5h0c4FURXOPiNNVOPPG/bCf/strrjbzgPY1XmyXmYzyC5bU2zceJnN1K2jAD8690RebAtZ79NOlYCQvs/VKQExLGBYxwouKCYBjis12qtLY2AT/HN3jK17mglHsIXZkPWFSQt47PRVulw4V62yqmptUQ1ea/VL/UXPjKmV/cry7Iu924lflcd/cssrSob+Jq/jG+05pSDNtLLQskkvITc/L2PQiDXCU1TNmSQ+F/S7FxAAAAY0GaI2xDf/6nhAMXU3SAWm6Heet150NvaiNsGfQST5TYE6TrkGEFX5O+3WEQrvdPfgNUuC5EMfCkFFYdXSJmMGliF3CLqEIlHM+bzmxQ9d+fL17d9hYfYRgYAABNP+WZnZGygAAAADJBnkF4hX8BsTrNvqwzjxAEEOLZ75yfp8SXFkiuggrf/yPX87ETQVDZYREdzSeJlhmSDwAAABMBnmJqQn8BLYjGE5xpIQ9dFfuAAAAAREGaZ0moQWiZTAhv//6nhALt2FfchsXOqO6n9U87tnhl6/wm+p+zUcwAJ1rWa9J1mLq+hWjSw9DwftD8cNCeZDmfTrSxAAAAOUGehUURLCv/Aacf7rYnRYO8nrbP0ANeWtnvnJ+nxJcWSK6CCt/+wjloiaEJIiP/KyQbYAQAJ57zgQAAABkBnqR0Qn8CGNJ3jP2dQeiSvuk4njHYSIp5AAAAEwGepmpCfwDtg0qbAoh66baMlIEAAAA7QZqrSahBbJlMCG///qeEALCIZi+dzh09Ek/zsgBOtazXpOsxdX0K0aWHoeD9ofjhoTzIcz6YdW74FXQAAAAyQZ7JRRUsK/8Akz5AAEKOLZ75yfp8SXFkiuggrf/0rwge2RNB/LnKFyskG2AEBSO9P1EAAAAUAZ7odEJ/AL87oRmrd4D0GZOqClcAAAARAZ7qakJ/AL8Jkjts2eXTXcsAAAA7QZrvSahBbJlMCG///qeEAIqtpk4VBzNe6e/EcIATrWs16TrMXV9CtGlh6Hg/aH44aE8yHM+mHVu+Bc0AAAAyQZ8NRRUsK/8AdFEsACFHFs985P0+JLiyRXQQVv/6V4QPbImg/lzlC5WSDbACApHen98AAAAVAZ8sdEJ/AJbu7p1MI0UShXK1BVOBAAAAEQGfLmpCfwCWxGI7bNnl014bAAAAGUGbM0moQWyZTAhv//6nhABucDMZRvt4REYAAAAnQZ9RRRUsK/8AXXxIAAHLNVekF8VB7WcZnubWbl9SzN/oOgb7Q91AAAAAHAGfcHRCfwB27H5gANiwl6jPjWkCb3gaxzoQbVUAAAAZAZ9yakJ/AHbLVx6G5xEAFf6FjSg317ekwAAAABJBm3dJqEFsmUwIb//+p4QANSAAAAALQZ+VRRUsK/8AK2EAAAAJAZ+0dEJ/ADegAAAACQGftmpCfwA3oQAAAENBm7tJqEFsmUwIb//+krFQADoQv/8IPVAkzovLY4AmJubJizQLvwbR2MALTxpYD3bc0Ar2bsnFIOxVsWdvufmyQMQ/AAAAIEGf2UUVLCv/OVXvrpNhYAblkvvLxwBhXB+xVk06d0eAAAAADgGf+HRCf0AZ4fQ8Vzv9AAAAGwGf+mpCfwEliuRPzIARTUfTQ2YpTNAWaQNfhAAAACdBm/9JqEFsmUwIb//+p4QA1/sqSQEQQwAhNmXwxxwDX7ivzZIF3/EAAAAfQZ4dRRUsK/8AsTSxTEAHFSr6KGzBkRzeYvoFAuLaIQAAABwBnjx0Qn8A4nFbgA4EE9+eOBj5UqqB+VV5/KyoAAAAGwGePmpCfwDdCsV++0AQ7UfTQ2YpTNAWaQNhJAAAACdBmiNJqEFsmUwIb//+p4QAo/vCmivrlwAbR1H3A44Br9xX5skC81kAAAAfQZ5BRRUsK/8AgsixTEAHFSr6KGzBkRzeYvoFAuLoYAAAABwBnmB0Qn8ArKW1wAcCCe/PHAx8qVVA/Kq8/lftAAAAGwGeYmpCfwCoM3In5kAIpqPpobMUpmgLNIGxnAAAACdBmmdJqEFsmUwIb//+p4QAef2VJICIIYAQmzL4Y44Br9xX5skC+fEAAAAfQZ6FRRUsK/8AZIg2piADipV9FDZgyI5vMX0CgXF/IQAAABwBnqR0Qn8AfvitwAcCCe/PHAx8qVVA/Kq8/lnpAAAAGwGepmpCfwB8QmzzZkAIpqPpobMUpmgLNIGzHQAAABVBmqtJqEFsmUwIb//+p4QAXX3hTGkAAAAZQZ7JRRUsK/8AS2RYpiAB2N775GDkeluYLAAAABsBnuh0Qn8AYf4q4AOBBPfnjgY+VKqgflVb/WEAAAAMAZ7qakJ/AF+EFeyvAAAAEkGa70moQWyZTAhn//6eEADPgAAAAAtBnw1FFSwr/wArYQAAAAkBnyx0Qn8AN6EAAAAJAZ8uakJ/ADehAAAAR0GbMkmoQWyZTAhn//6B+W+K0AWrh//4d30CVVo9Pzqpg0nveLkFBerYLLlEAsU8y/HKOqWU5ehWQsf9lT6KUJvak5104SnQAAAAEUGfUEUVLCv/Oarge6L9SgDAAAAAEAGfcWpCfz9F01YAB/H1EDcAAAA3QZtzSahBbJlMCGf//p4QA+HCdJehDPQA2uPtycbw+YV85uC4x1uZG3Uq+3NDKw8GRRsmbryPgAAAABZBm5RJ4QpSZTAhn/6eEAMKuvqeTraAAAAALEGbt0nhDomUwIZ//p4QAw/wePoMEDACz4+3JxvD5hXzm4LjHW5kbdSr7eNtAAAAF0Gf1UURPCv/ANLJlGl65k4XxGW2+e1AAAAAEwGf9mpCfwEN3HNZXpOjDI5bLusAAAAZQZv4SahBaJlMCG///qeEAM37KjIMtqykjQAAAC1BmhtJ4QpSZTAhn/6eEAJt8WNoFn/IAcDEw5ON4fMK+c3BcY63MjbqVfbx44AAAAAXQZ45RTRMK/8A0smRrS9cxc213qZVcoEAAAATAZ5aakJ/AQ3cXWsr0ljuSAK1jgAAABlBmlxJqEFomUwIb//+p4QAo/vCiLWi7oYZAAAALUGaf0nhClJlMCG//qeEAHy9lSWgjHAASni7t6vKPmdQhaBfcdblzwrI116NYQAAABVBnp1FNEwr/wDSyZCUtsJeGsjDqsAAAAARAZ6+akJ/AQ3cVKWV6S2LJxkAAAAZQZqgSahBaJlMCG///qeEAH99lRkGW1ZUNQAAABRBmsRJ4QpSZTAhv/6nhABk/ZUjaQAAAChBnuJFNEwr/wDSyY/q+PAHJicgtB2RoTMH5GRQNPp41/YSWATEjaIhAAAAEgGfAXRCfwENaqhbuYeD8aThGAAAAA4BnwNqQn8BDdxPQjpWVQAAABJBmwhJqEFomUwIb//+p4QANSEAAAANQZ8mRREsK/8A0thunwAAAAsBn0V0Qn8BDWqcGQAAAAsBn0dqQn8BDdxE+AAAAEtBm0xJqEFsmUwIb//+krS9wBxTx/+ww8a2VtaNL9QLEw6Jw8u/7xwfsABkT423NAMG4xHrK0fXouiBjUobTdVPC1p7WFkNCOoX8uAAAAAuQZ9qRRUsK/85Ve+zcpFgCJjMOODUY1/jG5VwcBiFgdPXnvI/BzJnm6WoN8pdwwAAACYBn4l0Qn8+7imzVjM0AQhvsqVIxuNhzqMfB06sjZP33/W6jYFr4AAAACUBn4tqQn8BRmeVLMwl3AEIb7KlSMbjYc6jHwdOq//+DzBDs4pRAAAAM0GbkEmoQWyZTAhv//6nhADteoyrCPL9EQARB5hc4pa0CyZr5rgKW3TWtPawshoR1DINgQAAAC1Bn65FFSwr/wDS2H7qPbQgCJjMOODUY1/jG5VwcAHR0uXjC+DmTPN0tQbpSGUAAAAlAZ/NdEJ/AQ1rObLVbwAbSfZUqRjcbDnUY+Dp1ZGyfvv+ommRIQAAACUBn89qQn8BDdy0SiQJdwBCG+ypUjG42HOox8HTqv//g8wQbNXeAAAAMkGb1EmoQWyZTAhv//6nhAC1+8JbweX6IgAiDzC5xS1oFkzXzXAUtumtae1hZDQjqGSFAAAALkGf8kUVLCv/ANLYcoIbdbwAjIzDjg1GNf4xuVcG/RjsfnvI/BzJnm6WoN5ptWEAAAAlAZ4RdEJ/AQ1qwNXxVvABtJ9lSpGNxsOdRj4OnVkbJ++/5zG2KAAAACUBnhNqQn8BDdxiHLKhO4AhDfZUqRjcbDnUY+Dp1X//weYINnB+AAAAMkGaGEmoQWyZTAhv//6nhACHfI1bweX6IgAiDzC5xS1oFkzXzXAUtumtae1hZDQjqGUfAAAALkGeNkUVLCv/ANLYcMLAzaEARMZhxwajGv8Y3KuDfv46XLxhfBzJnm6WoN5qEEAAAAAlAZ5VdEJ/AQ1qsCkPq3gA2k+ypUjG42HOox8HTqyNk/ff9BPfSQAAACYBnldqQn8BDdxVTm8QncAQhvsqVIxuNhzqMfB06r//4PMEBsMHgQAAABdBmlxJqEFsmUwIb//+p4QAZ32U/BevjwAAACxBnnpFFSwr/wDS2G/oht1vACMjMOODUY1/jG5Vwb+t43LxhfBzJnm6WoPccQAAACUBnpl0Qn8BDWqoXXxVvABtJ9lSpGNxsOdRj4OnVkbJ++/6H/GcAAAAGQGem2pCfwEN3E6yOu44AW1KoyYAylZht10AAAASQZqASahBbJlMCG///qeEADUhAAAADUGevkUVLCv/ANLYbp8AAAALAZ7ddEJ/AQ1qnBgAAAALAZ7fakJ/AQ3cRPkAAABKQZrESahBbJlMCG///ppQcOg20IA5mjH/7DDxrZW10mpfqBnOH/7bmgFpc5f4P4VYHveSvgMIQaLv5xTw0B8MRfJvpIWl9aKCJhUAAAA0QZ7iRRUsK/85Ve+vrjwAEKOLZ75yfp8SXFkiuggrgACvKHafpa9PAg7KtER3NJvRMLE9KQAAABMBnwF0Qn9AGeH2O9fQosM2cFjAAAAAFQGfA2pCfwEtiMcpjJntz+akqt7FoQAAADpBmwhJqEFsmUwIb//+p4QA2+BmMo5OsL3T34jhACda1mvSdZi6voVo0sPQ8H7Q/HDQnmQ5nyvEB9iRAAAANUGfJkUVLCv/ANLYeuAcACFHFs985P0+JLiyRXQQVwAgAiaWKomhADLwucrEmgVAJGOQzQTxAAAAFgGfRXRCfwENawy0jZyEC7JYIWNmmoEAAAAVAZ9HakJ/AQ3cnI6sBhwNnkp/0RobAAAAOkGbTEmoQWyZTAhv//6nhACwiGYvnc4dPRJP87IATrWs16TrMXV9CtGlh6Hg/aH44aE8l5TPleID8HAAAAA3QZ9qRRUsK/8A0th5ojoxNuQBBDi2e+cn6fElxZIroIK3//ROYGPImg/oTlC5WSDbACAlkOygSQAAABgBn4l0Qn8BDWsOab9lCM70Qfws0uI1bEAAAAAVAZ+LakJ/AQ3cpUtaeiHJ+IfPPA/AAAAAOkGbkEmoQWyZTAhv//6nhACKraZOFQczXunvxHCAE61rNek6zF1fQrRpYeh4P2h+OGhPJeUz5XiA/Z0AAAA3QZ+uRRUsK/8A0th6u50736gCCHFs985P0+JLiyRXQQVv/4Jg3uo3Cc5XOULlZINsAICI6QwRgQAAABgBn810Qn8BDWsOaX7soRmwXhbCZlpGvrEAAAAVAZ/PakJ/AQ3cpUsm7XM3PxD571vwAAAAGUGb1EmoQWyZTAhn//6eEAGvt5vjWwn2GzAAAAArQZ/yRRUsK/8A0th6u5yt94YAA5Zqr0gvioPazADN1X9y+pZm/0HQN3tnhQAAACABnhF0Qn8BDWsOaWoY8TAAhunmW4iCo+NyWh9uNzNcoAAAABwBnhNqQn8BDdylSxWIcz1nHgIAK/0LGlBvr271AAAAEkGaGEmoQWyZTAhf//6MsADQgQAAABBBnjZFFSwr/wDS2Hq7nB7AAAAADQGeVXRCfwENaw5pVDEAAAANAZ5XakJ/AQ3cpUsD2QAAABJBmlpJqEFsmUwUTCf//fEAB6QAAAANAZ55akJ/AQ4Xv8NvawAACKdtb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAAQBAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAH0XRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAQBAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAoAAAANIAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAEAQAAAQAAAEAAAAAB0ltZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADwAAAD2AFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAb0bWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAGtHN0YmwAAACYc3RzZAAAAAAAAAABAAAAiGF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAoADSAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAyYXZjQwFkAAz/4QAZZ2QADKzZQod+IhAAAAMAEAAAAwPA8UKZYAEABmjr48siwAAAABhzdHRzAAAAAAAAAAEAAAB7AAACAAAAABRzdHNzAAAAAAAAAAEAAAABAAADuGN0dHMAAAAAAAAAdQAAAAEAAAQAAAAAAQAACAAAAAACAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAgAABAAAAAABAAAIAAAAAAIAAAIAAAAAAQAABAAAAAABAAAIAAAAAAIAAAIAAAAAAQAABAAAAAABAAAIAAAAAAIAAAIAAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAABxzdHNjAAAAAAAAAAEAAAABAAAAewAAAAEAAAIAc3RzegAAAAAAAAAAAAAAewAABOMAAABnAAAANgAAABcAAABIAAAAPQAAAB0AAAAXAAAAPwAAADYAAAAYAAAAFQAAAD8AAAA2AAAAGQAAABUAAAAdAAAAKwAAACAAAAAdAAAAFgAAAA8AAAANAAAADQAAAEcAAAAkAAAAEgAAAB8AAAArAAAAIwAAACAAAAAfAAAAKwAAACMAAAAgAAAAHwAAACsAAAAjAAAAIAAAAB8AAAAZAAAAHQAAAB8AAAAQAAAAFgAAAA8AAAANAAAADQAAAEsAAAAVAAAAFAAAADsAAAAaAAAAMAAAABsAAAAXAAAAHQAAADEAAAAbAAAAFwAAAB0AAAAxAAAAGQAAABUAAAAdAAAAGAAAACwAAAAWAAAAEgAAABYAAAARAAAADwAAAA8AAABPAAAAMgAAACoAAAApAAAANwAAADEAAAApAAAAKQAAADYAAAAyAAAAKQAAACkAAAA2AAAAMgAAACkAAAAqAAAAGwAAADAAAAApAAAAHQAAABYAAAARAAAADwAAAA8AAABOAAAAOAAAABcAAAAZAAAAPgAAADkAAAAaAAAAGQAAAD4AAAA7AAAAHAAAABkAAAA+AAAAOwAAABwAAAAZAAAAHQAAAC8AAAAkAAAAIAAAABYAAAAUAAAAEQAAABEAAAAWAAAAEQAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1Ny44My4xMDA=\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "EPISODE 2\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 240 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 241 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 242 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 243 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 244 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 245 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 246 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 247 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 248 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 249 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 250 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 251 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 252 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 253 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 254 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 255 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 256 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 257 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 258 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 259 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 260 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 261 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 262 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 263 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 264 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 265 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 266 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 267 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 268 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 269 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 270 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 271 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 272 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 273 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 274 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 275 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 276 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 277 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 278 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 279 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 280 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 281 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 282 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 283 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 284 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 285 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 286 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 287 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 288 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 289 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 290 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 291 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 292 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 293 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 294 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 295 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 296 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 297 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 298 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 299 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "Now we save model\n",
            "TIMESTEP 300 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 301 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 302 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 303 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 304 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 305 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 306 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 307 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 308 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 309 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 310 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 311 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 312 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 313 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 314 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 315 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 316 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 317 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 318 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 319 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 320 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 321 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 322 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 323 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 324 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 325 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 326 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 327 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 328 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 329 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 330 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 331 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 332 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 333 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 334 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 335 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 336 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 337 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 338 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 339 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 340 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 341 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 342 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 343 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 344 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 345 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 346 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 347 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 348 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 349 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 350 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 351 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0.0\n",
            "Episode finished!\n",
            "************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "EPISODE 3\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 352 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 353 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 354 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 355 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 356 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 357 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 358 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 359 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 360 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n",
            "TIMESTEP 361 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b2886b37d48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-b2886b37d48d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPISODE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilonp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilonp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b2886b37d48d>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b2886b37d48d>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#input a stack of 4 images, get the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mmax_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0maction_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1721\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 719\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3121\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSK0iwfyf7NG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}