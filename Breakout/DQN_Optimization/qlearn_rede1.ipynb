{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qlearn_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdYve4Htb0_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617382a7-0763-421d-f0d9-ac9e4ce2ef2f"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!pip install atari_py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH6yAoJDzo4Z",
        "outputId": "ace2a925-e2c2-4aa7-89c0-026e38a8c4ae"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hqnsP3c8uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befeaeff-a6f6-4939-a2b0-dbb3d3f9543a"
      },
      "source": [
        "!python -m atari_py.import_roms '/content/drive/My Drive/CN_Breakout/ROMS'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzZrSgq67rId"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/drive/My Drive/CN_OPT\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrXhAuVQ7n-7"
      },
      "source": [
        "from config import (BATCH_SIZE, CLIP_REWARD, DISCOUNT_FACTOR, ENV_NAME,\n",
        "                    EVAL_LENGTH, FRAMES_BETWEEN_EVAL, INPUT_SHAPE,\n",
        "                    LEARNING_RATE,  LOAD_REPLAY_BUFFER,\n",
        "                    MAX_EPISODE_LENGTH, MAX_NOOP_STEPS, MEM_SIZE,\n",
        "                    MIN_REPLAY_BUFFER_SIZE, PRIORITY_SCALE, SAVE_PATH,\n",
        "                    TOTAL_FRAMES, UPDATE_FREQ, USE_PER,\n",
        "                    WRITE_TENSORBOARD, TENSORBOARD_DIR, LOAD_FROM)\n",
        "\n",
        "# TENSORBOARD_DOIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnIy8GynCPlT"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "\n",
        "import gym\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "from tensorflow.keras.layers import (Add, Conv2D, Dense, Flatten, Input,\n",
        "                                     Lambda, Subtract)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2dkPz9jfrfK"
      },
      "source": [
        "def process_frame(frame, shape=(84, 84)):\n",
        "    \"\"\"Preprocesses a 210x160x3 frame to 84x84x1 grayscale\n",
        "    Arguments:\n",
        "        frame: The frame to process.  Must have values ranging from 0-255\n",
        "    Returns:\n",
        "        The processed frame\n",
        "    \"\"\"\n",
        "    frame = frame.astype(np.uint8)  # cv2 requires np.uint8, other dtypes will not work\n",
        "\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame = frame[34:34+160, :160]  # crop image\n",
        "    frame = cv2.resize(frame, shape, interpolation=cv2.INTER_NEAREST)\n",
        "    frame = frame.reshape((*shape, 1))\n",
        "\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucNapzdSoRCb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "from tensorflow.keras.layers import (Add, Conv2D, Dense, Flatten, Input,\n",
        "                                     Lambda, Subtract)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "\n",
        "def build_q_network(n_actions, learning_rate=0.00001, input_shape=(84, 84), history_length=4):\n",
        "    \"\"\"Builds a dueling DQN as a Keras model\n",
        "    Arguments:\n",
        "        n_actions: Number of possible action the agent can take\n",
        "        learning_rate: Learning rate\n",
        "        input_shape: Shape of the preprocessed frame the model sees\n",
        "        history_length: Number of historical frames the agent can see\n",
        "    Returns:\n",
        "        A compiled Keras model\n",
        "    \"\"\"\n",
        "    model_input = Input(shape=(input_shape[0], input_shape[1], history_length))\n",
        "    x = Lambda(lambda layer: layer / 255)(model_input)  # normalize by 255\n",
        "\n",
        "    x = Conv2D(32, (8, 8), strides=4, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
        "    x = Conv2D(64, (4, 4), strides=2, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
        "    x = Conv2D(64, (3, 3), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
        "    x = Conv2D(1024, (7, 7), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
        "\n",
        "    # Split into value and advantage streams\n",
        "    val_stream, adv_stream = Lambda(lambda w: tf.split(w, 2, 3))(x)  # custom splitting layer\n",
        "\n",
        "    val_stream = Flatten()(val_stream)\n",
        "    val = Dense(1, kernel_initializer=VarianceScaling(scale=2.))(val_stream)\n",
        "\n",
        "    adv_stream = Flatten()(adv_stream)\n",
        "    adv = Dense(n_actions, kernel_initializer=VarianceScaling(scale=2.))(adv_stream)\n",
        "\n",
        "    # Combine streams into Q-Values\n",
        "    reduce_mean = Lambda(lambda w: tf.reduce_mean(w, axis=1, keepdims=True))  # custom layer for reduce mean\n",
        "\n",
        "    q_vals = Add()([val, Subtract()([adv, reduce_mean(adv)])])\n",
        "\n",
        "    # Build model\n",
        "    model = Model(model_input, q_vals)\n",
        "    model.compile(Adam(learning_rate), loss=tf.keras.losses.Huber())\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu0wtimgMIyV"
      },
      "source": [
        "def writeFiles(q_max, loss, reward):\n",
        "    with open(\"/content/drive/My Drive/CN_OPT/q_max.txt\", \"a\") as f_q_max:\n",
        "      for e in q_max:\n",
        "        f_q_max.write(str(e) + \"\\n\")\n",
        "\n",
        "    with open(\"/content/drive/My Drive/CN_OPT/loss.txt\", \"a\") as f_loss:\n",
        "      for e in loss:\n",
        "        f_loss.write(str(e) + \"\\n\")\n",
        "\n",
        "    with open(\"/content/drive/My Drive/CN_OPT/reward.txt\", \"a\") as f_rewards:\n",
        "      for e in reward:\n",
        "        f_rewards.write(str(e) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXNcU3_8oyim"
      },
      "source": [
        "class GameWrapper:\n",
        "    \"\"\"Wrapper for the environment provided by Gym\"\"\"\n",
        "    def __init__(self, env_name, no_op_steps=10, history_length=4):\n",
        "        self.env = gym.make(env_name)\n",
        "        self.no_op_steps = no_op_steps\n",
        "        self.history_length = 4\n",
        "\n",
        "        self.state = None\n",
        "        self.last_lives = 0\n",
        "\n",
        "    def reset(self, evaluation=False):\n",
        "        \"\"\"Resets the environment\n",
        "        Arguments:\n",
        "            evaluation: Set to True when the agent is being evaluated. Takes a random number of no-op steps if True.\n",
        "        \"\"\"\n",
        "\n",
        "        self.frame = self.env.reset()\n",
        "        self.last_lives = 0\n",
        "\n",
        "        # If evaluating, take a random number of no-op steps.\n",
        "        # This adds an element of randomness, so that the each\n",
        "        # evaluation is slightly different.\n",
        "        if evaluation:\n",
        "            for _ in range(random.randint(0, self.no_op_steps)):\n",
        "                self.env.step(1)\n",
        "\n",
        "        # For the initial state, we stack the first frame four times\n",
        "        self.state = np.repeat(process_frame(self.frame), self.history_length, axis=2)\n",
        "\n",
        "    def step(self, action, render_mode=None):\n",
        "        \"\"\"Performs an action and observes the result\n",
        "        Arguments:\n",
        "            action: An integer describe action the agent chose\n",
        "            render_mode: None doesn't render anything, 'human' renders the screen in a new window, 'rgb_array' returns an np.array with rgb values\n",
        "        Returns:\n",
        "            processed_frame: The processed new frame as a result of that action\n",
        "            reward: The reward for taking that action\n",
        "            terminal: Whether the game has ended\n",
        "            life_lost: Whether a life has been lost\n",
        "            new_frame: The raw new frame as a result of that action\n",
        "            If render_mode is set to 'rgb_array' this also returns the rendered rgb_array\n",
        "        \"\"\"\n",
        "        new_frame, reward, terminal, info = self.env.step(action)\n",
        "\n",
        "        # In the commonly ignored 'info' or 'meta' data returned by env.step\n",
        "        # we can get information such as the number of lives the agent has.\n",
        "\n",
        "        # We use this here to find out when the agent loses a life, and\n",
        "        # if so, we set life_lost to True.\n",
        "\n",
        "        # We use life_lost to force the agent to start the game\n",
        "        # and not sit around doing nothing.\n",
        "        if info['ale.lives'] < self.last_lives:\n",
        "            life_lost = True\n",
        "        else:\n",
        "            life_lost = terminal\n",
        "        self.last_lives = info['ale.lives']\n",
        "\n",
        "        processed_frame = process_frame(new_frame)\n",
        "        self.state = np.append(self.state[:, :, 1:], processed_frame, axis=2)\n",
        "\n",
        "       \n",
        "        if render_mode == 'rgb_array':\n",
        "            return processed_frame, reward, terminal, life_lost, self.env.render(render_mode)\n",
        "        elif render_mode == 'human':\n",
        "            self.env.render()\n",
        "\n",
        "        return processed_frame, reward, terminal, life_lost\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peZ6tHKio4Kd"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Replay Buffer to store transitions.\n",
        "    This implementation was heavily inspired by Fabio M. Graetz's replay buffer\n",
        "    here: https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\"\"\"\n",
        "    def __init__(self, size=1000000, input_shape=(84, 84), history_length=4, use_per=True):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            size: Integer, Number of stored transitions\n",
        "            input_shape: Shape of the preprocessed frame\n",
        "            history_length: Integer, Number of frames stacked together to create a state for the agent\n",
        "            use_per: Use PER instead of classic experience replay\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.input_shape = input_shape\n",
        "        self.history_length = history_length\n",
        "        self.count = 0  # total index of memory written to, always less than self.size\n",
        "        self.current = 0  # index to write to\n",
        "\n",
        "        # Pre-allocate memory\n",
        "        self.actions = np.empty(self.size, dtype=np.int32)\n",
        "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
        "        self.frames = np.empty((self.size, self.input_shape[0], self.input_shape[1]), dtype=np.uint8)\n",
        "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
        "        self.priorities = np.zeros(self.size, dtype=np.float32)\n",
        "\n",
        "        self.use_per = use_per\n",
        "\n",
        "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
        "        \"\"\"Saves a transition to the replay buffer\n",
        "        Arguments:\n",
        "            action: An integer between 0 and env.action_space.n - 1 \n",
        "                determining the action the agent perfomed\n",
        "            frame: A (84, 84, 1) frame of the game in grayscale\n",
        "            reward: A float determining the reward the agend received for performing an action\n",
        "            terminal: A bool stating whether the episode terminated\n",
        "        \"\"\"\n",
        "        if frame.shape != self.input_shape:\n",
        "            raise ValueError('Dimension of frame is wrong!')\n",
        "\n",
        "        if clip_reward:\n",
        "            reward = np.sign(reward)\n",
        "\n",
        "        # Write memory\n",
        "        self.actions[self.current] = action\n",
        "        self.frames[self.current, ...] = frame\n",
        "        self.rewards[self.current] = reward\n",
        "        self.terminal_flags[self.current] = terminal\n",
        "        self.priorities[self.current] = max(self.priorities.max(), 1)  # make the most recent experience important\n",
        "        self.count = max(self.count, self.current+1)\n",
        "        self.current = (self.current + 1) % self.size\n",
        "\n",
        "    def get_minibatch(self, batch_size=32, priority_scale=0.0):\n",
        "        \"\"\"Returns a minibatch of self.batch_size = 32 transitions\n",
        "        Arguments:\n",
        "            batch_size: How many samples to return\n",
        "            priority_scale: How much to weight priorities. 0 = completely random, 1 = completely based on priority\n",
        "        Returns:\n",
        "            A tuple of states, actions, rewards, new_states, and terminals\n",
        "            If use_per is True:\n",
        "                An array describing the importance of transition. Used for scaling gradient steps.\n",
        "                An array of each index that was sampled\n",
        "        \"\"\"\n",
        "\n",
        "        if self.count < self.history_length:\n",
        "            raise ValueError('Not enough memories to get a minibatch')\n",
        "\n",
        "        # Get sampling probabilities from priority list\n",
        "        if self.use_per:\n",
        "            scaled_priorities = self.priorities[self.history_length:self.count-1] ** priority_scale\n",
        "            sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
        "\n",
        "        # Get a list of valid indices\n",
        "        indices = []\n",
        "        for i in range(batch_size):\n",
        "            while True:\n",
        "                # Get a random number from history_length to maximum frame written with probabilities based on priority weights\n",
        "                if self.use_per:\n",
        "                    index = np.random.choice(np.arange(self.history_length, self.count-1), p=sample_probabilities)\n",
        "                else:\n",
        "                    index = random.randint(self.history_length, self.count - 1)\n",
        "\n",
        "                # We check that all frames are from same episode with the two following if statements.  If either are True, the index is invalid.\n",
        "                if index >= self.current and index - self.history_length <= self.current:\n",
        "                    continue\n",
        "                if self.terminal_flags[index - self.history_length:index].any():\n",
        "                    continue\n",
        "                break\n",
        "            indices.append(index)\n",
        "\n",
        "        # Retrieve states from memory\n",
        "        states = []\n",
        "        new_states = []\n",
        "        for idx in indices:\n",
        "            states.append(self.frames[idx-self.history_length:idx, ...])\n",
        "            new_states.append(self.frames[idx-self.history_length+1:idx+1, ...])\n",
        "\n",
        "        states = np.transpose(np.asarray(states), axes=(0, 2, 3, 1))\n",
        "        new_states = np.transpose(np.asarray(new_states), axes=(0, 2, 3, 1))\n",
        "\n",
        "        if self.use_per:\n",
        "            # Get importance weights from probabilities calculated earlier\n",
        "            importance = 1/self.count * 1/sample_probabilities[[index - self.history_length for index in indices]]\n",
        "            importance = importance / importance.max()\n",
        "\n",
        "            return (states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]), importance, indices\n",
        "        else:\n",
        "            return states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]\n",
        "\n",
        "    def set_priorities(self, indices, errors, offset=0.1):\n",
        "        \"\"\"Update priorities for PER\n",
        "        Arguments:\n",
        "            indices: Indices to update\n",
        "            errors: For each index, the error between the target Q-vals and the predicted Q-vals\n",
        "        \"\"\"\n",
        "        for i, e in zip(indices, errors):\n",
        "            self.priorities[i] = abs(e) + offset\n",
        "\n",
        "    def save(self, folder_name):\n",
        "        \"\"\"Save the replay buffer to a folder\"\"\"\n",
        "\n",
        "        print(\"HERE: \", folder_name)\n",
        "        np.save(folder_name + '/actions.npy', self.actions)\n",
        "        np.save(folder_name + '/frames.npy', self.frames)\n",
        "        np.save(folder_name + '/rewards.npy', self.rewards)\n",
        "        np.save(folder_name + '/terminal_flags.npy', self.terminal_flags)\n",
        "\n",
        "    def load(self, folder_name):\n",
        "        \"\"\"Loads the replay buffer from a folder\"\"\"\n",
        "        self.actions = np.load(folder_name + '/actions.npy')\n",
        "        self.frames = np.load(folder_name + '/frames.npy')\n",
        "        self.rewards = np.load(folder_name + '/rewards.npy')\n",
        "        self.terminal_flags = np.load(folder_name + '/terminal_flags.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTjFBsaspOaT"
      },
      "source": [
        "class Agent(object):\n",
        "    \"\"\"Implements a standard DDDQN agent\"\"\"\n",
        "    def __init__(self,\n",
        "                 dqn,\n",
        "                 target_dqn,\n",
        "                 replay_buffer,\n",
        "                 n_actions,\n",
        "                 input_shape=(84, 84),\n",
        "                 batch_size=32,\n",
        "                 history_length=4,\n",
        "                 eps_initial=1,\n",
        "                 eps_final=0.1,\n",
        "                 eps_final_frame=0.01,\n",
        "                 eps_evaluation=0.0,\n",
        "                 eps_annealing_frames=1000000,\n",
        "                 replay_buffer_start_size=50000,\n",
        "                 max_frames=25000000,\n",
        "                 use_per=True):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            dqn: A DQN (returned by the DQN function) to predict moves\n",
        "            target_dqn: A DQN (returned by the DQN function) to predict target-q values.  This can be initialized in the same way as the dqn argument\n",
        "            replay_buffer: A ReplayBuffer object for holding all previous experiences\n",
        "            n_actions: Number of possible actions for the given environment\n",
        "            input_shape: Tuple/list describing the shape of the pre-processed environment\n",
        "            batch_size: Number of samples to draw from the replay memory every updating session\n",
        "            history_length: Number of historical frames available to the agent\n",
        "            eps_initial: Initial epsilon value.\n",
        "            eps_final: The \"half-way\" epsilon value.  The epsilon value decreases more slowly after this\n",
        "            eps_final_frame: The final epsilon value\n",
        "            eps_evaluation: The epsilon value used during evaluation\n",
        "            eps_annealing_frames: Number of frames during which epsilon will be annealed to eps_final, then eps_final_frame\n",
        "            replay_buffer_start_size: Size of replay buffer before beginning to learn (after this many frames, epsilon is decreased more slowly)\n",
        "            max_frames: Number of total frames the agent will be trained for\n",
        "            use_per: Use PER instead of classic experience replay\n",
        "        \"\"\"\n",
        "\n",
        "        self.n_actions = n_actions\n",
        "        self.input_shape = input_shape\n",
        "        self.history_length = history_length\n",
        "\n",
        "        # Memory information\n",
        "        self.replay_buffer_start_size = replay_buffer_start_size\n",
        "        self.max_frames = max_frames\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.use_per = use_per\n",
        "\n",
        "        # Epsilon information\n",
        "        self.eps_initial = eps_initial\n",
        "        self.eps_final = eps_final\n",
        "        self.eps_final_frame = eps_final_frame\n",
        "        self.eps_evaluation = eps_evaluation\n",
        "        self.eps_annealing_frames = eps_annealing_frames\n",
        "\n",
        "        # Slopes and intercepts for exploration decrease\n",
        "        # (Credit to Fabio M. Graetz for this and calculating epsilon based on frame number)\n",
        "        self.slope = -(self.eps_initial - self.eps_final) / self.eps_annealing_frames\n",
        "        self.intercept = self.eps_initial - self.slope*self.replay_buffer_start_size\n",
        "        self.slope_2 = -(self.eps_final - self.eps_final_frame) / (self.max_frames - self.eps_annealing_frames - self.replay_buffer_start_size)\n",
        "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
        "\n",
        "        # DQN\n",
        "        self.DQN = dqn\n",
        "        self.target_dqn = target_dqn\n",
        "\n",
        "    def calc_epsilon(self, frame_number, evaluation=False):\n",
        "        \"\"\"Get the appropriate epsilon value from a given frame number\n",
        "        Arguments:\n",
        "            frame_number: Global frame number (used for epsilon)\n",
        "            evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
        "        Returns:\n",
        "            The appropriate epsilon value\n",
        "        \"\"\"\n",
        "        if evaluation:\n",
        "            return self.eps_evaluation\n",
        "        elif frame_number < self.replay_buffer_start_size:\n",
        "            return self.eps_initial\n",
        "        elif frame_number >= self.replay_buffer_start_size and frame_number < self.replay_buffer_start_size + self.eps_annealing_frames:\n",
        "            return self.slope*frame_number + self.intercept\n",
        "        elif frame_number >= self.replay_buffer_start_size + self.eps_annealing_frames:\n",
        "            return self.slope_2*frame_number + self.intercept_2\n",
        "\n",
        "    def get_action(self, frame_number, state, evaluation=False):\n",
        "        \"\"\"Query the DQN for an action given a state\n",
        "        Arguments:\n",
        "            frame_number: Global frame number (used for epsilon)\n",
        "            state: State to give an action for\n",
        "            evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
        "        Returns:\n",
        "            An integer as the predicted move\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate epsilon based on the frame number\n",
        "        eps = self.calc_epsilon(frame_number, evaluation)\n",
        "\n",
        "        # With chance epsilon, take a random action\n",
        "        if np.random.rand(1) < eps:\n",
        "            return np.random.randint(0, self.n_actions)\n",
        "\n",
        "        # Otherwise, query the DQN for an action\n",
        "        q_vals = self.DQN.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))[0]\n",
        "        return q_vals.argmax()\n",
        "\n",
        "    def get_intermediate_representation(self, state, layer_names=None, stack_state=True):\n",
        "        \"\"\"\n",
        "        Get the output of a hidden layer inside the model.  This will be/is used for visualizing model\n",
        "        Arguments:\n",
        "            state: The input to the model to get outputs for hidden layers from\n",
        "            layer_names: Names of the layers to get outputs from.  This can be a list of multiple names, or a single name\n",
        "            stack_state: Stack `state` four times so the model can take input on a single (84, 84, 1) frame\n",
        "        Returns:\n",
        "            Outputs to the hidden layers specified, in the order they were specified.\n",
        "        \"\"\"\n",
        "        # Prepare list of layers\n",
        "        if isinstance(layer_names, list) or isinstance(layer_names, tuple):\n",
        "            layers = [self.DQN.get_layer(name=layer_name).output for layer_name in layer_names]\n",
        "        else:\n",
        "            layers = self.DQN.get_layer(name=layer_names).output\n",
        "\n",
        "        # Model for getting intermediate output\n",
        "        temp_model = tf.keras.Model(self.DQN.inputs, layers)\n",
        "\n",
        "        # Stack state 4 times\n",
        "        if stack_state:\n",
        "            if len(state.shape) == 2:\n",
        "                state = state[:, :, np.newaxis]\n",
        "            state = np.repeat(state, self.history_length, axis=2)\n",
        "\n",
        "        # Put it all together\n",
        "        return temp_model.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Update the target Q network\"\"\"\n",
        "        self.target_dqn.set_weights(self.DQN.get_weights())\n",
        "\n",
        "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
        "        \"\"\"Wrapper function for adding an experience to the Agent's replay buffer\"\"\"\n",
        "        self.replay_buffer.add_experience(action, frame, reward, terminal, clip_reward)\n",
        "\n",
        "    def learn(self, batch_size, gamma, frame_number, priority_scale=1.0):\n",
        "        \"\"\"Sample a batch and use it to improve the DQN\n",
        "        Arguments:\n",
        "            batch_size: How many samples to draw for an update\n",
        "            gamma: Reward discount\n",
        "            frame_number: Global frame number (used for calculating importances)\n",
        "            priority_scale: How much to weight priorities when sampling the replay buffer. 0 = completely random, 1 = completely based on priority\n",
        "        Returns:\n",
        "            The loss between the predicted and target Q as a float\n",
        "        \"\"\"\n",
        "\n",
        "        if self.use_per:\n",
        "            (states, actions, rewards, new_states, terminal_flags), importance, indices = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
        "            importance = importance ** (1-self.calc_epsilon(frame_number))\n",
        "        else:\n",
        "            states, actions, rewards, new_states, terminal_flags = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
        "\n",
        "        # Main DQN estimates best action in new states\n",
        "        arg_q_max = self.DQN.predict(new_states).argmax(axis=1)\n",
        "\n",
        "        # Target DQN estimates q-vals for new states\n",
        "        future_q_vals = self.target_dqn.predict(new_states)\n",
        "        double_q = future_q_vals[range(batch_size), arg_q_max]\n",
        "\n",
        "        # Calculate targets (bellman equation)\n",
        "        target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
        "\n",
        "        # Use targets to calculate loss (and use loss to calculate gradients)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.DQN(states)\n",
        "\n",
        "            one_hot_actions = tf.keras.utils.to_categorical(actions, self.n_actions, dtype=np.float32)  # using tf.one_hot causes strange errors\n",
        "            Q = tf.reduce_sum(tf.multiply(q_values, one_hot_actions), axis=1)\n",
        "\n",
        "            error = Q - target_q\n",
        "            loss = tf.keras.losses.Huber()(target_q, Q)\n",
        "\n",
        "            if self.use_per:\n",
        "                # Multiply the loss by importance, so that the gradient is also scaled.\n",
        "                # The importance scale reduces bias against situataions that are sampled\n",
        "                # more frequently.\n",
        "                loss = tf.reduce_mean(loss * importance)\n",
        "\n",
        "        model_gradients = tape.gradient(loss, self.DQN.trainable_variables)\n",
        "        self.DQN.optimizer.apply_gradients(zip(model_gradients, self.DQN.trainable_variables))\n",
        "\n",
        "        if self.use_per:\n",
        "            self.replay_buffer.set_priorities(indices, error)\n",
        "\n",
        "        return float(loss.numpy()), error, double_q\n",
        "\n",
        "    def save(self, folder_name, **kwargs):\n",
        "        \"\"\"Saves the Agent and all corresponding properties into a folder\n",
        "        Arguments:\n",
        "            folder_name: Folder in which to save the Agent\n",
        "            **kwargs: Agent.save will also save any keyword arguments passed.  This is used for saving the frame_number\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # Save DQN and target DQN\n",
        "        self.DQN.save(folder_name + '/dqn.h5')\n",
        "        self.target_dqn.save(folder_name + '/target_dqn.h5')\n",
        "\n",
        "        # Save replay buffer\n",
        "        self.replay_buffer.save(folder_name)\n",
        "\n",
        "        # Save meta\n",
        "        with open(folder_name + '/meta.json', 'w+') as f:\n",
        "            f.write(json.dumps({**{'buff_count': self.replay_buffer.count, 'buff_curr': self.replay_buffer.current}, **kwargs}))  # save replay_buffer information and any other information\n",
        "\n",
        "    def load(self, folder_name, load_replay_buffer=True):\n",
        "        \"\"\"Load a previously saved Agent from a folder\n",
        "        Arguments:\n",
        "            folder_name: Folder from which to load the Agent\n",
        "        Returns:\n",
        "            All other saved attributes, e.g., frame number\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.isdir(folder_name):\n",
        "            raise ValueError(f'{folder_name} is not a valid directory')\n",
        "\n",
        "        # Load DQNs\n",
        "        self.DQN = tf.keras.models.load_model(folder_name + '/dqn.h5')\n",
        "        self.target_dqn = tf.keras.models.load_model(folder_name + '/target_dqn.h5')\n",
        "        self.optimizer = self.DQN.optimizer\n",
        "\n",
        "        # Load replay buffer\n",
        "        if load_replay_buffer:\n",
        "            self.replay_buffer.load(folder_name)\n",
        "\n",
        "        # Load meta\n",
        "        with open(folder_name + '/meta.json', 'r') as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        if load_replay_buffer:\n",
        "            self.replay_buffer.count = meta['buff_count']\n",
        "            self.replay_buffer.current = meta['buff_curr']\n",
        "\n",
        "        del meta['buff_count'], meta['buff_curr']  # we don't want to return this information\n",
        "        return meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w8B_nV1pRS6",
        "outputId": "5d455204-75c3-4215-be82-57678982b831"
      },
      "source": [
        "# Create environment\n",
        "game_wrapper = GameWrapper(ENV_NAME, MAX_NOOP_STEPS)\n",
        "print(\"The environment has the following {} actions: {}\".format(game_wrapper.env.action_space.n, game_wrapper.env.unwrapped.get_action_meanings()))\n",
        "\n",
        "# TensorBoard writer\n",
        "writer = tf.summary.create_file_writer(TENSORBOARD_DIR)\n",
        "\n",
        "# Build main and target networks\n",
        "MAIN_DQN = build_q_network(game_wrapper.env.action_space.n, LEARNING_RATE, input_shape=INPUT_SHAPE)\n",
        "TARGET_DQN = build_q_network(game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
        "\n",
        "replay_buffer = ReplayBuffer(size=MEM_SIZE, input_shape=INPUT_SHAPE, use_per=USE_PER)\n",
        "agent = Agent(MAIN_DQN, TARGET_DQN, replay_buffer, game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE, batch_size=BATCH_SIZE, use_per=USE_PER)\n",
        "\n",
        "# Training and evaluation\n",
        "if LOAD_FROM is None:\n",
        "    frame_number = 0\n",
        "    rewards = []\n",
        "    loss_list = []\n",
        "    double_q_list = []\n",
        "else:\n",
        "    print('Loading from', LOAD_FROM)\n",
        "    meta = agent.load(LOAD_FROM, LOAD_REPLAY_BUFFER)\n",
        "\n",
        "    # Apply information loaded from meta\n",
        "    frame_number = meta['frame_number']\n",
        "    rewards = meta['rewards']\n",
        "    loss_list = meta['loss_list']\n",
        "    double_q_list = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "Loading from /content/drive/My Drive/CN_OPT/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v8Uvf7SpZot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cf8f21-9c65-478c-edb2-0a3ec2fe3940"
      },
      "source": [
        "# Main loop\n",
        "try:\n",
        "    with writer.as_default():\n",
        "        while frame_number < TOTAL_FRAMES:\n",
        "            # Training\n",
        "\n",
        "            epoch_frame = 0\n",
        "            while epoch_frame < FRAMES_BETWEEN_EVAL:\n",
        "                start_time = time.time()\n",
        "                game_wrapper.reset()\n",
        "                life_lost = True\n",
        "                episode_reward_sum = 0\n",
        "                for _ in range(MAX_EPISODE_LENGTH):\n",
        "                    # Get action\n",
        "                    action = agent.get_action(frame_number, game_wrapper.state)\n",
        "\n",
        "                    # Take step\n",
        "                    processed_frame, reward, terminal, life_lost = game_wrapper.step(action)\n",
        "                    frame_number += 1\n",
        "                    epoch_frame += 1\n",
        "                    episode_reward_sum += reward\n",
        "\n",
        "                    # Add experience to replay memory\n",
        "                    agent.add_experience(action=action,\n",
        "                                        frame=processed_frame[:, :, 0],\n",
        "                                        reward=reward, clip_reward=CLIP_REWARD,\n",
        "                                        terminal=life_lost)\n",
        "\n",
        "                    # Update agent\n",
        "                    if frame_number % UPDATE_FREQ == 0 and agent.replay_buffer.count > MIN_REPLAY_BUFFER_SIZE:\n",
        "                        loss, _, double_q = agent.learn(BATCH_SIZE, gamma=DISCOUNT_FACTOR, frame_number=frame_number, priority_scale=PRIORITY_SCALE)\n",
        "                        loss_list.append(loss)\n",
        "                        double_q_list.append(double_q)\n",
        "\n",
        "                    # Update target network\n",
        "                    if frame_number % UPDATE_FREQ == 0 and frame_number > MIN_REPLAY_BUFFER_SIZE:\n",
        "                        agent.update_target_network()\n",
        "\n",
        "                    # Break the loop when the game is over\n",
        "                    if terminal:\n",
        "                        terminal = False\n",
        "                        break\n",
        "\n",
        "                rewards.append(episode_reward_sum)\n",
        "\n",
        "                # Output the progress every 10 games\n",
        "                if len(rewards) % 10 == 0:\n",
        "                    # Write to TensorBoard\n",
        "                    if WRITE_TENSORBOARD:\n",
        "                        tf.summary.scalar('Reward', np.mean(rewards[-10:]), frame_number)\n",
        "                        tf.summary.scalar('Loss', np.mean(loss_list[-100:]), frame_number)\n",
        "                        writer.flush()\n",
        "\n",
        "                    print(f'Game number: {str(len(rewards)).zfill(6)}  Frame number: {str(frame_number).zfill(8)}  Average reward: {np.mean(rewards[-10:]):0.1f}  Time taken: {(time.time() - start_time):.1f}s')\n",
        "\n",
        "            # Evaluation every `FRAMES_BETWEEN_EVAL` frames\n",
        "            terminal = True\n",
        "            eval_rewards = []\n",
        "            evaluate_frame_number = 0\n",
        "\n",
        "            for _ in range(EVAL_LENGTH):\n",
        "                if terminal:\n",
        "                    game_wrapper.reset(evaluation=True)\n",
        "                    life_lost = True\n",
        "                    episode_reward_sum = 0\n",
        "                    terminal = False\n",
        "\n",
        "                # Breakout requires a \"fire\" action (action #1) to start the\n",
        "                # game each time a life is lost.\n",
        "                # Otherwise, the agent would sit around doing nothing.\n",
        "                action = 1 if life_lost else agent.get_action(frame_number, game_wrapper.state, evaluation=True)\n",
        "\n",
        "                # Step action\n",
        "                _, reward, terminal, life_lost = game_wrapper.step(action)\n",
        "                evaluate_frame_number += 1\n",
        "                episode_reward_sum += reward\n",
        "\n",
        "                # On game-over\n",
        "\n",
        "                if terminal:\n",
        "                    eval_rewards.append(episode_reward_sum)\n",
        "\n",
        "            if len(eval_rewards) > 0:\n",
        "                final_score = np.mean(eval_rewards)\n",
        "            else:\n",
        "                # In case the game is longer than the number of frames allowed\n",
        "                final_score = episode_reward_sum\n",
        "            # Print score and write to tensorboard\n",
        "            print('Evaluation score:', final_score)\n",
        "            if WRITE_TENSORBOARD:\n",
        "                tf.summary.scalar('Evaluation score', final_score, frame_number)\n",
        "                writer.flush()\n",
        "\n",
        "            # Save model\n",
        "            if len(rewards) > 300 and SAVE_PATH is not None:\n",
        "                agent.save(\"/content/drive/My Drive/CN_OPT/models\", frame_number=frame_number, rewards=rewards, loss_list=loss_list)\n",
        "                writeFiles(double_q_list, loss_list, rewards)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('\\nTraining exited early.')\n",
        "    \n",
        "    writer.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Game number: 003560  Frame number: 00905424  Average reward: 14.6  Time taken: 32.1s\n",
            "Game number: 003570  Frame number: 00912067  Average reward: 18.2  Time taken: 37.4s\n",
            "Game number: 003580  Frame number: 00919831  Average reward: 21.8  Time taken: 20.7s\n",
            "Game number: 003590  Frame number: 00926573  Average reward: 17.9  Time taken: 29.2s\n",
            "Game number: 003600  Frame number: 00932218  Average reward: 13.4  Time taken: 23.1s\n",
            "Game number: 003610  Frame number: 00939724  Average reward: 21.7  Time taken: 38.3s\n",
            "Game number: 003620  Frame number: 00946228  Average reward: 17.3  Time taken: 32.0s\n",
            "Game number: 003630  Frame number: 00954116  Average reward: 23.4  Time taken: 27.6s\n",
            "Game number: 003640  Frame number: 00961267  Average reward: 19.6  Time taken: 38.1s\n",
            "Game number: 003650  Frame number: 00968764  Average reward: 20.0  Time taken: 40.2s\n",
            "Game number: 003660  Frame number: 00975623  Average reward: 20.5  Time taken: 18.7s\n",
            "Game number: 003670  Frame number: 00984214  Average reward: 28.0  Time taken: 44.6s\n",
            "Game number: 003680  Frame number: 00991764  Average reward: 21.6  Time taken: 49.5s\n",
            "Game number: 003690  Frame number: 00999387  Average reward: 20.9  Time taken: 39.1s\n",
            "Evaluation score: 31.555555555555557\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "HERE:  /content/drive/My Drive/CN_OPT/models\n",
            "Game number: 003700  Frame number: 01006853  Average reward: 22.5  Time taken: 17.3s\n",
            "Game number: 003710  Frame number: 01014180  Average reward: 22.4  Time taken: 47.5s\n",
            "Game number: 003720  Frame number: 01023265  Average reward: 31.1  Time taken: 56.0s\n",
            "Game number: 003730  Frame number: 01032084  Average reward: 29.1  Time taken: 37.5s\n",
            "Game number: 003740  Frame number: 01041930  Average reward: 33.6  Time taken: 88.1s\n",
            "Game number: 003750  Frame number: 01050813  Average reward: 29.4  Time taken: 69.7s\n",
            "Game number: 003760  Frame number: 01059996  Average reward: 29.8  Time taken: 54.5s\n",
            "Game number: 003770  Frame number: 01068980  Average reward: 29.7  Time taken: 41.4s\n",
            "Game number: 003780  Frame number: 01078925  Average reward: 34.2  Time taken: 47.1s\n",
            "Game number: 003790  Frame number: 01088829  Average reward: 36.2  Time taken: 62.5s\n",
            "Game number: 003800  Frame number: 01099100  Average reward: 35.5  Time taken: 79.1s\n",
            "Evaluation score: 43.25\n",
            "HERE:  /content/drive/My Drive/CN_OPT/models\n",
            "Game number: 003810  Frame number: 01109262  Average reward: 35.0  Time taken: 43.8s\n",
            "Game number: 003820  Frame number: 01120041  Average reward: 40.0  Time taken: 51.2s\n",
            "Game number: 003830  Frame number: 01129446  Average reward: 33.0  Time taken: 47.1s\n",
            "Game number: 003840  Frame number: 01139665  Average reward: 36.2  Time taken: 58.7s\n",
            "Game number: 003850  Frame number: 01148250  Average reward: 26.9  Time taken: 62.6s\n",
            "Game number: 003860  Frame number: 01158404  Average reward: 37.5  Time taken: 61.3s\n",
            "Game number: 003870  Frame number: 01168205  Average reward: 31.3  Time taken: 72.4s\n",
            "Game number: 003880  Frame number: 01179500  Average reward: 41.3  Time taken: 54.2s\n",
            "Game number: 003890  Frame number: 01189719  Average reward: 35.2  Time taken: 63.4s\n",
            "Game number: 003900  Frame number: 01200279  Average reward: 38.2  Time taken: 66.8s\n",
            "Evaluation score: 64.57142857142857\n",
            "HERE:  /content/drive/My Drive/CN_OPT/models\n",
            "Game number: 003910  Frame number: 01210649  Average reward: 38.6  Time taken: 53.9s\n",
            "Game number: 003920  Frame number: 01222823  Average reward: 45.3  Time taken: 38.0s\n",
            "Game number: 003930  Frame number: 01233877  Average reward: 39.7  Time taken: 29.5s\n",
            "Game number: 003940  Frame number: 01245561  Average reward: 50.5  Time taken: 68.9s\n",
            "Game number: 003950  Frame number: 01256584  Average reward: 42.1  Time taken: 84.3s\n",
            "Game number: 003960  Frame number: 01266408  Average reward: 37.9  Time taken: 53.0s\n",
            "Game number: 003970  Frame number: 01276662  Average reward: 39.2  Time taken: 83.6s\n",
            "Game number: 003980  Frame number: 01287691  Average reward: 41.8  Time taken: 64.3s\n",
            "Game number: 003990  Frame number: 01299623  Average reward: 45.3  Time taken: 74.1s\n",
            "Evaluation score: 49.125\n",
            "HERE:  /content/drive/My Drive/CN_OPT/models\n",
            "Game number: 004000  Frame number: 01309051  Average reward: 33.6  Time taken: 70.9s\n",
            "Game number: 004010  Frame number: 01320489  Average reward: 45.1  Time taken: 79.3s\n",
            "Game number: 004020  Frame number: 01331597  Average reward: 42.8  Time taken: 43.4s\n",
            "Game number: 004030  Frame number: 01343837  Average reward: 55.2  Time taken: 75.3s\n",
            "Game number: 004040  Frame number: 01355303  Average reward: 44.0  Time taken: 51.1s\n",
            "Game number: 004050  Frame number: 01367253  Average reward: 49.1  Time taken: 78.1s\n",
            "Game number: 004060  Frame number: 01379276  Average reward: 49.3  Time taken: 70.2s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}