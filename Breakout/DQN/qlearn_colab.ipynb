{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qlearn_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdYve4Htb0_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c814fa-24bd-46a1-f475-2198489f518b"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!pip install atari_py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH6yAoJDzo4Z",
        "outputId": "fa9a0f62-2feb-4435-905e-89d646eef005"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hqnsP3c8uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce42fc1-b601-4cb0-9873-64537522130a"
      },
      "source": [
        "!python -m atari_py.import_roms '/content/drive/My Drive/CN_Breakout/ROMS'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying mr_do.bin from /content/drive/My Drive/CN_Breakout/ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying lost_luggage.bin from /content/drive/My Drive/CN_Breakout/ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying elevator_action.bin from /content/drive/My Drive/CN_Breakout/ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying asterix.bin from /content/drive/My Drive/CN_Breakout/ROMS/Asterix (AKA Taz) (1984) (Atari, Jerome Domurat, Steve Woita) (CX2696).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying riverraid.bin from /content/drive/My Drive/CN_Breakout/ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying video_pinball.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying road_runner.bin from patched version of /content/drive/My Drive/CN_Breakout/ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying qbert.bin from /content/drive/My Drive/CN_Breakout/ROMS/Q. Bert (1983) (CCE) (C-822).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying surround.bin from /content/drive/My Drive/CN_Breakout/ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying ms_pacman.bin from /content/drive/My Drive/CN_Breakout/ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying up_n_down.bin from /content/drive/My Drive/CN_Breakout/ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying kung_fu_master.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying atlantis.bin from /content/drive/My Drive/CN_Breakout/ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying pitfall.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying chopper_command.bin from /content/drive/My Drive/CN_Breakout/ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying ice_hockey.bin from /content/drive/My Drive/CN_Breakout/ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying breakout.bin from /content/drive/My Drive/CN_Breakout/ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying boxing.bin from /content/drive/My Drive/CN_Breakout/ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying freeway.bin from /content/drive/My Drive/CN_Breakout/ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying kaboom.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying skiing.bin from /content/drive/My Drive/CN_Breakout/ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying tennis.bin from /content/drive/My Drive/CN_Breakout/ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying bank_heist.bin from /content/drive/My Drive/CN_Breakout/ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying demon_attack.bin from /content/drive/My Drive/CN_Breakout/ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying koolaid.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying star_gunner.bin from /content/drive/My Drive/CN_Breakout/ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying trondead.bin from /content/drive/My Drive/CN_Breakout/ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying robotank.bin from /content/drive/My Drive/CN_Breakout/ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying space_invaders.bin from /content/drive/My Drive/CN_Breakout/ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying hero.bin from /content/drive/My Drive/CN_Breakout/ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying montezuma_revenge.bin from /content/drive/My Drive/CN_Breakout/ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying alien.bin from /content/drive/My Drive/CN_Breakout/ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying asteroids.bin from /content/drive/My Drive/CN_Breakout/ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying air_raid.bin from /content/drive/My Drive/CN_Breakout/ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying assault.bin from /content/drive/My Drive/CN_Breakout/ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying amidar.bin from /content/drive/My Drive/CN_Breakout/ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying berzerk.bin from /content/drive/My Drive/CN_Breakout/ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from /content/drive/My Drive/CN_Breakout/ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying beam_rider.bin from /content/drive/My Drive/CN_Breakout/ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying battle_zone.bin from /content/drive/My Drive/CN_Breakout/ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying carnival.bin from /content/drive/My Drive/CN_Breakout/ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from /content/drive/My Drive/CN_Breakout/ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying crazy_climber.bin from /content/drive/My Drive/CN_Breakout/ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying donkey_kong.bin from /content/drive/My Drive/CN_Breakout/ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying defender.bin from /content/drive/My Drive/CN_Breakout/ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying double_dunk.bin from /content/drive/My Drive/CN_Breakout/ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying frostbite.bin from /content/drive/My Drive/CN_Breakout/ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying enduro.bin from /content/drive/My Drive/CN_Breakout/ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from /content/drive/My Drive/CN_Breakout/ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying frogger.bin from /content/drive/My Drive/CN_Breakout/ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying gopher.bin from /content/drive/My Drive/CN_Breakout/ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from /content/drive/My Drive/CN_Breakout/ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying galaxian.bin from /content/drive/My Drive/CN_Breakout/ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying journey_escape.bin from /content/drive/My Drive/CN_Breakout/ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kangaroo.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying krull.bin from /content/drive/My Drive/CN_Breakout/ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying name_this_game.bin from /content/drive/My Drive/CN_Breakout/ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from /content/drive/My Drive/CN_Breakout/ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying private_eye.bin from /content/drive/My Drive/CN_Breakout/ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying pooyan.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying seaquest.bin from /content/drive/My Drive/CN_Breakout/ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying solaris.bin from /content/drive/My Drive/CN_Breakout/ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying venture.bin from /content/drive/My Drive/CN_Breakout/ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from /content/drive/My Drive/CN_Breakout/ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from /content/drive/My Drive/CN_Breakout/ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from /content/drive/My Drive/CN_Breakout/ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from /content/drive/My Drive/CN_Breakout/ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying tutankham.bin from /content/drive/My Drive/CN_Breakout/ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying time_pilot.bin from /content/drive/My Drive/CN_Breakout/ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying jamesbond.bin from /content/drive/My Drive/CN_Breakout/ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying adventure.bin from /content/drive/My Drive/CN_Breakout/ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying pacman.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying keystone_kapers.bin from /content/drive/My Drive/CN_Breakout/ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from /content/drive/My Drive/CN_Breakout/ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from /content/drive/My Drive/CN_Breakout/ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying sir_lancelot.bin from /content/drive/My Drive/CN_Breakout/ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXbGMPQxbxk7",
        "outputId": "3443a34e-5c7b-401a-cc4d-22a97b1c0a40"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import skimage as skimage\n",
        "from skimage import transform, color, exposure\n",
        "from skimage.transform import rotate\n",
        "from skimage.viewer import ImageViewer\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import json\n",
        "from tensorflow.keras.initializers import identity\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD , Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import the gym module\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\n",
        "GAME = 'atari' # the name of the game being played for log files\n",
        "CONFIG = 'nothreshold'\n",
        "ACTIONS = 3 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVATION = 200. # timesteps to observe before training. de cada 3200 frames, vamos ao nosso buffer e selecionamos de forma aleatoria um batch size. Neste caso, 32 frames. em numpy arrays\n",
        "EXPLORE = 3000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.01 # starting value of epsilon EPSILON é para ver o exploration vs exploitation\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "#MAX_STEPS_PER_EPISODE = 1000\n",
        "EPISODES = 10000\n",
        "\n",
        "q_max_list = []\n",
        "loss_list = []\n",
        "reward_list = []\n",
        "\n",
        "img_rows, img_cols = 84, 84\n",
        "#Convert image into Black and white\n",
        "img_channels = 4 #We stack 4 frames\n",
        "\n",
        "def buildmodel(activation, padding, lr, loss):\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = tf.keras.layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = Conv2D(32, 8, strides=4, activation=activation, padding = padding)(inputs)\n",
        "    layer2 = Conv2D(64, 4, strides=2, activation=activation, padding = padding)(layer1)\n",
        "    layer3 = Conv2D(64, 3, strides=1, activation=activation, padding = padding)(layer2)\n",
        "\n",
        "    layer4 = Flatten()(layer3)\n",
        "\n",
        "    layer5 = Dense(512, activation=activation)(layer4)\n",
        "    action = Dense(ACTIONS, activation=\"linear\")(layer5)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=action)\n",
        "    adam = Adam(learning_rate = lr)\n",
        "    model.compile(loss=loss, optimizer = adam)\n",
        "\n",
        "    print(\"We finish building the model\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def writeFiles(q_max, loss, reward):\n",
        "    with open(\"/content/drive/My Drive/CN_Breakout/q_max.txt\", \"a\") as f_q_max:\n",
        "      for e in q_max:\n",
        "        f_q_max.write(str(e) + \"\\n\")\n",
        "\n",
        "    with open(\"/content/drive/My Drive/CN_Breakout/loss.txt\", \"a\") as f_loss:\n",
        "      for e in loss:\n",
        "        f_loss.write(str(e) + \"\\n\")\n",
        "\n",
        "    with open(\"/content/drive/My Drive/CN_Breakout/reward.txt\", \"a\") as f_rewards:\n",
        "      for e in reward:\n",
        "        f_rewards.write(str(e) + \"\\n\")\n",
        "        \n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def reshape(x_t):\n",
        "    x_t = skimage.color.rgb2gray(x_t)\n",
        "    x_t = skimage.transform.resize(x_t, (84,84))\n",
        "    x_t = skimage.exposure.rescale_intensity(x_t, out_range = (0,255))\n",
        "\n",
        "    x_t = x_t / 255.0\n",
        "    return x_t\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n",
        "def trainNetwork(model,args):\n",
        "    # open up a game state to communicate with emulator\n",
        "    env = wrap_env(gym.make('BreakoutDeterministic-v4'))\n",
        "    env.reset()\n",
        "    # store the previous observations in replay memory\n",
        "    \n",
        "    #----------------------------------------\n",
        "    #PARA OBTER O SIGNIFICADO DAS AÇÕES POSSíVEIS\n",
        "    #print(env.unwrapped.get_action_meanings())\n",
        "    #----------------------------------------\n",
        "    \n",
        "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        \n",
        "    x_t, r_0, terminal, info = env.step(1) #COMEÇAR O JOGO COM A AÇÃO \"FIRE\"\n",
        "    \n",
        "    env.render()\n",
        "\n",
        "    D = args['D']\n",
        "\n",
        "    x_t = reshape(x_t)\n",
        "\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2) #colocar a sequência de frames. 4 frames sequenciais, que vamos aplicar à nossa lista. Para conseguir a estabilidade de imagens sequenciais\n",
        "    \n",
        "    #print (s_t.shape)\n",
        "\n",
        "    #In Keras, need to reshape\n",
        "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
        "\n",
        "    t = args['t']\n",
        "\n",
        "    if args['mode'] == 'Run':\n",
        "        OBSERVE = 999999999\t#We keep observe, never train\n",
        "        epsilon = FINAL_EPSILON # higher epsilon, more timestamps?\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/My Drive/CN_Breakout/models/model_v1.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "\n",
        "    elif args['mode'] == 'CTrain': #Continue previous train\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = 0.07823368810419994 #0.08811709480229288\n",
        "        epsilon = args['epsilon']\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/My Drive/CN_Breakout/models/model.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        \n",
        "\n",
        "    else:\t\t\t\t\t   #We go to training mode -> -m \"Train\"\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = INITIAL_EPSILON #o EPSILON é o que divide a parte de exploration vs exploitation. se for abaixo de um dado valor é exploration. Caso contrário é exploitation\n",
        "        epsilon = args['epsilon']\n",
        "\n",
        "\n",
        "    lives = 5\n",
        "    r_total = 0\n",
        "    while (lives > 0):\n",
        "    #for i in range(MAX_STEPS_PER_EPISODE):\n",
        "        loss = 0\n",
        "        Q_sa = 0 # Q(s, a) representing the maximum discounted future reward when we perform action a in state s.\n",
        "        action_index = 0\n",
        "        r_t = 0 #reward\n",
        "        a_t = np.zeros([ACTIONS]) #action\n",
        "      \n",
        "        #choose an action epsilon greedy\n",
        "        if t % FRAME_PER_ACTION == 0:\n",
        "            if random.random() <= epsilon:\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
        "                a_t[action_index] = 1\n",
        "                \n",
        "            else:\n",
        "                q = model.predict(s_t)\t   #input a stack of 4 images, get the prediction\n",
        "                max_Q = np.argmax(q)\n",
        "                action_index = max_Q\n",
        "                a_t[max_Q] = 1\n",
        "\n",
        "        #We reduce the epsilon gradually\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        #run the selected action and observed next state and reward. DEPOIS DE UM \"STEP\" correr sempre o \"RENDER\"\n",
        "        x_t1_colored, r_t, terminal, info = env.step(list(a_t).index(1) + 1) #FUNÇÂO \"WHERE\" para obter o índice do valor do array que está a 1\n",
        "\n",
        "        if info['ale.lives'] < lives:\n",
        "          lives -= 1\n",
        "          r_t = -1.0\n",
        "          if info['ale.lives'] > 0:\n",
        "            env.step(1)\n",
        "\n",
        "        env.render()\n",
        "        \n",
        "        x_t1 = reshape(x_t1_colored)\n",
        "\n",
        "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis = 3)\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        #only train if done observing\n",
        "        if t > OBSERVE: #train ou update da nossa rede. de quantas em quantas frames vamos precisar para fazer um treino. se replay_mem começar a ficar mto cheio retira a última entrada. e fazemos append das novas decisoes que foram sendo adquiridas\n",
        "            #sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "\n",
        "            #Now we do the experience replay\n",
        "            state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "            state_t = np.concatenate(state_t)\n",
        "            state_t1 = np.concatenate(state_t1)\n",
        "            targets = model.predict(state_t)\n",
        "            Q_sa = model.predict(state_t1)\n",
        "            targets[range(BATCH), action_t] = reward_t + GAMMA * np.max(Q_sa, axis = 1) * np.invert(terminal) #qual o target associado\n",
        "            \n",
        "            \n",
        "            loss += model.train_on_batch(state_t, targets) #quanto mais proximo de zero, mais proximo está de convergir para conseguir estimar o key value de acordo com o par (estado, ação)\n",
        "            \n",
        "        s_t = s_t1\n",
        "        t = t + 1\n",
        "\n",
        "        # save progress every 1000 iterations\n",
        "        if t % 100 == 0:\n",
        "            print(\"Now we save model\")\n",
        "            model.save_weights(\"/content/drive/My Drive/CN_Breakout/models/model.h5\", overwrite = True)\n",
        "            with open(\"model.json\", \"w\") as outfile:\n",
        "                json.dump(model.to_json(), outfile)\n",
        "\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
        "\n",
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
        "        \n",
        "        q_max_list.append(np.max(Q_sa))\n",
        "        loss_list.append(loss)\n",
        "        reward_list.append(r_t)\n",
        "\n",
        "    env.close()\n",
        "    writeFiles(q_max_list, loss_list, reward_list)\n",
        "    \n",
        "        \n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")\n",
        "    return t, epsilon, D\n",
        "\n",
        "\n",
        "def playGame(args):\n",
        "    model = buildmodel('relu', 'same', LEARNING_RATE, 'mse')\n",
        "    t, epsilon, D = trainNetwork(model,args)\n",
        "    return t, epsilon, D\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description = 'Description of your program')\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True)\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True) adicionar o argumento de número de episódios\n",
        "    #args = vars(parser.parse_args())\n",
        "    t = 0\n",
        "    epsilon = INITIAL_EPSILON\n",
        "    D = deque()\n",
        "    for i in range(EPISODES):\n",
        "        print(\"EPISODE\", i)\n",
        "        tp, epsilonp, Dp = playGame({'mode': 'CTrain', 't': t, 'epsilon': epsilon, 'D': D})\n",
        "        t = tp\n",
        "        epsilon = epsilonp\n",
        "        D = Dp\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Viewer requires Qt\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPISODE 0\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 201 / STATE explore / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 202 / STATE explore / EPSILON 0.0099967 / ACTION 1 / REWARD 0.0 / Q_MAX  8.73625 / Loss  0.04477382078766823\n",
            "TIMESTEP 203 / STATE explore / EPSILON 0.009993400000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  7.999029 / Loss  0.01272568665444851\n",
            "TIMESTEP 204 / STATE explore / EPSILON 0.009990100000000002 / ACTION 2 / REWARD 0.0 / Q_MAX  7.810622 / Loss  0.015492160804569721\n",
            "TIMESTEP 205 / STATE explore / EPSILON 0.009986800000000002 / ACTION 2 / REWARD 0.0 / Q_MAX  7.557529 / Loss  0.011050219647586346\n",
            "TIMESTEP 206 / STATE explore / EPSILON 0.009983500000000003 / ACTION 1 / REWARD 1.0 / Q_MAX  7.2919464 / Loss  0.04415169358253479\n",
            "TIMESTEP 207 / STATE explore / EPSILON 0.009980200000000003 / ACTION 2 / REWARD 0.0 / Q_MAX  6.9385624 / Loss  0.0068075042217969894\n",
            "TIMESTEP 208 / STATE explore / EPSILON 0.009976900000000004 / ACTION 1 / REWARD 0.0 / Q_MAX  6.777623 / Loss  0.0045580253936350346\n",
            "TIMESTEP 209 / STATE explore / EPSILON 0.009973600000000004 / ACTION 2 / REWARD 0.0 / Q_MAX  6.468873 / Loss  0.006837866269052029\n",
            "TIMESTEP 210 / STATE explore / EPSILON 0.009970300000000005 / ACTION 2 / REWARD 0.0 / Q_MAX  6.2361927 / Loss  0.00499243289232254\n",
            "TIMESTEP 211 / STATE explore / EPSILON 0.009967000000000005 / ACTION 2 / REWARD 0.0 / Q_MAX  5.834503 / Loss  0.012782390229403973\n",
            "TIMESTEP 212 / STATE explore / EPSILON 0.009963700000000006 / ACTION 2 / REWARD 0.0 / Q_MAX  5.871811 / Loss  0.007422465831041336\n",
            "TIMESTEP 213 / STATE explore / EPSILON 0.009960400000000006 / ACTION 2 / REWARD 0.0 / Q_MAX  5.063057 / Loss  0.007845144718885422\n",
            "TIMESTEP 214 / STATE explore / EPSILON 0.009957100000000007 / ACTION 2 / REWARD 0.0 / Q_MAX  5.5516744 / Loss  0.004349063616245985\n",
            "TIMESTEP 215 / STATE explore / EPSILON 0.009953800000000007 / ACTION 2 / REWARD 0.0 / Q_MAX  5.2647448 / Loss  0.009958707727491856\n",
            "TIMESTEP 216 / STATE explore / EPSILON 0.009950500000000008 / ACTION 2 / REWARD 0.0 / Q_MAX  5.089821 / Loss  0.005903544835746288\n",
            "TIMESTEP 217 / STATE explore / EPSILON 0.009947200000000008 / ACTION 2 / REWARD 0.0 / Q_MAX  4.528677 / Loss  0.006222096271812916\n",
            "TIMESTEP 218 / STATE explore / EPSILON 0.009943900000000009 / ACTION 2 / REWARD 0.0 / Q_MAX  4.4182115 / Loss  0.005052835680544376\n",
            "TIMESTEP 219 / STATE explore / EPSILON 0.00994060000000001 / ACTION 2 / REWARD 0.0 / Q_MAX  4.4196653 / Loss  0.012778456322848797\n",
            "TIMESTEP 220 / STATE explore / EPSILON 0.00993730000000001 / ACTION 2 / REWARD 0.0 / Q_MAX  4.040408 / Loss  0.003974621184170246\n",
            "TIMESTEP 221 / STATE explore / EPSILON 0.00993400000000001 / ACTION 2 / REWARD 0.0 / Q_MAX  3.921467 / Loss  0.01519692037254572\n",
            "TIMESTEP 222 / STATE explore / EPSILON 0.009930700000000011 / ACTION 0 / REWARD 0.0 / Q_MAX  3.9249737 / Loss  0.005661386996507645\n",
            "TIMESTEP 223 / STATE explore / EPSILON 0.009927400000000012 / ACTION 0 / REWARD 0.0 / Q_MAX  3.894155 / Loss  0.00967482104897499\n",
            "TIMESTEP 224 / STATE explore / EPSILON 0.009924100000000012 / ACTION 2 / REWARD 0.0 / Q_MAX  3.6591039 / Loss  0.020213618874549866\n",
            "TIMESTEP 225 / STATE explore / EPSILON 0.009920800000000013 / ACTION 2 / REWARD 0.0 / Q_MAX  3.328519 / Loss  0.005757484119385481\n",
            "TIMESTEP 226 / STATE explore / EPSILON 0.009917500000000013 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0677614 / Loss  0.003761834464967251\n",
            "TIMESTEP 227 / STATE explore / EPSILON 0.009914200000000014 / ACTION 2 / REWARD 0.0 / Q_MAX  3.16288 / Loss  0.006047368980944157\n",
            "TIMESTEP 228 / STATE explore / EPSILON 0.009910900000000014 / ACTION 2 / REWARD -1.0 / Q_MAX  2.9636862 / Loss  0.008652118034660816\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 1\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 229 / STATE explore / EPSILON 0.009907600000000015 / ACTION 0 / REWARD 0.0 / Q_MAX  8.578112 / Loss  0.012772079557180405\n",
            "TIMESTEP 230 / STATE explore / EPSILON 0.009904300000000015 / ACTION 0 / REWARD 0.0 / Q_MAX  8.118502 / Loss  0.011294418014585972\n",
            "TIMESTEP 231 / STATE explore / EPSILON 0.009901000000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  7.851182 / Loss  0.01253480277955532\n",
            "TIMESTEP 232 / STATE explore / EPSILON 0.009897700000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  7.1008124 / Loss  0.03609749302268028\n",
            "TIMESTEP 233 / STATE explore / EPSILON 0.009894400000000017 / ACTION 0 / REWARD 0.0 / Q_MAX  6.956506 / Loss  0.007574884220957756\n",
            "TIMESTEP 234 / STATE explore / EPSILON 0.009891100000000017 / ACTION 0 / REWARD 0.0 / Q_MAX  7.1525254 / Loss  0.014514528214931488\n",
            "TIMESTEP 235 / STATE explore / EPSILON 0.009887800000000018 / ACTION 2 / REWARD 0.0 / Q_MAX  6.341532 / Loss  0.03852561488747597\n",
            "TIMESTEP 236 / STATE explore / EPSILON 0.009884500000000018 / ACTION 2 / REWARD 0.0 / Q_MAX  6.7311606 / Loss  0.010459833778440952\n",
            "TIMESTEP 237 / STATE explore / EPSILON 0.009881200000000019 / ACTION 0 / REWARD 0.0 / Q_MAX  5.5846615 / Loss  0.005594369024038315\n",
            "TIMESTEP 238 / STATE explore / EPSILON 0.00987790000000002 / ACTION 2 / REWARD 0.0 / Q_MAX  6.380131 / Loss  0.09033941477537155\n",
            "TIMESTEP 239 / STATE explore / EPSILON 0.00987460000000002 / ACTION 2 / REWARD 0.0 / Q_MAX  5.768862 / Loss  0.005991116166114807\n",
            "TIMESTEP 240 / STATE explore / EPSILON 0.00987130000000002 / ACTION 0 / REWARD 0.0 / Q_MAX  5.7599964 / Loss  0.007515920326113701\n",
            "TIMESTEP 241 / STATE explore / EPSILON 0.00986800000000002 / ACTION 1 / REWARD 0.0 / Q_MAX  5.806885 / Loss  0.009144711308181286\n",
            "TIMESTEP 242 / STATE explore / EPSILON 0.009864700000000021 / ACTION 2 / REWARD 0.0 / Q_MAX  5.596465 / Loss  0.007277818862348795\n",
            "TIMESTEP 243 / STATE explore / EPSILON 0.009861400000000022 / ACTION 1 / REWARD 0.0 / Q_MAX  4.9581957 / Loss  0.008853849023580551\n",
            "TIMESTEP 244 / STATE explore / EPSILON 0.009858100000000022 / ACTION 1 / REWARD 0.0 / Q_MAX  5.591312 / Loss  0.007794391829520464\n",
            "TIMESTEP 245 / STATE explore / EPSILON 0.009854800000000023 / ACTION 1 / REWARD 0.0 / Q_MAX  5.1473446 / Loss  0.007243207655847073\n",
            "TIMESTEP 246 / STATE explore / EPSILON 0.009851500000000023 / ACTION 0 / REWARD 0.0 / Q_MAX  5.3615527 / Loss  0.004786398261785507\n",
            "TIMESTEP 247 / STATE explore / EPSILON 0.009848200000000024 / ACTION 0 / REWARD 0.0 / Q_MAX  5.398929 / Loss  0.0045906840823590755\n",
            "TIMESTEP 248 / STATE explore / EPSILON 0.009844900000000024 / ACTION 0 / REWARD 0.0 / Q_MAX  5.240491 / Loss  0.009422946721315384\n",
            "TIMESTEP 249 / STATE explore / EPSILON 0.009841600000000025 / ACTION 0 / REWARD 0.0 / Q_MAX  4.6784625 / Loss  0.006020992528647184\n",
            "TIMESTEP 250 / STATE explore / EPSILON 0.009838300000000025 / ACTION 0 / REWARD 0.0 / Q_MAX  5.221102 / Loss  0.016909068450331688\n",
            "TIMESTEP 251 / STATE explore / EPSILON 0.009835000000000026 / ACTION 0 / REWARD 0.0 / Q_MAX  5.0161195 / Loss  0.006540464237332344\n",
            "TIMESTEP 252 / STATE explore / EPSILON 0.009831700000000027 / ACTION 0 / REWARD -1.0 / Q_MAX  4.871705 / Loss  0.01980314590036869\n",
            "TIMESTEP 253 / STATE explore / EPSILON 0.009828400000000027 / ACTION 2 / REWARD 0.0 / Q_MAX  4.860562 / Loss  0.00409251032397151\n",
            "TIMESTEP 254 / STATE explore / EPSILON 0.009825100000000028 / ACTION 1 / REWARD 0.0 / Q_MAX  4.8902445 / Loss  0.004402900114655495\n",
            "TIMESTEP 255 / STATE explore / EPSILON 0.009821800000000028 / ACTION 1 / REWARD 0.0 / Q_MAX  4.8155727 / Loss  0.003614777000620961\n",
            "TIMESTEP 256 / STATE explore / EPSILON 0.009818500000000029 / ACTION 2 / REWARD 0.0 / Q_MAX  4.8719873 / Loss  0.005346720572561026\n",
            "TIMESTEP 257 / STATE explore / EPSILON 0.009815200000000029 / ACTION 1 / REWARD 0.0 / Q_MAX  4.3188396 / Loss  0.0063590239733457565\n",
            "TIMESTEP 258 / STATE explore / EPSILON 0.00981190000000003 / ACTION 1 / REWARD 0.0 / Q_MAX  4.626209 / Loss  0.0072792936116456985\n",
            "TIMESTEP 259 / STATE explore / EPSILON 0.00980860000000003 / ACTION 1 / REWARD 0.0 / Q_MAX  4.394107 / Loss  0.016740629449486732\n",
            "TIMESTEP 260 / STATE explore / EPSILON 0.00980530000000003 / ACTION 2 / REWARD 0.0 / Q_MAX  4.1399107 / Loss  0.003758429316803813\n",
            "TIMESTEP 261 / STATE explore / EPSILON 0.009802000000000031 / ACTION 0 / REWARD 0.0 / Q_MAX  4.4117084 / Loss  0.010445544496178627\n",
            "TIMESTEP 262 / STATE explore / EPSILON 0.009798700000000032 / ACTION 0 / REWARD 0.0 / Q_MAX  4.337525 / Loss  0.01068201195448637\n",
            "TIMESTEP 263 / STATE explore / EPSILON 0.009795400000000032 / ACTION 0 / REWARD 0.0 / Q_MAX  4.245796 / Loss  0.005477378144860268\n",
            "TIMESTEP 264 / STATE explore / EPSILON 0.009792100000000033 / ACTION 0 / REWARD 0.0 / Q_MAX  3.918285 / Loss  0.005890478380024433\n",
            "TIMESTEP 265 / STATE explore / EPSILON 0.009788800000000033 / ACTION 0 / REWARD 0.0 / Q_MAX  4.100515 / Loss  0.01581493765115738\n",
            "TIMESTEP 266 / STATE explore / EPSILON 0.009785500000000034 / ACTION 0 / REWARD 0.0 / Q_MAX  3.9618466 / Loss  0.011058719828724861\n",
            "TIMESTEP 267 / STATE explore / EPSILON 0.009782200000000034 / ACTION 2 / REWARD 0.0 / Q_MAX  3.709736 / Loss  0.004271923564374447\n",
            "TIMESTEP 268 / STATE explore / EPSILON 0.009778900000000035 / ACTION 2 / REWARD 0.0 / Q_MAX  3.618668 / Loss  0.007532570511102676\n",
            "TIMESTEP 269 / STATE explore / EPSILON 0.009775600000000035 / ACTION 2 / REWARD 0.0 / Q_MAX  3.7131329 / Loss  0.005434960592538118\n",
            "TIMESTEP 270 / STATE explore / EPSILON 0.009772300000000036 / ACTION 1 / REWARD 0.0 / Q_MAX  3.6072736 / Loss  0.016649339348077774\n",
            "TIMESTEP 271 / STATE explore / EPSILON 0.009769000000000036 / ACTION 1 / REWARD 0.0 / Q_MAX  3.5210154 / Loss  0.00583745026960969\n",
            "----------Random Action----------\n",
            "TIMESTEP 272 / STATE explore / EPSILON 0.009765700000000037 / ACTION 1 / REWARD 0.0 / Q_MAX  3.263699 / Loss  0.004510289058089256\n",
            "TIMESTEP 273 / STATE explore / EPSILON 0.009762400000000037 / ACTION 0 / REWARD 0.0 / Q_MAX  3.0532072 / Loss  0.00646569300442934\n",
            "TIMESTEP 274 / STATE explore / EPSILON 0.009759100000000038 / ACTION 0 / REWARD 0.0 / Q_MAX  3.2741299 / Loss  0.004636878147721291\n",
            "TIMESTEP 275 / STATE explore / EPSILON 0.009755800000000038 / ACTION 0 / REWARD -1.0 / Q_MAX  3.0735216 / Loss  0.013596021570265293\n",
            "TIMESTEP 276 / STATE explore / EPSILON 0.009752500000000039 / ACTION 1 / REWARD 0.0 / Q_MAX  2.9685056 / Loss  0.012824333272874355\n",
            "TIMESTEP 277 / STATE explore / EPSILON 0.00974920000000004 / ACTION 1 / REWARD 0.0 / Q_MAX  2.9124162 / Loss  0.01994188129901886\n",
            "TIMESTEP 278 / STATE explore / EPSILON 0.00974590000000004 / ACTION 1 / REWARD 0.0 / Q_MAX  2.9590635 / Loss  0.007392584346234798\n",
            "TIMESTEP 279 / STATE explore / EPSILON 0.00974260000000004 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8084004 / Loss  0.011159438639879227\n",
            "TIMESTEP 280 / STATE explore / EPSILON 0.009739300000000041 / ACTION 1 / REWARD 0.0 / Q_MAX  2.7608728 / Loss  0.006610572803765535\n",
            "TIMESTEP 281 / STATE explore / EPSILON 0.009736000000000041 / ACTION 1 / REWARD 0.0 / Q_MAX  2.7453394 / Loss  0.003859730903059244\n",
            "TIMESTEP 282 / STATE explore / EPSILON 0.009732700000000042 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8287752 / Loss  0.003256298368796706\n",
            "TIMESTEP 283 / STATE explore / EPSILON 0.009729400000000043 / ACTION 1 / REWARD 0.0 / Q_MAX  2.9077957 / Loss  0.01023772545158863\n",
            "TIMESTEP 284 / STATE explore / EPSILON 0.009726100000000043 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8417788 / Loss  0.027567017823457718\n",
            "TIMESTEP 285 / STATE explore / EPSILON 0.009722800000000044 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8523688 / Loss  0.008269048295915127\n",
            "TIMESTEP 286 / STATE explore / EPSILON 0.009719500000000044 / ACTION 1 / REWARD 0.0 / Q_MAX  2.3251195 / Loss  0.01812775805592537\n",
            "TIMESTEP 287 / STATE explore / EPSILON 0.009716200000000045 / ACTION 1 / REWARD 0.0 / Q_MAX  2.6810193 / Loss  0.005913485772907734\n",
            "TIMESTEP 288 / STATE explore / EPSILON 0.009712900000000045 / ACTION 1 / REWARD 0.0 / Q_MAX  2.7272117 / Loss  0.008972871117293835\n",
            "TIMESTEP 289 / STATE explore / EPSILON 0.009709600000000046 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8211675 / Loss  0.003375986125320196\n",
            "----------Random Action----------\n",
            "TIMESTEP 290 / STATE explore / EPSILON 0.009706300000000046 / ACTION 1 / REWARD 0.0 / Q_MAX  2.761519 / Loss  0.016559554263949394\n",
            "TIMESTEP 291 / STATE explore / EPSILON 0.009703000000000047 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8603475 / Loss  0.004255399573594332\n",
            "TIMESTEP 292 / STATE explore / EPSILON 0.009699700000000047 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8384829 / Loss  0.004001321271061897\n",
            "TIMESTEP 293 / STATE explore / EPSILON 0.009696400000000048 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8990839 / Loss  0.0038306154310703278\n",
            "TIMESTEP 294 / STATE explore / EPSILON 0.009693100000000048 / ACTION 1 / REWARD 0.0 / Q_MAX  2.988518 / Loss  0.0027415836229920387\n",
            "TIMESTEP 295 / STATE explore / EPSILON 0.009689800000000049 / ACTION 1 / REWARD 0.0 / Q_MAX  2.881874 / Loss  0.005799643695354462\n",
            "TIMESTEP 296 / STATE explore / EPSILON 0.00968650000000005 / ACTION 1 / REWARD 0.0 / Q_MAX  2.9829528 / Loss  0.0037075921427458525\n",
            "TIMESTEP 297 / STATE explore / EPSILON 0.00968320000000005 / ACTION 1 / REWARD 0.0 / Q_MAX  3.060203 / Loss  0.007245018146932125\n",
            "TIMESTEP 298 / STATE explore / EPSILON 0.00967990000000005 / ACTION 1 / REWARD -1.0 / Q_MAX  3.0929008 / Loss  0.00455852597951889\n",
            "TIMESTEP 299 / STATE explore / EPSILON 0.00967660000000005 / ACTION 0 / REWARD 0.0 / Q_MAX  3.1544666 / Loss  0.002059002872556448\n",
            "Now we save model\n",
            "TIMESTEP 300 / STATE explore / EPSILON 0.009673300000000051 / ACTION 0 / REWARD 0.0 / Q_MAX  3.3166382 / Loss  0.00246178125962615\n",
            "TIMESTEP 301 / STATE explore / EPSILON 0.009670000000000052 / ACTION 0 / REWARD 0.0 / Q_MAX  3.1973686 / Loss  0.0037234800402075052\n",
            "TIMESTEP 302 / STATE explore / EPSILON 0.009666700000000052 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9791675 / Loss  0.004032979719340801\n",
            "TIMESTEP 303 / STATE explore / EPSILON 0.009663400000000053 / ACTION 2 / REWARD 0.0 / Q_MAX  3.3144233 / Loss  0.0043948800303041935\n",
            "TIMESTEP 304 / STATE explore / EPSILON 0.009660100000000053 / ACTION 2 / REWARD 0.0 / Q_MAX  3.3665833 / Loss  0.005791699513792992\n",
            "TIMESTEP 305 / STATE explore / EPSILON 0.009656800000000054 / ACTION 2 / REWARD 0.0 / Q_MAX  3.414791 / Loss  0.005827009677886963\n",
            "TIMESTEP 306 / STATE explore / EPSILON 0.009653500000000054 / ACTION 2 / REWARD 0.0 / Q_MAX  3.4527135 / Loss  0.0030831564217805862\n",
            "TIMESTEP 307 / STATE explore / EPSILON 0.009650200000000055 / ACTION 2 / REWARD 0.0 / Q_MAX  3.4285698 / Loss  0.04269637539982796\n",
            "TIMESTEP 308 / STATE explore / EPSILON 0.009646900000000055 / ACTION 2 / REWARD 0.0 / Q_MAX  3.1999793 / Loss  0.004786846227943897\n",
            "TIMESTEP 309 / STATE explore / EPSILON 0.009643600000000056 / ACTION 2 / REWARD 0.0 / Q_MAX  3.223106 / Loss  0.004602482076734304\n",
            "TIMESTEP 310 / STATE explore / EPSILON 0.009640300000000056 / ACTION 2 / REWARD 0.0 / Q_MAX  3.252985 / Loss  0.00645811902359128\n",
            "TIMESTEP 311 / STATE explore / EPSILON 0.009637000000000057 / ACTION 2 / REWARD 0.0 / Q_MAX  3.1420913 / Loss  0.0019119762582704425\n",
            "TIMESTEP 312 / STATE explore / EPSILON 0.009633700000000057 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0044422 / Loss  0.00608940701931715\n",
            "TIMESTEP 313 / STATE explore / EPSILON 0.009630400000000058 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0439327 / Loss  0.006162512116134167\n",
            "TIMESTEP 314 / STATE explore / EPSILON 0.009627100000000059 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9687617 / Loss  0.0023659449070692062\n",
            "TIMESTEP 315 / STATE explore / EPSILON 0.009623800000000059 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9084888 / Loss  0.007465994451195002\n",
            "TIMESTEP 316 / STATE explore / EPSILON 0.00962050000000006 / ACTION 2 / REWARD 0.0 / Q_MAX  2.844459 / Loss  0.0017896172357723117\n",
            "----------Random Action----------\n",
            "TIMESTEP 317 / STATE explore / EPSILON 0.00961720000000006 / ACTION 2 / REWARD 0.0 / Q_MAX  2.678102 / Loss  0.002076547360047698\n",
            "TIMESTEP 318 / STATE explore / EPSILON 0.00961390000000006 / ACTION 1 / REWARD 0.0 / Q_MAX  2.763088 / Loss  0.0025822045281529427\n",
            "TIMESTEP 319 / STATE explore / EPSILON 0.009610600000000061 / ACTION 1 / REWARD 0.0 / Q_MAX  2.6430037 / Loss  0.013354532420635223\n",
            "TIMESTEP 320 / STATE explore / EPSILON 0.009607300000000062 / ACTION 2 / REWARD 0.0 / Q_MAX  2.667522 / Loss  0.0025245705619454384\n",
            "TIMESTEP 321 / STATE explore / EPSILON 0.009604000000000062 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5356364 / Loss  0.003900315146893263\n",
            "TIMESTEP 322 / STATE explore / EPSILON 0.009600700000000063 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5339346 / Loss  0.001359323738142848\n",
            "TIMESTEP 323 / STATE explore / EPSILON 0.009597400000000063 / ACTION 1 / REWARD 0.0 / Q_MAX  2.526533 / Loss  0.0014497494557872415\n",
            "TIMESTEP 324 / STATE explore / EPSILON 0.009594100000000064 / ACTION 1 / REWARD 0.0 / Q_MAX  2.5594857 / Loss  0.007742403540760279\n",
            "TIMESTEP 325 / STATE explore / EPSILON 0.009590800000000064 / ACTION 1 / REWARD 0.0 / Q_MAX  2.4381208 / Loss  0.002548724412918091\n",
            "TIMESTEP 326 / STATE explore / EPSILON 0.009587500000000065 / ACTION 0 / REWARD 0.0 / Q_MAX  2.4910438 / Loss  0.0018089492805302143\n",
            "TIMESTEP 327 / STATE explore / EPSILON 0.009584200000000065 / ACTION 0 / REWARD 0.0 / Q_MAX  2.5637958 / Loss  0.006376820150762796\n",
            "TIMESTEP 328 / STATE explore / EPSILON 0.009580900000000066 / ACTION 0 / REWARD 0.0 / Q_MAX  2.5921059 / Loss  0.010846611112356186\n",
            "TIMESTEP 329 / STATE explore / EPSILON 0.009577600000000066 / ACTION 0 / REWARD 1.0 / Q_MAX  2.6018834 / Loss  0.0021088551729917526\n",
            "TIMESTEP 330 / STATE explore / EPSILON 0.009574300000000067 / ACTION 0 / REWARD 0.0 / Q_MAX  2.5621033 / Loss  0.0027716271579265594\n",
            "TIMESTEP 331 / STATE explore / EPSILON 0.009571000000000067 / ACTION 0 / REWARD 0.0 / Q_MAX  2.5818572 / Loss  0.0026156138628721237\n",
            "TIMESTEP 332 / STATE explore / EPSILON 0.009567700000000068 / ACTION 0 / REWARD 0.0 / Q_MAX  2.4621122 / Loss  0.0017092943890020251\n",
            "TIMESTEP 333 / STATE explore / EPSILON 0.009564400000000068 / ACTION 0 / REWARD 0.0 / Q_MAX  2.499672 / Loss  0.04078738018870354\n",
            "TIMESTEP 334 / STATE explore / EPSILON 0.009561100000000069 / ACTION 0 / REWARD 0.0 / Q_MAX  2.5045266 / Loss  0.002057038713246584\n",
            "TIMESTEP 335 / STATE explore / EPSILON 0.00955780000000007 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5479176 / Loss  0.004353699274361134\n",
            "TIMESTEP 336 / STATE explore / EPSILON 0.00955450000000007 / ACTION 1 / REWARD 0.0 / Q_MAX  2.4969728 / Loss  0.004759486299008131\n",
            "TIMESTEP 337 / STATE explore / EPSILON 0.00955120000000007 / ACTION 1 / REWARD 0.0 / Q_MAX  2.4833891 / Loss  0.006795348599553108\n",
            "TIMESTEP 338 / STATE explore / EPSILON 0.009547900000000071 / ACTION 2 / REWARD 0.0 / Q_MAX  2.55189 / Loss  0.004511605948209763\n",
            "TIMESTEP 339 / STATE explore / EPSILON 0.009544600000000071 / ACTION 2 / REWARD 0.0 / Q_MAX  2.4422257 / Loss  0.0021208978723734617\n",
            "TIMESTEP 340 / STATE explore / EPSILON 0.009541300000000072 / ACTION 1 / REWARD 0.0 / Q_MAX  2.547814 / Loss  0.00441167363896966\n",
            "TIMESTEP 341 / STATE explore / EPSILON 0.009538000000000072 / ACTION 1 / REWARD 0.0 / Q_MAX  2.4733639 / Loss  0.0035765988286584616\n",
            "TIMESTEP 342 / STATE explore / EPSILON 0.009534700000000073 / ACTION 2 / REWARD 0.0 / Q_MAX  2.6625738 / Loss  0.0027014808729290962\n",
            "TIMESTEP 343 / STATE explore / EPSILON 0.009531400000000073 / ACTION 2 / REWARD 0.0 / Q_MAX  2.468588 / Loss  0.0036619720049202442\n",
            "TIMESTEP 344 / STATE explore / EPSILON 0.009528100000000074 / ACTION 1 / REWARD 0.0 / Q_MAX  2.6118193 / Loss  0.003224396612495184\n",
            "TIMESTEP 345 / STATE explore / EPSILON 0.009524800000000075 / ACTION 1 / REWARD 0.0 / Q_MAX  2.5325367 / Loss  0.0015884956810623407\n",
            "----------Random Action----------\n",
            "TIMESTEP 346 / STATE explore / EPSILON 0.009521500000000075 / ACTION 1 / REWARD 0.0 / Q_MAX  2.6845636 / Loss  0.006733912043273449\n",
            "TIMESTEP 347 / STATE explore / EPSILON 0.009518200000000076 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7050085 / Loss  0.0030611231923103333\n",
            "TIMESTEP 348 / STATE explore / EPSILON 0.009514900000000076 / ACTION 1 / REWARD 0.0 / Q_MAX  2.7324297 / Loss  0.002214564708992839\n",
            "TIMESTEP 349 / STATE explore / EPSILON 0.009511600000000077 / ACTION 2 / REWARD 0.0 / Q_MAX  2.4669266 / Loss  0.004638673737645149\n",
            "TIMESTEP 350 / STATE explore / EPSILON 0.009508300000000077 / ACTION 2 / REWARD -1.0 / Q_MAX  2.9639964 / Loss  0.0024501471780240536\n",
            "TIMESTEP 351 / STATE explore / EPSILON 0.009505000000000078 / ACTION 0 / REWARD 0.0 / Q_MAX  2.6193314 / Loss  0.0018221999052911997\n",
            "TIMESTEP 352 / STATE explore / EPSILON 0.009501700000000078 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9443986 / Loss  0.002043680287897587\n",
            "TIMESTEP 353 / STATE explore / EPSILON 0.009498400000000079 / ACTION 2 / REWARD 0.0 / Q_MAX  2.625237 / Loss  0.004820064175873995\n",
            "TIMESTEP 354 / STATE explore / EPSILON 0.00949510000000008 / ACTION 2 / REWARD 0.0 / Q_MAX  2.6999266 / Loss  0.008145137690007687\n",
            "TIMESTEP 355 / STATE explore / EPSILON 0.00949180000000008 / ACTION 1 / REWARD 0.0 / Q_MAX  2.7167964 / Loss  0.0024269064888358116\n",
            "TIMESTEP 356 / STATE explore / EPSILON 0.00948850000000008 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8703759 / Loss  0.0016787161584943533\n",
            "TIMESTEP 357 / STATE explore / EPSILON 0.00948520000000008 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7677264 / Loss  0.016076866537332535\n",
            "TIMESTEP 358 / STATE explore / EPSILON 0.009481900000000081 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7144246 / Loss  0.0018992496188730001\n",
            "TIMESTEP 359 / STATE explore / EPSILON 0.009478600000000082 / ACTION 1 / REWARD 0.0 / Q_MAX  2.7508163 / Loss  0.004716968629509211\n",
            "TIMESTEP 360 / STATE explore / EPSILON 0.009475300000000082 / ACTION 2 / REWARD 0.0 / Q_MAX  2.4392173 / Loss  0.0029034991748631\n",
            "TIMESTEP 361 / STATE explore / EPSILON 0.009472000000000083 / ACTION 1 / REWARD 0.0 / Q_MAX  2.6893559 / Loss  0.005480124149471521\n",
            "TIMESTEP 362 / STATE explore / EPSILON 0.009468700000000083 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7466688 / Loss  0.002885180525481701\n",
            "TIMESTEP 363 / STATE explore / EPSILON 0.009465400000000084 / ACTION 2 / REWARD 0.0 / Q_MAX  3.1157634 / Loss  0.002778261434286833\n",
            "TIMESTEP 364 / STATE explore / EPSILON 0.009462100000000084 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5302649 / Loss  0.0029013855382800102\n",
            "TIMESTEP 365 / STATE explore / EPSILON 0.009458800000000085 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8799524 / Loss  0.002751472406089306\n",
            "TIMESTEP 366 / STATE explore / EPSILON 0.009455500000000085 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8114777 / Loss  0.002188793383538723\n",
            "TIMESTEP 367 / STATE explore / EPSILON 0.009452200000000086 / ACTION 2 / REWARD 0.0 / Q_MAX  2.65723 / Loss  0.01379463728517294\n",
            "TIMESTEP 368 / STATE explore / EPSILON 0.009448900000000086 / ACTION 2 / REWARD 0.0 / Q_MAX  2.599204 / Loss  0.005060221068561077\n",
            "TIMESTEP 369 / STATE explore / EPSILON 0.009445600000000087 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0636096 / Loss  0.002549506025388837\n",
            "TIMESTEP 370 / STATE explore / EPSILON 0.009442300000000087 / ACTION 1 / REWARD 0.0 / Q_MAX  2.971622 / Loss  0.006211579777300358\n",
            "TIMESTEP 371 / STATE explore / EPSILON 0.009439000000000088 / ACTION 1 / REWARD 0.0 / Q_MAX  2.5969386 / Loss  0.003055132692679763\n",
            "TIMESTEP 372 / STATE explore / EPSILON 0.009435700000000088 / ACTION 1 / REWARD 0.0 / Q_MAX  2.4282763 / Loss  0.0037220038939267397\n",
            "TIMESTEP 373 / STATE explore / EPSILON 0.009432400000000089 / ACTION 2 / REWARD 0.0 / Q_MAX  2.26845 / Loss  0.000982611090876162\n",
            "TIMESTEP 374 / STATE explore / EPSILON 0.00942910000000009 / ACTION 2 / REWARD -1.0 / Q_MAX  2.4672623 / Loss  0.002391840796917677\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 2\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 375 / STATE explore / EPSILON 0.00942580000000009 / ACTION 2 / REWARD 0.0 / Q_MAX  3.3553321 / Loss  0.002989593194797635\n",
            "TIMESTEP 376 / STATE explore / EPSILON 0.00942250000000009 / ACTION 2 / REWARD 0.0 / Q_MAX  3.511469 / Loss  0.0028924918733537197\n",
            "TIMESTEP 377 / STATE explore / EPSILON 0.009419200000000091 / ACTION 0 / REWARD 0.0 / Q_MAX  3.3619583 / Loss  0.010332283563911915\n",
            "TIMESTEP 378 / STATE explore / EPSILON 0.009415900000000092 / ACTION 2 / REWARD 0.0 / Q_MAX  3.4430108 / Loss  0.03363323211669922\n",
            "TIMESTEP 379 / STATE explore / EPSILON 0.009412600000000092 / ACTION 2 / REWARD 0.0 / Q_MAX  3.2927701 / Loss  0.012129072099924088\n",
            "TIMESTEP 380 / STATE explore / EPSILON 0.009409300000000093 / ACTION 2 / REWARD 0.0 / Q_MAX  3.4370337 / Loss  0.0065844678319990635\n",
            "TIMESTEP 381 / STATE explore / EPSILON 0.009406000000000093 / ACTION 2 / REWARD 0.0 / Q_MAX  3.377884 / Loss  0.02295204997062683\n",
            "TIMESTEP 382 / STATE explore / EPSILON 0.009402700000000094 / ACTION 2 / REWARD 0.0 / Q_MAX  3.276871 / Loss  0.003508397378027439\n",
            "TIMESTEP 383 / STATE explore / EPSILON 0.009399400000000094 / ACTION 2 / REWARD 0.0 / Q_MAX  3.3557184 / Loss  0.0029542618431150913\n",
            "TIMESTEP 384 / STATE explore / EPSILON 0.009396100000000095 / ACTION 2 / REWARD 0.0 / Q_MAX  3.1821218 / Loss  0.00512566976249218\n",
            "TIMESTEP 385 / STATE explore / EPSILON 0.009392800000000095 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0814543 / Loss  0.04054425656795502\n",
            "TIMESTEP 386 / STATE explore / EPSILON 0.009389500000000096 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0700414 / Loss  0.0031534936279058456\n",
            "TIMESTEP 387 / STATE explore / EPSILON 0.009386200000000096 / ACTION 2 / REWARD 0.0 / Q_MAX  2.909123 / Loss  0.002575692255049944\n",
            "TIMESTEP 388 / STATE explore / EPSILON 0.009382900000000097 / ACTION 2 / REWARD 0.0 / Q_MAX  2.8983567 / Loss  0.0018003389704972506\n",
            "TIMESTEP 389 / STATE explore / EPSILON 0.009379600000000097 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7931485 / Loss  0.004798045381903648\n",
            "TIMESTEP 390 / STATE explore / EPSILON 0.009376300000000098 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7421696 / Loss  0.0022251480259001255\n",
            "TIMESTEP 391 / STATE explore / EPSILON 0.009373000000000098 / ACTION 2 / REWARD 0.0 / Q_MAX  2.6854458 / Loss  0.0035733645781874657\n",
            "TIMESTEP 392 / STATE explore / EPSILON 0.009369700000000099 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5988667 / Loss  0.0019618915393948555\n",
            "TIMESTEP 393 / STATE explore / EPSILON 0.0093664000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  2.5925903 / Loss  0.012893635779619217\n",
            "----------Random Action----------\n",
            "TIMESTEP 394 / STATE explore / EPSILON 0.0093631000000001 / ACTION 2 / REWARD 0.0 / Q_MAX  2.412952 / Loss  0.00509280851110816\n",
            "TIMESTEP 395 / STATE explore / EPSILON 0.0093598000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  2.5425935 / Loss  0.008873065002262592\n",
            "TIMESTEP 396 / STATE explore / EPSILON 0.0093565000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  2.346213 / Loss  0.0022271014750003815\n",
            "TIMESTEP 397 / STATE explore / EPSILON 0.009353200000000101 / ACTION 0 / REWARD 0.0 / Q_MAX  2.4554343 / Loss  0.0027122029569000006\n",
            "TIMESTEP 398 / STATE explore / EPSILON 0.009349900000000102 / ACTION 0 / REWARD 0.0 / Q_MAX  2.20043 / Loss  0.004296512342989445\n",
            "TIMESTEP 399 / STATE explore / EPSILON 0.009346600000000102 / ACTION 1 / REWARD 0.0 / Q_MAX  2.0346303 / Loss  0.0047977762296795845\n",
            "Now we save model\n",
            "TIMESTEP 400 / STATE explore / EPSILON 0.009343300000000103 / ACTION 0 / REWARD 0.0 / Q_MAX  2.128658 / Loss  0.012434998527169228\n",
            "TIMESTEP 401 / STATE explore / EPSILON 0.009340000000000103 / ACTION 0 / REWARD 0.0 / Q_MAX  2.0492582 / Loss  0.006447562947869301\n",
            "TIMESTEP 402 / STATE explore / EPSILON 0.009336700000000104 / ACTION 0 / REWARD 0.0 / Q_MAX  1.9866874 / Loss  0.002764886012300849\n",
            "TIMESTEP 403 / STATE explore / EPSILON 0.009333400000000104 / ACTION 0 / REWARD 0.0 / Q_MAX  1.7620256 / Loss  0.006583338137716055\n",
            "TIMESTEP 404 / STATE explore / EPSILON 0.009330100000000105 / ACTION 0 / REWARD 0.0 / Q_MAX  1.6480988 / Loss  0.0038249590434134007\n",
            "TIMESTEP 405 / STATE explore / EPSILON 0.009326800000000105 / ACTION 0 / REWARD 1.0 / Q_MAX  1.8253702 / Loss  0.004087675362825394\n",
            "TIMESTEP 406 / STATE explore / EPSILON 0.009323500000000106 / ACTION 0 / REWARD 0.0 / Q_MAX  1.6285529 / Loss  0.009149815887212753\n",
            "TIMESTEP 407 / STATE explore / EPSILON 0.009320200000000107 / ACTION 0 / REWARD 0.0 / Q_MAX  1.6437916 / Loss  0.002888519549742341\n",
            "TIMESTEP 408 / STATE explore / EPSILON 0.009316900000000107 / ACTION 0 / REWARD 0.0 / Q_MAX  1.5701519 / Loss  0.0019504954107105732\n",
            "TIMESTEP 409 / STATE explore / EPSILON 0.009313600000000108 / ACTION 0 / REWARD 0.0 / Q_MAX  1.5239637 / Loss  0.014013714157044888\n",
            "TIMESTEP 410 / STATE explore / EPSILON 0.009310300000000108 / ACTION 0 / REWARD 0.0 / Q_MAX  1.500355 / Loss  0.002449865685775876\n",
            "TIMESTEP 411 / STATE explore / EPSILON 0.009307000000000109 / ACTION 0 / REWARD 0.0 / Q_MAX  1.6122581 / Loss  0.004050991497933865\n",
            "TIMESTEP 412 / STATE explore / EPSILON 0.009303700000000109 / ACTION 0 / REWARD 0.0 / Q_MAX  1.4705052 / Loss  0.0010580234229564667\n",
            "TIMESTEP 413 / STATE explore / EPSILON 0.00930040000000011 / ACTION 0 / REWARD 0.0 / Q_MAX  1.6641911 / Loss  0.012134382501244545\n",
            "TIMESTEP 414 / STATE explore / EPSILON 0.00929710000000011 / ACTION 0 / REWARD 0.0 / Q_MAX  1.5599667 / Loss  0.002099903766065836\n",
            "TIMESTEP 415 / STATE explore / EPSILON 0.00929380000000011 / ACTION 0 / REWARD 0.0 / Q_MAX  1.6497892 / Loss  0.005897642113268375\n",
            "TIMESTEP 416 / STATE explore / EPSILON 0.009290500000000111 / ACTION 0 / REWARD 0.0 / Q_MAX  1.7021564 / Loss  0.0021501556038856506\n",
            "TIMESTEP 417 / STATE explore / EPSILON 0.009287200000000112 / ACTION 0 / REWARD 0.0 / Q_MAX  1.5989003 / Loss  0.0019054674776270986\n",
            "TIMESTEP 418 / STATE explore / EPSILON 0.009283900000000112 / ACTION 0 / REWARD 0.0 / Q_MAX  1.5929809 / Loss  0.005219185724854469\n",
            "TIMESTEP 419 / STATE explore / EPSILON 0.009280600000000113 / ACTION 0 / REWARD 0.0 / Q_MAX  1.6234769 / Loss  0.0017440862720832229\n",
            "TIMESTEP 420 / STATE explore / EPSILON 0.009277300000000113 / ACTION 0 / REWARD 0.0 / Q_MAX  1.7061145 / Loss  0.0083597581833601\n",
            "TIMESTEP 421 / STATE explore / EPSILON 0.009274000000000114 / ACTION 2 / REWARD 0.0 / Q_MAX  1.7311827 / Loss  0.003305154386907816\n",
            "TIMESTEP 422 / STATE explore / EPSILON 0.009270700000000114 / ACTION 2 / REWARD 0.0 / Q_MAX  1.786638 / Loss  0.009499846026301384\n",
            "TIMESTEP 423 / STATE explore / EPSILON 0.009267400000000115 / ACTION 2 / REWARD 0.0 / Q_MAX  1.6656591 / Loss  0.0027259779162704945\n",
            "TIMESTEP 424 / STATE explore / EPSILON 0.009264100000000115 / ACTION 2 / REWARD 0.0 / Q_MAX  1.8289552 / Loss  0.0026387805119156837\n",
            "TIMESTEP 425 / STATE explore / EPSILON 0.009260800000000116 / ACTION 2 / REWARD 0.0 / Q_MAX  1.9248033 / Loss  0.0028195036575198174\n",
            "TIMESTEP 426 / STATE explore / EPSILON 0.009257500000000116 / ACTION 2 / REWARD -1.0 / Q_MAX  1.9468396 / Loss  0.003938402980566025\n",
            "TIMESTEP 427 / STATE explore / EPSILON 0.009254200000000117 / ACTION 1 / REWARD 0.0 / Q_MAX  1.726133 / Loss  0.006563583388924599\n",
            "TIMESTEP 428 / STATE explore / EPSILON 0.009250900000000117 / ACTION 1 / REWARD 0.0 / Q_MAX  1.7249388 / Loss  0.004639643244445324\n",
            "TIMESTEP 429 / STATE explore / EPSILON 0.009247600000000118 / ACTION 1 / REWARD 0.0 / Q_MAX  2.0036235 / Loss  0.0050175245851278305\n",
            "TIMESTEP 430 / STATE explore / EPSILON 0.009244300000000118 / ACTION 1 / REWARD 0.0 / Q_MAX  2.699997 / Loss  0.0027228451799601316\n",
            "TIMESTEP 431 / STATE explore / EPSILON 0.009241000000000119 / ACTION 1 / REWARD 0.0 / Q_MAX  2.7310746 / Loss  0.004739090334624052\n",
            "TIMESTEP 432 / STATE explore / EPSILON 0.00923770000000012 / ACTION 1 / REWARD 0.0 / Q_MAX  1.8941276 / Loss  0.0012367346789687872\n",
            "TIMESTEP 433 / STATE explore / EPSILON 0.00923440000000012 / ACTION 2 / REWARD 0.0 / Q_MAX  2.0309408 / Loss  0.002460100222378969\n",
            "TIMESTEP 434 / STATE explore / EPSILON 0.00923110000000012 / ACTION 1 / REWARD 0.0 / Q_MAX  2.6632946 / Loss  0.005274536088109016\n",
            "TIMESTEP 435 / STATE explore / EPSILON 0.009227800000000121 / ACTION 2 / REWARD 0.0 / Q_MAX  2.3833718 / Loss  0.003528632689267397\n",
            "TIMESTEP 436 / STATE explore / EPSILON 0.009224500000000121 / ACTION 2 / REWARD 0.0 / Q_MAX  2.091601 / Loss  0.0026910905726253986\n",
            "TIMESTEP 437 / STATE explore / EPSILON 0.009221200000000122 / ACTION 2 / REWARD 0.0 / Q_MAX  2.1672509 / Loss  0.0026358915492892265\n",
            "TIMESTEP 438 / STATE explore / EPSILON 0.009217900000000123 / ACTION 1 / REWARD 0.0 / Q_MAX  2.4075584 / Loss  0.006382653024047613\n",
            "TIMESTEP 439 / STATE explore / EPSILON 0.009214600000000123 / ACTION 1 / REWARD 0.0 / Q_MAX  2.758804 / Loss  0.005082857329398394\n",
            "TIMESTEP 440 / STATE explore / EPSILON 0.009211300000000124 / ACTION 1 / REWARD 0.0 / Q_MAX  2.1921494 / Loss  0.002279409673064947\n",
            "TIMESTEP 441 / STATE explore / EPSILON 0.009208000000000124 / ACTION 1 / REWARD 0.0 / Q_MAX  1.9150635 / Loss  0.0026203684974461794\n",
            "TIMESTEP 442 / STATE explore / EPSILON 0.009204700000000125 / ACTION 2 / REWARD 0.0 / Q_MAX  2.6056833 / Loss  0.008393198251724243\n",
            "TIMESTEP 443 / STATE explore / EPSILON 0.009201400000000125 / ACTION 2 / REWARD 0.0 / Q_MAX  2.4717646 / Loss  0.006071571260690689\n",
            "TIMESTEP 444 / STATE explore / EPSILON 0.009198100000000126 / ACTION 1 / REWARD 0.0 / Q_MAX  2.0271187 / Loss  0.0037345660384744406\n",
            "TIMESTEP 445 / STATE explore / EPSILON 0.009194800000000126 / ACTION 1 / REWARD 0.0 / Q_MAX  1.9930872 / Loss  0.0043957168236374855\n",
            "TIMESTEP 446 / STATE explore / EPSILON 0.009191500000000127 / ACTION 2 / REWARD 0.0 / Q_MAX  1.8596053 / Loss  0.0029894085600972176\n",
            "TIMESTEP 447 / STATE explore / EPSILON 0.009188200000000127 / ACTION 1 / REWARD 0.0 / Q_MAX  2.0018826 / Loss  0.0025170070584863424\n",
            "TIMESTEP 448 / STATE explore / EPSILON 0.009184900000000128 / ACTION 1 / REWARD 0.0 / Q_MAX  1.7700474 / Loss  0.005279148928821087\n",
            "TIMESTEP 449 / STATE explore / EPSILON 0.009181600000000128 / ACTION 2 / REWARD 0.0 / Q_MAX  1.4795595 / Loss  0.00391265656799078\n",
            "TIMESTEP 450 / STATE explore / EPSILON 0.009178300000000129 / ACTION 2 / REWARD -1.0 / Q_MAX  1.76819 / Loss  0.012760793790221214\n",
            "TIMESTEP 451 / STATE explore / EPSILON 0.00917500000000013 / ACTION 0 / REWARD 0.0 / Q_MAX  1.7490366 / Loss  0.0029232194647192955\n",
            "TIMESTEP 452 / STATE explore / EPSILON 0.00917170000000013 / ACTION 0 / REWARD 0.0 / Q_MAX  1.726395 / Loss  0.004091699607670307\n",
            "TIMESTEP 453 / STATE explore / EPSILON 0.00916840000000013 / ACTION 0 / REWARD 0.0 / Q_MAX  1.6302527 / Loss  0.005000373814254999\n",
            "TIMESTEP 454 / STATE explore / EPSILON 0.00916510000000013 / ACTION 2 / REWARD 0.0 / Q_MAX  1.5950344 / Loss  0.0014663892798125744\n",
            "TIMESTEP 455 / STATE explore / EPSILON 0.009161800000000131 / ACTION 2 / REWARD 0.0 / Q_MAX  1.7014487 / Loss  0.007604999467730522\n",
            "TIMESTEP 456 / STATE explore / EPSILON 0.009158500000000132 / ACTION 2 / REWARD 0.0 / Q_MAX  1.2706105 / Loss  0.003721734741702676\n",
            "TIMESTEP 457 / STATE explore / EPSILON 0.009155200000000132 / ACTION 2 / REWARD 0.0 / Q_MAX  1.2004632 / Loss  0.006085512228310108\n",
            "TIMESTEP 458 / STATE explore / EPSILON 0.009151900000000133 / ACTION 2 / REWARD 0.0 / Q_MAX  1.3408784 / Loss  0.004388714674860239\n",
            "TIMESTEP 459 / STATE explore / EPSILON 0.009148600000000133 / ACTION 2 / REWARD 0.0 / Q_MAX  1.1993773 / Loss  0.0019414928974583745\n",
            "TIMESTEP 460 / STATE explore / EPSILON 0.009145300000000134 / ACTION 2 / REWARD 0.0 / Q_MAX  1.6177945 / Loss  0.0037585441023111343\n",
            "TIMESTEP 461 / STATE explore / EPSILON 0.009142000000000134 / ACTION 2 / REWARD 0.0 / Q_MAX  1.1489646 / Loss  0.0030759572982788086\n",
            "TIMESTEP 462 / STATE explore / EPSILON 0.009138700000000135 / ACTION 2 / REWARD 0.0 / Q_MAX  1.169548 / Loss  0.005625030025839806\n",
            "TIMESTEP 463 / STATE explore / EPSILON 0.009135400000000135 / ACTION 2 / REWARD 0.0 / Q_MAX  1.1783719 / Loss  0.008435377851128578\n",
            "TIMESTEP 464 / STATE explore / EPSILON 0.009132100000000136 / ACTION 2 / REWARD 0.0 / Q_MAX  1.5558385 / Loss  0.0063859326764941216\n",
            "TIMESTEP 465 / STATE explore / EPSILON 0.009128800000000136 / ACTION 2 / REWARD 0.0 / Q_MAX  1.5210743 / Loss  0.008367919363081455\n",
            "TIMESTEP 466 / STATE explore / EPSILON 0.009125500000000137 / ACTION 2 / REWARD 0.0 / Q_MAX  1.1107923 / Loss  0.0018295201007276773\n",
            "----------Random Action----------\n",
            "TIMESTEP 467 / STATE explore / EPSILON 0.009122200000000137 / ACTION 1 / REWARD 0.0 / Q_MAX  1.1095465 / Loss  0.0036981217563152313\n",
            "TIMESTEP 468 / STATE explore / EPSILON 0.009118900000000138 / ACTION 2 / REWARD 0.0 / Q_MAX  1.0889453 / Loss  0.001774506294168532\n",
            "TIMESTEP 469 / STATE explore / EPSILON 0.009115600000000139 / ACTION 2 / REWARD 0.0 / Q_MAX  1.0789406 / Loss  0.010216021910309792\n",
            "TIMESTEP 470 / STATE explore / EPSILON 0.009112300000000139 / ACTION 2 / REWARD 0.0 / Q_MAX  1.456536 / Loss  0.0034915865398943424\n",
            "TIMESTEP 471 / STATE explore / EPSILON 0.00910900000000014 / ACTION 2 / REWARD 0.0 / Q_MAX  1.247717 / Loss  0.005341023206710815\n",
            "TIMESTEP 472 / STATE explore / EPSILON 0.00910570000000014 / ACTION 2 / REWARD 0.0 / Q_MAX  1.2956216 / Loss  0.005203514825552702\n",
            "TIMESTEP 473 / STATE explore / EPSILON 0.00910240000000014 / ACTION 1 / REWARD 0.0 / Q_MAX  1.355934 / Loss  0.002306652255356312\n",
            "TIMESTEP 474 / STATE explore / EPSILON 0.009099100000000141 / ACTION 2 / REWARD 0.0 / Q_MAX  1.4104962 / Loss  0.004488271661102772\n",
            "----------Random Action----------\n",
            "TIMESTEP 475 / STATE explore / EPSILON 0.009095800000000142 / ACTION 1 / REWARD 0.0 / Q_MAX  1.7096593 / Loss  0.008724020794034004\n",
            "TIMESTEP 476 / STATE explore / EPSILON 0.009092500000000142 / ACTION 2 / REWARD 0.0 / Q_MAX  1.7901678 / Loss  0.005720505956560373\n",
            "TIMESTEP 477 / STATE explore / EPSILON 0.009089200000000143 / ACTION 1 / REWARD 0.0 / Q_MAX  1.2992456 / Loss  0.0037380314897745848\n",
            "TIMESTEP 478 / STATE explore / EPSILON 0.009085900000000143 / ACTION 1 / REWARD 0.0 / Q_MAX  2.413031 / Loss  0.0022223929408937693\n",
            "TIMESTEP 479 / STATE explore / EPSILON 0.009082600000000144 / ACTION 1 / REWARD 0.0 / Q_MAX  1.5517927 / Loss  0.0020108306780457497\n",
            "TIMESTEP 480 / STATE explore / EPSILON 0.009079300000000144 / ACTION 2 / REWARD 0.0 / Q_MAX  2.6653154 / Loss  0.003534778719767928\n",
            "TIMESTEP 481 / STATE explore / EPSILON 0.009076000000000145 / ACTION 1 / REWARD 0.0 / Q_MAX  2.906543 / Loss  0.0017753510037437081\n",
            "TIMESTEP 482 / STATE explore / EPSILON 0.009072700000000145 / ACTION 1 / REWARD 1.0 / Q_MAX  2.1000533 / Loss  0.0038935376796871424\n",
            "TIMESTEP 483 / STATE explore / EPSILON 0.009069400000000146 / ACTION 2 / REWARD 0.0 / Q_MAX  1.7951809 / Loss  0.0058158948086202145\n",
            "TIMESTEP 484 / STATE explore / EPSILON 0.009066100000000146 / ACTION 2 / REWARD 0.0 / Q_MAX  2.260722 / Loss  0.0026558381505310535\n",
            "TIMESTEP 485 / STATE explore / EPSILON 0.009062800000000147 / ACTION 1 / REWARD 0.0 / Q_MAX  2.1211767 / Loss  0.0051509481854736805\n",
            "TIMESTEP 486 / STATE explore / EPSILON 0.009059500000000147 / ACTION 1 / REWARD 0.0 / Q_MAX  2.0093 / Loss  0.0032899510115385056\n",
            "TIMESTEP 487 / STATE explore / EPSILON 0.009056200000000148 / ACTION 1 / REWARD 0.0 / Q_MAX  2.6749225 / Loss  0.003203232306987047\n",
            "TIMESTEP 488 / STATE explore / EPSILON 0.009052900000000148 / ACTION 2 / REWARD 0.0 / Q_MAX  2.0862522 / Loss  0.005598458461463451\n",
            "TIMESTEP 489 / STATE explore / EPSILON 0.009049600000000149 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8460836 / Loss  0.0016326825134456158\n",
            "TIMESTEP 490 / STATE explore / EPSILON 0.00904630000000015 / ACTION 1 / REWARD 0.0 / Q_MAX  2.2649138 / Loss  0.0021732272580266\n",
            "TIMESTEP 491 / STATE explore / EPSILON 0.00904300000000015 / ACTION 2 / REWARD 0.0 / Q_MAX  3.5287313 / Loss  0.004160224460065365\n",
            "TIMESTEP 492 / STATE explore / EPSILON 0.00903970000000015 / ACTION 2 / REWARD 0.0 / Q_MAX  3.6880612 / Loss  0.004231777507811785\n",
            "TIMESTEP 493 / STATE explore / EPSILON 0.009036400000000151 / ACTION 2 / REWARD 0.0 / Q_MAX  2.418308 / Loss  0.0023956620134413242\n",
            "TIMESTEP 494 / STATE explore / EPSILON 0.009033100000000151 / ACTION 1 / REWARD 0.0 / Q_MAX  2.4575198 / Loss  0.004524771589785814\n",
            "----------Random Action----------\n",
            "TIMESTEP 495 / STATE explore / EPSILON 0.009029800000000152 / ACTION 0 / REWARD 0.0 / Q_MAX  3.7444606 / Loss  0.0022043276112526655\n",
            "TIMESTEP 496 / STATE explore / EPSILON 0.009026500000000152 / ACTION 1 / REWARD 0.0 / Q_MAX  2.9626608 / Loss  0.004953713621944189\n",
            "TIMESTEP 497 / STATE explore / EPSILON 0.009023200000000153 / ACTION 1 / REWARD 0.0 / Q_MAX  3.637681 / Loss  0.0014999073464423418\n",
            "TIMESTEP 498 / STATE explore / EPSILON 0.009019900000000153 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9757724 / Loss  0.002301260596141219\n",
            "TIMESTEP 499 / STATE explore / EPSILON 0.009016600000000154 / ACTION 1 / REWARD 0.0 / Q_MAX  3.715965 / Loss  0.006632097065448761\n",
            "Now we save model\n",
            "TIMESTEP 500 / STATE explore / EPSILON 0.009013300000000155 / ACTION 2 / REWARD 0.0 / Q_MAX  3.5875301 / Loss  0.00501288753002882\n",
            "TIMESTEP 501 / STATE explore / EPSILON 0.009010000000000155 / ACTION 2 / REWARD 0.0 / Q_MAX  1.8775703 / Loss  0.003976303152740002\n",
            "TIMESTEP 502 / STATE explore / EPSILON 0.009006700000000156 / ACTION 2 / REWARD 0.0 / Q_MAX  2.1538455 / Loss  0.00394335575401783\n",
            "TIMESTEP 503 / STATE explore / EPSILON 0.009003400000000156 / ACTION 1 / REWARD 0.0 / Q_MAX  2.4382885 / Loss  0.002418846357613802\n",
            "TIMESTEP 504 / STATE explore / EPSILON 0.009000100000000157 / ACTION 2 / REWARD 0.0 / Q_MAX  3.1696956 / Loss  0.0030001308768987656\n",
            "TIMESTEP 505 / STATE explore / EPSILON 0.008996800000000157 / ACTION 1 / REWARD 0.0 / Q_MAX  3.3856943 / Loss  0.002419255208224058\n",
            "TIMESTEP 506 / STATE explore / EPSILON 0.008993500000000158 / ACTION 1 / REWARD 0.0 / Q_MAX  2.9502172 / Loss  0.002063889754936099\n",
            "TIMESTEP 507 / STATE explore / EPSILON 0.008990200000000158 / ACTION 2 / REWARD 0.0 / Q_MAX  2.4717405 / Loss  0.002730344654992223\n",
            "TIMESTEP 508 / STATE explore / EPSILON 0.008986900000000159 / ACTION 1 / REWARD 0.0 / Q_MAX  2.38535 / Loss  0.004183931741863489\n",
            "TIMESTEP 509 / STATE explore / EPSILON 0.00898360000000016 / ACTION 2 / REWARD 0.0 / Q_MAX  2.709671 / Loss  0.004589501302689314\n",
            "TIMESTEP 510 / STATE explore / EPSILON 0.00898030000000016 / ACTION 2 / REWARD 0.0 / Q_MAX  3.3022702 / Loss  0.005957483313977718\n",
            "TIMESTEP 511 / STATE explore / EPSILON 0.00897700000000016 / ACTION 1 / REWARD 0.0 / Q_MAX  2.1241722 / Loss  0.002343984553590417\n",
            "TIMESTEP 512 / STATE explore / EPSILON 0.00897370000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  3.5364988 / Loss  0.0023900242522358894\n",
            "TIMESTEP 513 / STATE explore / EPSILON 0.008970400000000161 / ACTION 1 / REWARD 0.0 / Q_MAX  2.5221705 / Loss  0.005010311026126146\n",
            "TIMESTEP 514 / STATE explore / EPSILON 0.008967100000000162 / ACTION 1 / REWARD 1.0 / Q_MAX  2.8912182 / Loss  0.008374141529202461\n",
            "TIMESTEP 515 / STATE explore / EPSILON 0.008963800000000162 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5555074 / Loss  0.002361553953960538\n",
            "TIMESTEP 516 / STATE explore / EPSILON 0.008960500000000163 / ACTION 1 / REWARD 0.0 / Q_MAX  2.5041263 / Loss  0.003670786041766405\n",
            "TIMESTEP 517 / STATE explore / EPSILON 0.008957200000000163 / ACTION 2 / REWARD 0.0 / Q_MAX  3.417366 / Loss  0.0050545125268399715\n",
            "----------Random Action----------\n",
            "TIMESTEP 518 / STATE explore / EPSILON 0.008953900000000164 / ACTION 2 / REWARD 0.0 / Q_MAX  2.6910825 / Loss  0.0018792434129863977\n",
            "----------Random Action----------\n",
            "TIMESTEP 519 / STATE explore / EPSILON 0.008950600000000164 / ACTION 0 / REWARD 0.0 / Q_MAX  2.4778984 / Loss  0.0017429639119654894\n",
            "TIMESTEP 520 / STATE explore / EPSILON 0.008947300000000165 / ACTION 2 / REWARD 0.0 / Q_MAX  3.26478 / Loss  0.003471086733043194\n",
            "TIMESTEP 521 / STATE explore / EPSILON 0.008944000000000165 / ACTION 1 / REWARD 0.0 / Q_MAX  2.447553 / Loss  0.004797737579792738\n",
            "TIMESTEP 522 / STATE explore / EPSILON 0.008940700000000166 / ACTION 2 / REWARD 0.0 / Q_MAX  2.3184137 / Loss  0.006090009119361639\n",
            "TIMESTEP 523 / STATE explore / EPSILON 0.008937400000000166 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0447664 / Loss  0.0029991064220666885\n",
            "TIMESTEP 524 / STATE explore / EPSILON 0.008934100000000167 / ACTION 1 / REWARD 0.0 / Q_MAX  3.2586653 / Loss  0.004500058013945818\n",
            "----------Random Action----------\n",
            "TIMESTEP 525 / STATE explore / EPSILON 0.008930800000000167 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0387595 / Loss  0.0022674058564007282\n",
            "TIMESTEP 526 / STATE explore / EPSILON 0.008927500000000168 / ACTION 1 / REWARD 0.0 / Q_MAX  2.502827 / Loss  0.0027050303760915995\n",
            "TIMESTEP 527 / STATE explore / EPSILON 0.008924200000000168 / ACTION 1 / REWARD 0.0 / Q_MAX  2.243946 / Loss  0.012634526938199997\n",
            "TIMESTEP 528 / STATE explore / EPSILON 0.008920900000000169 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7052586 / Loss  0.0018552262336015701\n",
            "TIMESTEP 529 / STATE explore / EPSILON 0.00891760000000017 / ACTION 2 / REWARD 0.0 / Q_MAX  2.3720772 / Loss  0.005470682866871357\n",
            "TIMESTEP 530 / STATE explore / EPSILON 0.00891430000000017 / ACTION 2 / REWARD 0.0 / Q_MAX  2.2439773 / Loss  0.0036430370528250933\n",
            "TIMESTEP 531 / STATE explore / EPSILON 0.00891100000000017 / ACTION 1 / REWARD 0.0 / Q_MAX  2.0917463 / Loss  0.002605969086289406\n",
            "TIMESTEP 532 / STATE explore / EPSILON 0.008907700000000171 / ACTION 1 / REWARD 0.0 / Q_MAX  3.1755075 / Loss  0.0020776500459760427\n",
            "TIMESTEP 533 / STATE explore / EPSILON 0.008904400000000172 / ACTION 1 / REWARD 0.0 / Q_MAX  2.8376868 / Loss  0.002963499166071415\n",
            "TIMESTEP 534 / STATE explore / EPSILON 0.008901100000000172 / ACTION 1 / REWARD 0.0 / Q_MAX  2.3399656 / Loss  0.008790860883891582\n",
            "TIMESTEP 535 / STATE explore / EPSILON 0.008897800000000173 / ACTION 2 / REWARD -1.0 / Q_MAX  2.4447482 / Loss  0.004376321565359831\n",
            "TIMESTEP 536 / STATE explore / EPSILON 0.008894500000000173 / ACTION 0 / REWARD 0.0 / Q_MAX  2.9731033 / Loss  0.0019971951842308044\n",
            "TIMESTEP 537 / STATE explore / EPSILON 0.008891200000000174 / ACTION 2 / REWARD 0.0 / Q_MAX  2.2672987 / Loss  0.005729971453547478\n",
            "TIMESTEP 538 / STATE explore / EPSILON 0.008887900000000174 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9785233 / Loss  0.0018430869095027447\n",
            "TIMESTEP 539 / STATE explore / EPSILON 0.008884600000000175 / ACTION 2 / REWARD 0.0 / Q_MAX  2.961991 / Loss  0.007984156720340252\n",
            "TIMESTEP 540 / STATE explore / EPSILON 0.008881300000000175 / ACTION 2 / REWARD 0.0 / Q_MAX  2.2973135 / Loss  0.0027395919896662235\n",
            "TIMESTEP 541 / STATE explore / EPSILON 0.008878000000000176 / ACTION 2 / REWARD 0.0 / Q_MAX  3.1126413 / Loss  0.003762556705623865\n",
            "TIMESTEP 542 / STATE explore / EPSILON 0.008874700000000176 / ACTION 2 / REWARD 0.0 / Q_MAX  3.1636765 / Loss  0.0026194839738309383\n",
            "TIMESTEP 543 / STATE explore / EPSILON 0.008871400000000177 / ACTION 2 / REWARD 0.0 / Q_MAX  2.6628356 / Loss  0.004326784983277321\n",
            "TIMESTEP 544 / STATE explore / EPSILON 0.008868100000000177 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5143375 / Loss  0.00815577432513237\n",
            "TIMESTEP 545 / STATE explore / EPSILON 0.008864800000000178 / ACTION 2 / REWARD 0.0 / Q_MAX  2.8560483 / Loss  0.0034689181484282017\n",
            "TIMESTEP 546 / STATE explore / EPSILON 0.008861500000000178 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5781422 / Loss  0.0025107956025749445\n",
            "----------Random Action----------\n",
            "TIMESTEP 547 / STATE explore / EPSILON 0.008858200000000179 / ACTION 2 / REWARD 0.0 / Q_MAX  3.165142 / Loss  0.0021661794744431973\n",
            "TIMESTEP 548 / STATE explore / EPSILON 0.00885490000000018 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9724088 / Loss  0.002512113656848669\n",
            "TIMESTEP 549 / STATE explore / EPSILON 0.00885160000000018 / ACTION 2 / REWARD 0.0 / Q_MAX  2.987151 / Loss  0.0035792505368590355\n",
            "TIMESTEP 550 / STATE explore / EPSILON 0.00884830000000018 / ACTION 2 / REWARD 0.0 / Q_MAX  2.497601 / Loss  0.0022743947338312864\n",
            "TIMESTEP 551 / STATE explore / EPSILON 0.00884500000000018 / ACTION 2 / REWARD 0.0 / Q_MAX  3.0894618 / Loss  0.0016819877782836556\n",
            "TIMESTEP 552 / STATE explore / EPSILON 0.008841700000000181 / ACTION 2 / REWARD 0.0 / Q_MAX  3.2988334 / Loss  0.0036119194701313972\n",
            "TIMESTEP 553 / STATE explore / EPSILON 0.008838400000000182 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5351007 / Loss  0.006867755204439163\n",
            "TIMESTEP 554 / STATE explore / EPSILON 0.008835100000000182 / ACTION 2 / REWARD 0.0 / Q_MAX  3.3274527 / Loss  0.0017531702760607004\n",
            "TIMESTEP 555 / STATE explore / EPSILON 0.008831800000000183 / ACTION 1 / REWARD 0.0 / Q_MAX  3.130984 / Loss  0.002878356259316206\n",
            "TIMESTEP 556 / STATE explore / EPSILON 0.008828500000000183 / ACTION 1 / REWARD 0.0 / Q_MAX  3.343127 / Loss  0.0030548255890607834\n",
            "TIMESTEP 557 / STATE explore / EPSILON 0.008825200000000184 / ACTION 1 / REWARD 0.0 / Q_MAX  2.678883 / Loss  0.0016864630160853267\n",
            "TIMESTEP 558 / STATE explore / EPSILON 0.008821900000000184 / ACTION 1 / REWARD 0.0 / Q_MAX  3.3058152 / Loss  0.006372732575982809\n",
            "TIMESTEP 559 / STATE explore / EPSILON 0.008818600000000185 / ACTION 1 / REWARD 0.0 / Q_MAX  3.1969147 / Loss  0.0017884422559291124\n",
            "TIMESTEP 560 / STATE explore / EPSILON 0.008815300000000185 / ACTION 1 / REWARD 0.0 / Q_MAX  3.2677207 / Loss  0.0022613939363509417\n",
            "TIMESTEP 561 / STATE explore / EPSILON 0.008812000000000186 / ACTION 1 / REWARD 0.0 / Q_MAX  3.260046 / Loss  0.004063009284436703\n",
            "TIMESTEP 562 / STATE explore / EPSILON 0.008808700000000187 / ACTION 2 / REWARD 0.0 / Q_MAX  3.2670393 / Loss  0.0023175354581326246\n",
            "TIMESTEP 563 / STATE explore / EPSILON 0.008805400000000187 / ACTION 2 / REWARD 0.0 / Q_MAX  3.276246 / Loss  0.003764897119253874\n",
            "TIMESTEP 564 / STATE explore / EPSILON 0.008802100000000188 / ACTION 1 / REWARD 0.0 / Q_MAX  3.0264196 / Loss  0.002461445750668645\n",
            "TIMESTEP 565 / STATE explore / EPSILON 0.008798800000000188 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7111952 / Loss  0.009170956909656525\n",
            "TIMESTEP 566 / STATE explore / EPSILON 0.008795500000000189 / ACTION 1 / REWARD 0.0 / Q_MAX  3.0769808 / Loss  0.002155332127586007\n",
            "TIMESTEP 567 / STATE explore / EPSILON 0.008792200000000189 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9401188 / Loss  0.003236374817788601\n",
            "TIMESTEP 568 / STATE explore / EPSILON 0.00878890000000019 / ACTION 1 / REWARD 1.0 / Q_MAX  2.6391637 / Loss  0.004709341563284397\n",
            "TIMESTEP 569 / STATE explore / EPSILON 0.00878560000000019 / ACTION 2 / REWARD 0.0 / Q_MAX  2.5433466 / Loss  0.0018312528263777494\n",
            "TIMESTEP 570 / STATE explore / EPSILON 0.00878230000000019 / ACTION 0 / REWARD 0.0 / Q_MAX  2.7019951 / Loss  0.003100452944636345\n",
            "TIMESTEP 571 / STATE explore / EPSILON 0.008779000000000191 / ACTION 0 / REWARD 0.0 / Q_MAX  2.924447 / Loss  0.002214703243225813\n",
            "TIMESTEP 572 / STATE explore / EPSILON 0.008775700000000192 / ACTION 0 / REWARD 0.0 / Q_MAX  2.228511 / Loss  0.008263160474598408\n",
            "TIMESTEP 573 / STATE explore / EPSILON 0.008772400000000192 / ACTION 2 / REWARD 0.0 / Q_MAX  2.9003825 / Loss  0.0045602088794112206\n",
            "TIMESTEP 574 / STATE explore / EPSILON 0.008769100000000193 / ACTION 2 / REWARD 0.0 / Q_MAX  2.90754 / Loss  0.006875225342810154\n",
            "TIMESTEP 575 / STATE explore / EPSILON 0.008765800000000193 / ACTION 2 / REWARD 0.0 / Q_MAX  2.7475896 / Loss  0.003617101814597845\n",
            "TIMESTEP 576 / STATE explore / EPSILON 0.008762500000000194 / ACTION 0 / REWARD 0.0 / Q_MAX  2.9396224 / Loss  0.006236851215362549\n",
            "TIMESTEP 577 / STATE explore / EPSILON 0.008759200000000194 / ACTION 0 / REWARD 0.0 / Q_MAX  2.6471486 / Loss  0.002105029998347163\n",
            "TIMESTEP 578 / STATE explore / EPSILON 0.008755900000000195 / ACTION 0 / REWARD 0.0 / Q_MAX  2.9162772 / Loss  0.009711302816867828\n",
            "TIMESTEP 579 / STATE explore / EPSILON 0.008752600000000195 / ACTION 0 / REWARD 0.0 / Q_MAX  3.0192983 / Loss  0.0021879966370761395\n",
            "TIMESTEP 580 / STATE explore / EPSILON 0.008749300000000196 / ACTION 0 / REWARD 0.0 / Q_MAX  2.8698742 / Loss  0.004365818575024605\n",
            "TIMESTEP 581 / STATE explore / EPSILON 0.008746000000000196 / ACTION 0 / REWARD 0.0 / Q_MAX  3.0609143 / Loss  0.002065011765807867\n",
            "TIMESTEP 582 / STATE explore / EPSILON 0.008742700000000197 / ACTION 0 / REWARD 0.0 / Q_MAX  2.8598542 / Loss  0.005522704683244228\n",
            "TIMESTEP 583 / STATE explore / EPSILON 0.008739400000000197 / ACTION 0 / REWARD 0.0 / Q_MAX  2.788413 / Loss  0.0029945257119834423\n",
            "TIMESTEP 584 / STATE explore / EPSILON 0.008736100000000198 / ACTION 0 / REWARD 0.0 / Q_MAX  3.1641243 / Loss  0.004051827359944582\n",
            "TIMESTEP 585 / STATE explore / EPSILON 0.008732800000000198 / ACTION 0 / REWARD 0.0 / Q_MAX  2.9739077 / Loss  0.004103947430849075\n",
            "TIMESTEP 586 / STATE explore / EPSILON 0.008729500000000199 / ACTION 0 / REWARD 0.0 / Q_MAX  3.197273 / Loss  0.0033697807230055332\n",
            "TIMESTEP 587 / STATE explore / EPSILON 0.0087262000000002 / ACTION 0 / REWARD 0.0 / Q_MAX  3.1093836 / Loss  0.005656481720507145\n",
            "TIMESTEP 588 / STATE explore / EPSILON 0.0087229000000002 / ACTION 0 / REWARD 0.0 / Q_MAX  2.7742147 / Loss  0.003028369974344969\n",
            "TIMESTEP 589 / STATE explore / EPSILON 0.0087196000000002 / ACTION 0 / REWARD 0.0 / Q_MAX  3.4215136 / Loss  0.0024103508330881596\n",
            "TIMESTEP 590 / STATE explore / EPSILON 0.008716300000000201 / ACTION 0 / REWARD -1.0 / Q_MAX  3.2590716 / Loss  0.003117227926850319\n",
            "TIMESTEP 591 / STATE explore / EPSILON 0.008713000000000201 / ACTION 0 / REWARD 0.0 / Q_MAX  3.1521292 / Loss  0.009287369437515736\n",
            "TIMESTEP 592 / STATE explore / EPSILON 0.008709700000000202 / ACTION 0 / REWARD 0.0 / Q_MAX  2.9701023 / Loss  0.0033621222246438265\n",
            "TIMESTEP 593 / STATE explore / EPSILON 0.008706400000000203 / ACTION 0 / REWARD 0.0 / Q_MAX  3.441555 / Loss  0.002906965324655175\n",
            "TIMESTEP 594 / STATE explore / EPSILON 0.008703100000000203 / ACTION 0 / REWARD 0.0 / Q_MAX  3.338148 / Loss  0.005786207504570484\n",
            "TIMESTEP 595 / STATE explore / EPSILON 0.008699800000000204 / ACTION 0 / REWARD 0.0 / Q_MAX  3.5261831 / Loss  0.007631377782672644\n",
            "TIMESTEP 596 / STATE explore / EPSILON 0.008696500000000204 / ACTION 0 / REWARD 0.0 / Q_MAX  3.4460652 / Loss  0.002278196392580867\n",
            "TIMESTEP 597 / STATE explore / EPSILON 0.008693200000000205 / ACTION 0 / REWARD 0.0 / Q_MAX  3.49354 / Loss  0.003734320867806673\n",
            "TIMESTEP 598 / STATE explore / EPSILON 0.008689900000000205 / ACTION 0 / REWARD 0.0 / Q_MAX  3.8013113 / Loss  0.0024730858858674765\n",
            "TIMESTEP 599 / STATE explore / EPSILON 0.008686600000000206 / ACTION 0 / REWARD 0.0 / Q_MAX  3.3410366 / Loss  0.0030936049297451973\n",
            "Now we save model\n",
            "TIMESTEP 600 / STATE explore / EPSILON 0.008683300000000206 / ACTION 0 / REWARD 0.0 / Q_MAX  3.1980057 / Loss  0.004885165020823479\n",
            "TIMESTEP 601 / STATE explore / EPSILON 0.008680000000000207 / ACTION 0 / REWARD 0.0 / Q_MAX  3.0762112 / Loss  0.007451324257999659\n",
            "TIMESTEP 602 / STATE explore / EPSILON 0.008676700000000207 / ACTION 0 / REWARD 0.0 / Q_MAX  3.0354736 / Loss  0.0042616818100214005\n",
            "TIMESTEP 603 / STATE explore / EPSILON 0.008673400000000208 / ACTION 0 / REWARD 0.0 / Q_MAX  3.2526536 / Loss  0.009352602064609528\n",
            "TIMESTEP 604 / STATE explore / EPSILON 0.008670100000000208 / ACTION 0 / REWARD 0.0 / Q_MAX  2.9865704 / Loss  0.0017926479922607541\n",
            "TIMESTEP 605 / STATE explore / EPSILON 0.008666800000000209 / ACTION 0 / REWARD 0.0 / Q_MAX  2.8539896 / Loss  0.011211100965738297\n",
            "TIMESTEP 606 / STATE explore / EPSILON 0.00866350000000021 / ACTION 0 / REWARD 0.0 / Q_MAX  2.5220134 / Loss  0.006538708694279194\n",
            "TIMESTEP 607 / STATE explore / EPSILON 0.00866020000000021 / ACTION 0 / REWARD 0.0 / Q_MAX  3.0672207 / Loss  0.0024828636087477207\n",
            "TIMESTEP 608 / STATE explore / EPSILON 0.00865690000000021 / ACTION 0 / REWARD 0.0 / Q_MAX  2.8934455 / Loss  0.007001333869993687\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}