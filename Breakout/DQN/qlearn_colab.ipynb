{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qlearn_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdYve4Htb0_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e587ea7-abd3-48a6-c217-6821c006fd42"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!pip install atari_py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH6yAoJDzo4Z",
        "outputId": "6e7b17cf-a5ac-45cc-837e-68f6aebc1867"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hqnsP3c8uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0fc1b8-82ee-4c6d-8282-fed0cfe7ca14"
      },
      "source": [
        "!python -m atari_py.import_roms '/content/drive/My Drive/CN_Breakout/ROMS'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying mr_do.bin from /content/drive/My Drive/CN_Breakout/ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying lost_luggage.bin from /content/drive/My Drive/CN_Breakout/ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying elevator_action.bin from /content/drive/My Drive/CN_Breakout/ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying asterix.bin from /content/drive/My Drive/CN_Breakout/ROMS/Asterix (AKA Taz) (1984) (Atari, Jerome Domurat, Steve Woita) (CX2696).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying riverraid.bin from /content/drive/My Drive/CN_Breakout/ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying video_pinball.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying road_runner.bin from patched version of /content/drive/My Drive/CN_Breakout/ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying qbert.bin from /content/drive/My Drive/CN_Breakout/ROMS/Q. Bert (1983) (CCE) (C-822).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying surround.bin from /content/drive/My Drive/CN_Breakout/ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying ms_pacman.bin from /content/drive/My Drive/CN_Breakout/ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying up_n_down.bin from /content/drive/My Drive/CN_Breakout/ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying kung_fu_master.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying atlantis.bin from /content/drive/My Drive/CN_Breakout/ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying pitfall.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying chopper_command.bin from /content/drive/My Drive/CN_Breakout/ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying ice_hockey.bin from /content/drive/My Drive/CN_Breakout/ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying breakout.bin from /content/drive/My Drive/CN_Breakout/ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying boxing.bin from /content/drive/My Drive/CN_Breakout/ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying freeway.bin from /content/drive/My Drive/CN_Breakout/ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying kaboom.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying skiing.bin from /content/drive/My Drive/CN_Breakout/ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying tennis.bin from /content/drive/My Drive/CN_Breakout/ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying bank_heist.bin from /content/drive/My Drive/CN_Breakout/ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying demon_attack.bin from /content/drive/My Drive/CN_Breakout/ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying koolaid.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying star_gunner.bin from /content/drive/My Drive/CN_Breakout/ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying trondead.bin from /content/drive/My Drive/CN_Breakout/ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying robotank.bin from /content/drive/My Drive/CN_Breakout/ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying space_invaders.bin from /content/drive/My Drive/CN_Breakout/ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying hero.bin from /content/drive/My Drive/CN_Breakout/ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying montezuma_revenge.bin from /content/drive/My Drive/CN_Breakout/ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying alien.bin from /content/drive/My Drive/CN_Breakout/ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying asteroids.bin from /content/drive/My Drive/CN_Breakout/ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying air_raid.bin from /content/drive/My Drive/CN_Breakout/ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying assault.bin from /content/drive/My Drive/CN_Breakout/ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying amidar.bin from /content/drive/My Drive/CN_Breakout/ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying berzerk.bin from /content/drive/My Drive/CN_Breakout/ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from /content/drive/My Drive/CN_Breakout/ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying beam_rider.bin from /content/drive/My Drive/CN_Breakout/ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying battle_zone.bin from /content/drive/My Drive/CN_Breakout/ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying carnival.bin from /content/drive/My Drive/CN_Breakout/ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from /content/drive/My Drive/CN_Breakout/ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying crazy_climber.bin from /content/drive/My Drive/CN_Breakout/ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying donkey_kong.bin from /content/drive/My Drive/CN_Breakout/ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying defender.bin from /content/drive/My Drive/CN_Breakout/ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying double_dunk.bin from /content/drive/My Drive/CN_Breakout/ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying frostbite.bin from /content/drive/My Drive/CN_Breakout/ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying enduro.bin from /content/drive/My Drive/CN_Breakout/ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from /content/drive/My Drive/CN_Breakout/ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying frogger.bin from /content/drive/My Drive/CN_Breakout/ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying gopher.bin from /content/drive/My Drive/CN_Breakout/ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from /content/drive/My Drive/CN_Breakout/ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying galaxian.bin from /content/drive/My Drive/CN_Breakout/ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying journey_escape.bin from /content/drive/My Drive/CN_Breakout/ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kangaroo.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying krull.bin from /content/drive/My Drive/CN_Breakout/ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying name_this_game.bin from /content/drive/My Drive/CN_Breakout/ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from /content/drive/My Drive/CN_Breakout/ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying private_eye.bin from /content/drive/My Drive/CN_Breakout/ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying pooyan.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying seaquest.bin from /content/drive/My Drive/CN_Breakout/ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying solaris.bin from /content/drive/My Drive/CN_Breakout/ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying venture.bin from /content/drive/My Drive/CN_Breakout/ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from /content/drive/My Drive/CN_Breakout/ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from /content/drive/My Drive/CN_Breakout/ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from /content/drive/My Drive/CN_Breakout/ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from /content/drive/My Drive/CN_Breakout/ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying tutankham.bin from /content/drive/My Drive/CN_Breakout/ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying time_pilot.bin from /content/drive/My Drive/CN_Breakout/ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying jamesbond.bin from /content/drive/My Drive/CN_Breakout/ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying adventure.bin from /content/drive/My Drive/CN_Breakout/ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying pacman.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying keystone_kapers.bin from /content/drive/My Drive/CN_Breakout/ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from /content/drive/My Drive/CN_Breakout/ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from /content/drive/My Drive/CN_Breakout/ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying sir_lancelot.bin from /content/drive/My Drive/CN_Breakout/ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXbGMPQxbxk7",
        "outputId": "db7cc7d8-94ac-4016-c50c-68317cf3a865"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\n",
        "#https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import skimage as skimage\n",
        "from skimage import transform, color, exposure\n",
        "from skimage.transform import rotate\n",
        "from skimage.viewer import ImageViewer\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import json\n",
        "from tensorflow.keras.initializers import identity\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD , Adam\n",
        "import tensorflow as tf\n",
        "#import agent\n",
        "# Import the gym module\n",
        "import gym\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\n",
        "GAME = 'atari' # the name of the game being played for log files\n",
        "CONFIG = 'nothreshold'\n",
        "ACTIONS = 3 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVATION = 200. # timesteps to observe before training. de cada 3200 frames, vamos ao nosso buffer e selecionamos de forma aleatoria um batch size. Neste caso, 32 frames. em numpy arrays\n",
        "EXPLORE = 3000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.01 # starting value of epsilon EPSILON é para ver o exploration vs exploitation\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "#MAX_STEPS_PER_EPISODE = 1000\n",
        "EPISODES = 10000\n",
        "q_max_list = []\n",
        "loss_list = []\n",
        "reward_list = []\n",
        "\n",
        "img_rows, img_cols = 84, 84\n",
        "#Convert image into Black and white\n",
        "img_channels = 4 #We stack 4 frames\n",
        "\n",
        "def buildmodel():\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = tf.keras.layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = Conv2D(32, 8, strides=4, activation=\"relu\", padding = 'same')(inputs)\n",
        "    layer2 = Conv2D(64, 4, strides=2, activation=\"relu\", padding = 'same')(layer1)\n",
        "    layer3 = Conv2D(64, 3, strides=1, activation=\"relu\", padding = 'same')(layer2)\n",
        "\n",
        "    layer4 = Flatten()(layer3)\n",
        "\n",
        "    layer5 = Dense(512, activation=\"relu\")(layer4)\n",
        "    action = Dense(ACTIONS, activation=\"linear\")(layer5)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=action)\n",
        "    adam = Adam(learning_rate = LEARNING_RATE)\n",
        "    model.compile(loss='mse', optimizer = adam)\n",
        "\n",
        "    print(\"We finish building the model\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n",
        "def trainNetwork(model,args):\n",
        "    # open up a game state to communicate with emulator\n",
        "    env = wrap_env(gym.make('BreakoutDeterministic-v4'))\n",
        "    env.reset()\n",
        "    # store the previous observations in replay memory\n",
        "    \n",
        "    #----------------------------------------\n",
        "    #PARA OBTER O SIGNIFICADO DAS AÇÕES POSSíVEIS\n",
        "    #print(env.unwrapped.get_action_meanings())\n",
        "    #----------------------------------------\n",
        "    \n",
        "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        \n",
        "    x_t, r_0, terminal, info = env.step(1) #COMEÇAR O JOGO COM A AÇÃO \"FIRE\"\n",
        "    \n",
        "    env.render()\n",
        "\n",
        "    D = args['D']\n",
        "\n",
        "    x_t = skimage.color.rgb2gray(x_t)\n",
        "    x_t = skimage.transform.resize(x_t, (84,84))\n",
        "    x_t = skimage.exposure.rescale_intensity(x_t, out_range = (0,255))\n",
        "\n",
        "    x_t = x_t / 255.0\n",
        "\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2) #colocar a sequência de frames. 4 frames sequenciais, que vamos aplicar à nossa lista. Para conseguir a estabilidade de imagens sequenciais\n",
        "    \n",
        "    #print (s_t.shape)\n",
        "\n",
        "    #In Keras, need to reshape\n",
        "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
        "\n",
        "    t = args['t']\n",
        "\n",
        "    if args['mode'] == 'Run':\n",
        "        OBSERVE = 999999999\t#We keep observe, never train\n",
        "        epsilon = FINAL_EPSILON # higher epsilon, more timestamps?\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/My Drive/CN_Breakout/models/model_v1.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "\n",
        "    elif args['mode'] == 'CTrain': #Continue previous train\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = 0.07823368810419994 #0.08811709480229288\n",
        "        epsilon = args['epsilon']\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/My Drive/CN_Breakout/models/model.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        \n",
        "\n",
        "    else:\t\t\t\t\t   #We go to training mode -> -m \"Train\"\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = INITIAL_EPSILON #o EPSILON é o que divide a parte de exploration vs exploitation. se for abaixo de um dado valor é exploration. Caso contrário é exploitation\n",
        "        epsilon = args['epsilon']\n",
        "\n",
        "\n",
        "    lives = 5\n",
        "    r_total = 0\n",
        "    while (lives > 0):\n",
        "    #for i in range(MAX_STEPS_PER_EPISODE):\n",
        "        loss = 0\n",
        "        Q_sa = 0 # Q(s, a) representing the maximum discounted future reward when we perform action a in state s.\n",
        "        action_index = 0\n",
        "        r_t = 0 #reward\n",
        "        a_t = np.zeros([ACTIONS]) #action\n",
        "      \n",
        "        #choose an action epsilon greedy\n",
        "        if t % FRAME_PER_ACTION == 0:\n",
        "            if random.random() <= epsilon:\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
        "                a_t[action_index] = 1\n",
        "                \n",
        "            else:\n",
        "                q = model.predict(s_t)\t   #input a stack of 4 images, get the prediction\n",
        "                max_Q = np.argmax(q)\n",
        "               # print(max_Q, q, a_t)\n",
        "                action_index = max_Q\n",
        "                a_t[max_Q] = 1\n",
        "\n",
        "        #We reduce the epsilon gradually\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        #run the selected action and observed next state and reward. DEPOIS DE UM \"STEP\" correr sempre o \"RENDER\"\n",
        "        x_t1_colored, r_t, terminal, info = env.step(list(a_t).index(1) + 1) #FUNÇÂO \"WHERE\" para obter o índice do valor do array que está a 1\n",
        "\n",
        "        if info['ale.lives'] < lives:\n",
        "          lives -= 1\n",
        "          r_t = -1.0\n",
        "          if info['ale.lives'] > 0:\n",
        "            env.step(1)\n",
        "\n",
        "        env.render()\n",
        "        \n",
        "        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n",
        "        x_t1 = skimage.transform.resize(x_t1, (84, 84))\n",
        "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range = (0, 255))\n",
        "        \n",
        "        x_t1 = x_t1 / 255.0\n",
        "\n",
        "\n",
        "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis = 3)\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        #only train if done observing\n",
        "        if t > OBSERVE: #train ou update da nossa rede. de quantas em quantas frames vamos precisar para fazer um treino. se replay_mem começar a ficar mto cheio retira a última entrada. e fazemos append das novas decisoes que foram sendo adquiridas\n",
        "            #sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "\n",
        "            #Now we do the experience replay\n",
        "            state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "            state_t = np.concatenate(state_t)\n",
        "            state_t1 = np.concatenate(state_t1)\n",
        "            targets = model.predict(state_t)\n",
        "            Q_sa = model.predict(state_t1)\n",
        "            targets[range(BATCH), action_t] = reward_t + GAMMA * np.max(Q_sa, axis = 1) * np.invert(terminal) #qual o target associado\n",
        "            \n",
        "            \n",
        "            loss += model.train_on_batch(state_t, targets) #quanto mais proximo de zero, mais proximo está de convergir para conseguir estimar o key value de acordo com o par (estado, ação)\n",
        "            \n",
        "        s_t = s_t1\n",
        "        t = t + 1\n",
        "\n",
        "        # save progress every 1000 iterations\n",
        "        if t % 100 == 0:\n",
        "            print(\"Now we save model\")\n",
        "            model.save_weights(\"/content/drive/My Drive/CN_Breakout/models/model.h5\", overwrite = True)\n",
        "            with open(\"model.json\", \"w\") as outfile:\n",
        "                json.dump(model.to_json(), outfile)\n",
        "\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
        "\n",
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
        "        \n",
        "        q_max_list.append(np.max(Q_sa))\n",
        "        loss_list.append(loss)\n",
        "        reward_list.append(r_t)\n",
        "\n",
        "    env.close()\n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")\n",
        "    return t, epsilon, D\n",
        "\n",
        "def playGame(args):\n",
        "    model = buildmodel()\n",
        "    t, epsilon, D = trainNetwork(model,args)\n",
        "    return t, epsilon, D\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description = 'Description of your program')\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True)\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True) adicionar o argumento de número de episódios\n",
        "    #args = vars(parser.parse_args())\n",
        "    t = 0\n",
        "    epsilon = INITIAL_EPSILON\n",
        "    D = deque()\n",
        "    for i in range(EPISODES):\n",
        "        print(\"EPISODE\", i)\n",
        "        tp, epsilonp, Dp = playGame({'mode': 'CTrain', 't': t, 'epsilon': epsilon, 'D': D})\n",
        "        #show_video()\n",
        "        t = tp\n",
        "        epsilon = epsilonp\n",
        "        D = Dp\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPISODE 0\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.01 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 4.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 201 / STATE explore / EPSILON 0.01 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "TIMESTEP 202 / STATE explore / EPSILON 0.0099967 / ACTION 2 / REWARD 0.0 / Q_MAX  14.029871 / Loss  0.03242676705121994\n",
            "TIMESTEP 203 / STATE explore / EPSILON 0.009993400000000001 / ACTION 2 / REWARD 0.0 / Q_MAX  12.853568 / Loss  0.022100280970335007\n",
            "TIMESTEP 204 / STATE explore / EPSILON 0.009990100000000002 / ACTION 2 / REWARD 0.0 / Q_MAX  12.377348 / Loss  0.003924014046788216\n",
            "TIMESTEP 205 / STATE explore / EPSILON 0.009986800000000002 / ACTION 2 / REWARD 0.0 / Q_MAX  12.250671 / Loss  0.010903658345341682\n",
            "TIMESTEP 206 / STATE explore / EPSILON 0.009983500000000003 / ACTION 2 / REWARD 0.0 / Q_MAX  10.970205 / Loss  0.011895077303051949\n",
            "TIMESTEP 207 / STATE explore / EPSILON 0.009980200000000003 / ACTION 2 / REWARD 0.0 / Q_MAX  11.385621 / Loss  0.0053236037492752075\n",
            "TIMESTEP 208 / STATE explore / EPSILON 0.009976900000000004 / ACTION 2 / REWARD 0.0 / Q_MAX  10.336596 / Loss  0.012730946764349937\n",
            "TIMESTEP 209 / STATE explore / EPSILON 0.009973600000000004 / ACTION 2 / REWARD 0.0 / Q_MAX  10.390202 / Loss  0.008152995258569717\n",
            "TIMESTEP 210 / STATE explore / EPSILON 0.009970300000000005 / ACTION 0 / REWARD 0.0 / Q_MAX  10.090986 / Loss  0.009159911423921585\n",
            "TIMESTEP 211 / STATE explore / EPSILON 0.009967000000000005 / ACTION 0 / REWARD 0.0 / Q_MAX  9.639118 / Loss  0.0078116548247635365\n",
            "TIMESTEP 212 / STATE explore / EPSILON 0.009963700000000006 / ACTION 1 / REWARD 0.0 / Q_MAX  9.148201 / Loss  0.0105449752882123\n",
            "TIMESTEP 213 / STATE explore / EPSILON 0.009960400000000006 / ACTION 1 / REWARD 0.0 / Q_MAX  8.770836 / Loss  0.007529641967266798\n",
            "TIMESTEP 214 / STATE explore / EPSILON 0.009957100000000007 / ACTION 1 / REWARD 0.0 / Q_MAX  8.112603 / Loss  0.006729826796799898\n",
            "TIMESTEP 215 / STATE explore / EPSILON 0.009953800000000007 / ACTION 1 / REWARD 0.0 / Q_MAX  7.6974735 / Loss  0.009084491059184074\n",
            "TIMESTEP 216 / STATE explore / EPSILON 0.009950500000000008 / ACTION 2 / REWARD 0.0 / Q_MAX  7.812071 / Loss  0.003978142514824867\n",
            "TIMESTEP 217 / STATE explore / EPSILON 0.009947200000000008 / ACTION 1 / REWARD -1.0 / Q_MAX  7.640437 / Loss  0.009360135532915592\n",
            "TIMESTEP 218 / STATE explore / EPSILON 0.009943900000000009 / ACTION 0 / REWARD 0.0 / Q_MAX  7.505035 / Loss  0.004125006031244993\n",
            "TIMESTEP 219 / STATE explore / EPSILON 0.00994060000000001 / ACTION 2 / REWARD 0.0 / Q_MAX  7.2071366 / Loss  0.014703355729579926\n",
            "TIMESTEP 220 / STATE explore / EPSILON 0.00993730000000001 / ACTION 2 / REWARD 0.0 / Q_MAX  7.211743 / Loss  0.004651648923754692\n",
            "TIMESTEP 221 / STATE explore / EPSILON 0.00993400000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  6.692212 / Loss  0.01005073357373476\n",
            "TIMESTEP 222 / STATE explore / EPSILON 0.009930700000000011 / ACTION 1 / REWARD 0.0 / Q_MAX  6.9119277 / Loss  0.0044601187109947205\n",
            "TIMESTEP 223 / STATE explore / EPSILON 0.009927400000000012 / ACTION 2 / REWARD 0.0 / Q_MAX  7.106362 / Loss  0.004749841522425413\n",
            "TIMESTEP 224 / STATE explore / EPSILON 0.009924100000000012 / ACTION 1 / REWARD 0.0 / Q_MAX  6.904847 / Loss  0.0056819249875843525\n",
            "TIMESTEP 225 / STATE explore / EPSILON 0.009920800000000013 / ACTION 1 / REWARD 0.0 / Q_MAX  6.903741 / Loss  0.033263906836509705\n",
            "TIMESTEP 226 / STATE explore / EPSILON 0.009917500000000013 / ACTION 1 / REWARD 0.0 / Q_MAX  6.710673 / Loss  0.010132206603884697\n",
            "TIMESTEP 227 / STATE explore / EPSILON 0.009914200000000014 / ACTION 2 / REWARD 0.0 / Q_MAX  6.6855574 / Loss  0.008027682080864906\n",
            "TIMESTEP 228 / STATE explore / EPSILON 0.009910900000000014 / ACTION 2 / REWARD 0.0 / Q_MAX  6.3309402 / Loss  0.027388855814933777\n",
            "TIMESTEP 229 / STATE explore / EPSILON 0.009907600000000015 / ACTION 2 / REWARD 0.0 / Q_MAX  6.4952497 / Loss  0.00803875457495451\n",
            "TIMESTEP 230 / STATE explore / EPSILON 0.009904300000000015 / ACTION 2 / REWARD 0.0 / Q_MAX  6.4153366 / Loss  0.007964303717017174\n",
            "TIMESTEP 231 / STATE explore / EPSILON 0.009901000000000016 / ACTION 2 / REWARD 0.0 / Q_MAX  6.3719907 / Loss  0.003270914778113365\n",
            "TIMESTEP 232 / STATE explore / EPSILON 0.009897700000000016 / ACTION 1 / REWARD 0.0 / Q_MAX  6.043507 / Loss  0.003018546849489212\n",
            "TIMESTEP 233 / STATE explore / EPSILON 0.009894400000000017 / ACTION 1 / REWARD 0.0 / Q_MAX  6.236407 / Loss  0.0026708911173045635\n",
            "TIMESTEP 234 / STATE explore / EPSILON 0.009891100000000017 / ACTION 1 / REWARD 0.0 / Q_MAX  6.2051363 / Loss  0.04297730699181557\n",
            "TIMESTEP 235 / STATE explore / EPSILON 0.009887800000000018 / ACTION 2 / REWARD 0.0 / Q_MAX  6.3282237 / Loss  0.006826960016041994\n",
            "TIMESTEP 236 / STATE explore / EPSILON 0.009884500000000018 / ACTION 1 / REWARD 0.0 / Q_MAX  6.3566995 / Loss  0.009536477737128735\n",
            "TIMESTEP 237 / STATE explore / EPSILON 0.009881200000000019 / ACTION 1 / REWARD 0.0 / Q_MAX  6.368221 / Loss  0.009901231154799461\n",
            "TIMESTEP 238 / STATE explore / EPSILON 0.00987790000000002 / ACTION 1 / REWARD 0.0 / Q_MAX  5.9849916 / Loss  0.005698100198060274\n",
            "TIMESTEP 239 / STATE explore / EPSILON 0.00987460000000002 / ACTION 2 / REWARD 0.0 / Q_MAX  6.026648 / Loss  0.0024206042289733887\n",
            "TIMESTEP 240 / STATE explore / EPSILON 0.00987130000000002 / ACTION 2 / REWARD 0.0 / Q_MAX  6.251096 / Loss  0.03743419423699379\n",
            "TIMESTEP 241 / STATE explore / EPSILON 0.00986800000000002 / ACTION 2 / REWARD -1.0 / Q_MAX  6.2386856 / Loss  0.005391557700932026\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 1\n",
            "We finish building the model\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 242 / STATE explore / EPSILON 0.009864700000000021 / ACTION 2 / REWARD 0.0 / Q_MAX  13.713559 / Loss  0.007594251539558172\n",
            "TIMESTEP 243 / STATE explore / EPSILON 0.009861400000000022 / ACTION 2 / REWARD 0.0 / Q_MAX  13.162386 / Loss  0.027432434260845184\n",
            "TIMESTEP 244 / STATE explore / EPSILON 0.009858100000000022 / ACTION 2 / REWARD 0.0 / Q_MAX  11.978588 / Loss  0.018024789169430733\n",
            "TIMESTEP 245 / STATE explore / EPSILON 0.009854800000000023 / ACTION 1 / REWARD 0.0 / Q_MAX  11.653226 / Loss  0.008889122866094112\n",
            "TIMESTEP 246 / STATE explore / EPSILON 0.009851500000000023 / ACTION 1 / REWARD 0.0 / Q_MAX  10.657451 / Loss  0.008919005282223225\n",
            "TIMESTEP 247 / STATE explore / EPSILON 0.009848200000000024 / ACTION 2 / REWARD 0.0 / Q_MAX  9.956074 / Loss  0.009532289579510689\n",
            "TIMESTEP 248 / STATE explore / EPSILON 0.009844900000000024 / ACTION 1 / REWARD 0.0 / Q_MAX  9.999714 / Loss  0.011614331975579262\n",
            "TIMESTEP 249 / STATE explore / EPSILON 0.009841600000000025 / ACTION 2 / REWARD 0.0 / Q_MAX  9.402068 / Loss  0.010504772886633873\n",
            "TIMESTEP 250 / STATE explore / EPSILON 0.009838300000000025 / ACTION 2 / REWARD 0.0 / Q_MAX  8.536233 / Loss  0.015163966454565525\n",
            "TIMESTEP 251 / STATE explore / EPSILON 0.009835000000000026 / ACTION 2 / REWARD 0.0 / Q_MAX  8.588763 / Loss  0.03722602128982544\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}