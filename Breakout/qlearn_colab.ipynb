{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qlearn1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5Rirj0VNWwq",
        "outputId": "ae2eb918-3dbb-4ae1-b261-83e7e52a3b87"
      },
      "source": [
        "!pip install agent"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting agent\n",
            "  Downloading https://files.pythonhosted.org/packages/85/69/3586641905a917f4929d584a79ac64f6df842f4e3ee51301643a73a8196e/agent-0.1.2.tar.gz\n",
            "Building wheels for collected packages: agent\n",
            "  Building wheel for agent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for agent: filename=agent-0.1.2-cp37-none-any.whl size=4487 sha256=429e4723432c995830b9229e543347dd06f73a6ac266167cf7168ff1dfcee42a\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/54/c0/5ad0bd3bc87c06d3e131eda83be31bb8b48e340637fcc9e56a\n",
            "Successfully built agent\n",
            "Installing collected packages: agent\n",
            "Successfully installed agent-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdYve4Htb0_w"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hqnsP3c8uA"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXbGMPQxbxk7",
        "outputId": "10688f17-c089-498a-83a9-635302b2d2cf"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\n",
        "#https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import skimage as skimage\n",
        "from skimage import transform, color, exposure\n",
        "from skimage.transform import rotate\n",
        "from skimage.viewer import ImageViewer\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import json\n",
        "from tensorflow.keras.initializers import identity\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD , Adam\n",
        "import tensorflow as tf\n",
        "import agent\n",
        "# Import the gym module\n",
        "import gym\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "GAME = 'atari' # the name of the game being played for log files\n",
        "CONFIG = 'nothreshold'\n",
        "ACTIONS = 4 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVATION = 3200. # timesteps to observe before training. de cada 3200 frames, vamos ao nosso buffer e selecionamos de forma aleatoria um batch size. Neste caso, 32 frames. em numpy arrays\n",
        "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.1 # starting value of epsilon EPSILON é para ver o exploration vs exploitation\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "#MAX_STEPS_PER_EPISODE = 1000\n",
        "EPISODES = 10000\n",
        "\n",
        "img_rows, img_cols = 84, 84\n",
        "#Convert image into Black and white\n",
        "img_channels = 4 #We stack 4 frames\n",
        "\n",
        "\n",
        "\n",
        "def buildmodel():\n",
        "    print(\"Now we build the model\")\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters = 32, kernel_size = (8, 8), strides = (4, 4), padding = 'same', input_shape = (img_rows, img_cols, img_channels)))  #80*80*4\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (4, 4), strides = (2, 2), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(2))\n",
        "\n",
        "    adam = Adam(learning_rate = LEARNING_RATE)\n",
        "    model.compile(loss='mse', optimizer = adam)\n",
        "    print(\"We finish building the model\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n",
        "def trainNetwork(model,args):\n",
        "    # open up a game state to communicate with emulator\n",
        "    env = wrap_env(gym.make('BreakoutDeterministic-v4'))\n",
        "    env.reset()\n",
        "    # store the previous observations in replay memory\n",
        "    D = deque()\n",
        "    \n",
        "    #----------------------------------------\n",
        "    #PARA OBTER O SIGNIFICADO DAS AÇÕES POSSíVEIS\n",
        "    print(env.unwrapped.get_action_meanings())\n",
        "    #----------------------------------------\n",
        "    \n",
        "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        \n",
        "    x_t, r_0, terminal, info = env.step(1) #COMEÇAR O JOGO COM A AÇÃO \"FIRE\"\n",
        "    env.render()\n",
        "\n",
        "    x_t = skimage.color.rgb2gray(x_t)\n",
        "    x_t = skimage.transform.resize(x_t, (84,84))\n",
        "    x_t = skimage.exposure.rescale_intensity(x_t, out_range = (0,255))\n",
        "\n",
        "    x_t = x_t / 255.0\n",
        "\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2) #colocar a sequência de frames. 4 frames sequenciais, que vamos aplicar à nossa lista. Para conseguir a estabilidade de imagens sequenciais\n",
        "    \n",
        "    #print (s_t.shape)\n",
        "\n",
        "    #In Keras, need to reshape\n",
        "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
        "\n",
        "\n",
        "\n",
        "    if args['mode'] == 'Run':\n",
        "        OBSERVE = 999999999\t#We keep observe, never train\n",
        "        epsilon = FINAL_EPSILON # higher epsilon, more timestamps?\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"model_v1.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        t = 0\n",
        "\n",
        "    elif args['mode'] == 'CTrain': #Continue previous train\n",
        "        OBSERVE = OBSERVATION\n",
        "        epsilon = 0.07823368810419994 #0.08811709480229288\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"model.h5\")\n",
        "        adam = Adam(lr = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        t = 0 #360045\n",
        "\n",
        "    else:\t\t\t\t\t   #We go to training mode -> -m \"Train\"\n",
        "        OBSERVE = OBSERVATION\n",
        "        epsilon = INITIAL_EPSILON #o EPSILON é o que divide a parte de exploration vs exploitation. se for abaixo de um dado valor é exploration. Caso contrário é exploitation\n",
        "        t = 0\n",
        "\n",
        "    while (not terminal):\n",
        "    #for i in range(MAX_STEPS_PER_EPISODE):\n",
        "        loss = 0\n",
        "        Q_sa = 0 # Q(s, a) representing the maximum discounted future reward when we perform action a in state s.\n",
        "        action_index = 0\n",
        "        r_t = 0 #reward\n",
        "        a_t = np.zeros([ACTIONS]) #action\n",
        "        #choose an action epsilon greedy\n",
        "        if t % FRAME_PER_ACTION == 0:\n",
        "            if random.random() <= epsilon:\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
        "                a_t[action_index] = 1\n",
        "                \n",
        "            else:\n",
        "                q = model.predict(s_t)\t   #input a stack of 4 images, get the prediction\n",
        "                max_Q = np.argmax(q)\n",
        "                action_index = max_Q\n",
        "                a_t[max_Q] = 1\n",
        "\n",
        "        #We reduce the epsilon gradually\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        #run the selected action and observed next state and reward. DEPOIS DE UM \"STEP\" correr sempre o \"RENDER\"\n",
        "        x_t1_colored, r_t, terminal, info = env.step(np.where(a_t == 1)[0][0]) #FUNÇÂO \"WHERE\" para obter o índice do valor do array que está a 1\n",
        "        env.render()\n",
        "        \n",
        "        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n",
        "        x_t1 = skimage.transform.resize(x_t1, (84, 84))\n",
        "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range = (0, 255))\n",
        "        \n",
        "        x_t1 = x_t1 / 255.0\n",
        "\n",
        "\n",
        "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis = 3)\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        #only train if done observing\n",
        "        if t > OBSERVE: #train ou update da nossa rede. de quantas em quantas frames vamos precisar para fazer um treino. se replay_mem começar a ficar mto cheio retira a última entrada. e fazemos append das novas decisoes que foram sendo adquiridas\n",
        "            #sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "\n",
        "            #Now we do the experience replay\n",
        "            state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "            state_t = np.concatenate(state_t)\n",
        "            state_t1 = np.concatenate(state_t1)\n",
        "            targets = model.predict(state_t)\n",
        "            Q_sa = model.predict(state_t1)\n",
        "            targets[range(BATCH), action_t] = reward_t + GAMMA * np.max(Q_sa, axis = 1) * np.invert(terminal) #qual o target associado\n",
        "\n",
        "            loss += model.train_on_batch(state_t, targets) #quanto mais proximo de zero, mais proximo está de convergir para conseguir estimar o key value de acordo com o par (estado, ação)\n",
        "            \n",
        "        s_t = s_t1\n",
        "        t = t + 1\n",
        "\n",
        "        # save progress every 1000 iterations\n",
        "        if t % 10 == 0:\n",
        "            print(\"Now we save model\")\n",
        "            model.save_weights(\"model.h5\", overwrite = True)\n",
        "            with open(\"model.json\", \"w\") as outfile:\n",
        "                json.dump(model.to_json(), outfile)\n",
        "\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
        "        print(\"LOSS\", loss)\n",
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
        "    env.close()\n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")\n",
        "\n",
        "def playGame(args):\n",
        "    model = buildmodel()\n",
        "    trainNetwork(model,args)\n",
        "\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description = 'Description of your program')\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True)\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True) adicionar o argumento de número de episódios\n",
        "    #args = vars(parser.parse_args())\n",
        "    for i in range(EPISODES):\n",
        "        print(\"EPISODE\", i)\n",
        "        playGame({'mode': 'Train'})\n",
        "        #show_video()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPISODE 0\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 1\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 240 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 241 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 242 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 243 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 244 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 245 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 246 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 247 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 248 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 249 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 250 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 251 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 252 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 253 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 254 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 255 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 256 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 257 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 258 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 259 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 260 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 261 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 262 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 263 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 264 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 265 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 266 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 267 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 268 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 269 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 270 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 271 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 272 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 273 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 274 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 275 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 276 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 277 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 278 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 279 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 280 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 281 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 282 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 283 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 284 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 285 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 286 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 287 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 288 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 289 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 290 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 291 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 292 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 293 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 294 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 295 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 296 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 297 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 298 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 299 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 300 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 301 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 302 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 303 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 304 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 305 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 306 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 307 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 308 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 309 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 310 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 311 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 312 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 313 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 314 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 315 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 316 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 317 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 318 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 319 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 320 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 321 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 322 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 323 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 324 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 325 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 326 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 327 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 328 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 329 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 330 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 331 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 332 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 333 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 334 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 335 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 336 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 337 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 338 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 339 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 340 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 341 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 342 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 343 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 344 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 345 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 346 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 347 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 348 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 349 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 350 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 351 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 352 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 353 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 354 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 355 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 356 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 357 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 358 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 359 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 360 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 361 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 362 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 363 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 364 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 365 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 366 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 367 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 368 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 369 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 370 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 371 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 372 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 373 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 374 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 375 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 376 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 377 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 378 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 379 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 380 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 381 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 382 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 383 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 384 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 385 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 2\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 240 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 241 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 242 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 243 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 244 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 245 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 246 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 247 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 248 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 249 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 250 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 251 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 252 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 253 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 254 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 255 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 256 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 257 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 258 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 259 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 260 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 261 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 5\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 6\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 7\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 240 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 241 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 242 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 243 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 244 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 245 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 246 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 247 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 248 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 249 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 250 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 251 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 252 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 253 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 254 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 255 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 256 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 257 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 258 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 259 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 260 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 261 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 262 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 263 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 264 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 265 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 266 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 267 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 268 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 269 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 270 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 271 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 272 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 273 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 274 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 275 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 276 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 277 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 278 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 279 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 280 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 281 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 282 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 283 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}