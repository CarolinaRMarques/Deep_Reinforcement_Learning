{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
<<<<<<< HEAD
      "name": "qlearn_colab.ipynb",
=======
      "name": "qlearn1.ipynb",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5Rirj0VNWwq",
<<<<<<< HEAD
        "outputId": "15e3f387-2434-4209-d716-672167840d12"
=======
        "outputId": "ae2eb918-3dbb-4ae1-b261-83e7e52a3b87"
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
      },
      "source": [
        "!pip install agent"
      ],
<<<<<<< HEAD
      "execution_count": 1,
=======
      "execution_count": 2,
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting agent\n",
            "  Downloading https://files.pythonhosted.org/packages/85/69/3586641905a917f4929d584a79ac64f6df842f4e3ee51301643a73a8196e/agent-0.1.2.tar.gz\n",
            "Building wheels for collected packages: agent\n",
            "  Building wheel for agent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
<<<<<<< HEAD
            "  Created wheel for agent: filename=agent-0.1.2-cp37-none-any.whl size=4487 sha256=e3ae72e4f7206abd57d993cc38f128e435e35408860d5b22426eb1517d0af155\n",
=======
            "  Created wheel for agent: filename=agent-0.1.2-cp37-none-any.whl size=4487 sha256=429e4723432c995830b9229e543347dd06f73a6ac266167cf7168ff1dfcee42a\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
            "  Stored in directory: /root/.cache/pip/wheels/34/54/c0/5ad0bd3bc87c06d3e131eda83be31bb8b48e340637fcc9e56a\n",
            "Successfully built agent\n",
            "Installing collected packages: agent\n",
            "Successfully installed agent-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "KdYve4Htb0_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241723e2-469f-4419-b98c-ce1737dfa4ce"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!pip install atari_py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH6yAoJDzo4Z",
        "outputId": "ed08e5ab-9fb9-46f9-c912-e688384a295f"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
=======
        "id": "KdYve4Htb0_w"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": []
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "l1hqnsP3c8uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33863d17-31b7-4a37-ad7c-d169bbbaec9c"
      },
      "source": [
        "!python -m atari_py.import_roms '/content/drive/My Drive/CN_Breakout/ROMS'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying mr_do.bin from /content/drive/My Drive/CN_Breakout/ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying lost_luggage.bin from /content/drive/My Drive/CN_Breakout/ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying elevator_action.bin from /content/drive/My Drive/CN_Breakout/ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying asterix.bin from /content/drive/My Drive/CN_Breakout/ROMS/Asterix (AKA Taz) (1984) (Atari, Jerome Domurat, Steve Woita) (CX2696).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying riverraid.bin from /content/drive/My Drive/CN_Breakout/ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying video_pinball.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying road_runner.bin from patched version of /content/drive/My Drive/CN_Breakout/ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying qbert.bin from /content/drive/My Drive/CN_Breakout/ROMS/Q. Bert (1983) (CCE) (C-822).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying surround.bin from /content/drive/My Drive/CN_Breakout/ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying ms_pacman.bin from /content/drive/My Drive/CN_Breakout/ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying up_n_down.bin from /content/drive/My Drive/CN_Breakout/ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying kung_fu_master.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying atlantis.bin from /content/drive/My Drive/CN_Breakout/ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying pitfall.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying chopper_command.bin from /content/drive/My Drive/CN_Breakout/ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying ice_hockey.bin from /content/drive/My Drive/CN_Breakout/ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying breakout.bin from /content/drive/My Drive/CN_Breakout/ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying boxing.bin from /content/drive/My Drive/CN_Breakout/ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying freeway.bin from /content/drive/My Drive/CN_Breakout/ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying kaboom.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying skiing.bin from /content/drive/My Drive/CN_Breakout/ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying tennis.bin from /content/drive/My Drive/CN_Breakout/ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying bank_heist.bin from /content/drive/My Drive/CN_Breakout/ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying demon_attack.bin from /content/drive/My Drive/CN_Breakout/ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying koolaid.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying star_gunner.bin from /content/drive/My Drive/CN_Breakout/ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying trondead.bin from /content/drive/My Drive/CN_Breakout/ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying robotank.bin from /content/drive/My Drive/CN_Breakout/ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying space_invaders.bin from /content/drive/My Drive/CN_Breakout/ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying hero.bin from /content/drive/My Drive/CN_Breakout/ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying montezuma_revenge.bin from /content/drive/My Drive/CN_Breakout/ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying amidar.bin from /content/drive/My Drive/CN_Breakout/ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying asteroids.bin from /content/drive/My Drive/CN_Breakout/ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying air_raid.bin from /content/drive/My Drive/CN_Breakout/ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying assault.bin from /content/drive/My Drive/CN_Breakout/ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying alien.bin from /content/drive/My Drive/CN_Breakout/ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying berzerk.bin from /content/drive/My Drive/CN_Breakout/ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from /content/drive/My Drive/CN_Breakout/ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying beam_rider.bin from /content/drive/My Drive/CN_Breakout/ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying battle_zone.bin from /content/drive/My Drive/CN_Breakout/ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying centipede.bin from /content/drive/My Drive/CN_Breakout/ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying crazy_climber.bin from /content/drive/My Drive/CN_Breakout/ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying carnival.bin from /content/drive/My Drive/CN_Breakout/ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying donkey_kong.bin from /content/drive/My Drive/CN_Breakout/ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying defender.bin from /content/drive/My Drive/CN_Breakout/ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying double_dunk.bin from /content/drive/My Drive/CN_Breakout/ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying enduro.bin from /content/drive/My Drive/CN_Breakout/ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying frogger.bin from /content/drive/My Drive/CN_Breakout/ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from /content/drive/My Drive/CN_Breakout/ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying fishing_derby.bin from /content/drive/My Drive/CN_Breakout/ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying gopher.bin from /content/drive/My Drive/CN_Breakout/ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from /content/drive/My Drive/CN_Breakout/ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying galaxian.bin from /content/drive/My Drive/CN_Breakout/ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying krull.bin from /content/drive/My Drive/CN_Breakout/ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying journey_escape.bin from /content/drive/My Drive/CN_Breakout/ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kangaroo.bin from /content/drive/My Drive/CN_Breakout/ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying name_this_game.bin from /content/drive/My Drive/CN_Breakout/ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from /content/drive/My Drive/CN_Breakout/ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying private_eye.bin from /content/drive/My Drive/CN_Breakout/ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying pooyan.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying seaquest.bin from /content/drive/My Drive/CN_Breakout/ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying solaris.bin from /content/drive/My Drive/CN_Breakout/ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying pong.bin from /content/drive/My Drive/CN_Breakout/ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from /content/drive/My Drive/CN_Breakout/ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying tutankham.bin from /content/drive/My Drive/CN_Breakout/ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying zaxxon.bin from /content/drive/My Drive/CN_Breakout/ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying venture.bin from /content/drive/My Drive/CN_Breakout/ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying yars_revenge.bin from /content/drive/My Drive/CN_Breakout/ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying time_pilot.bin from /content/drive/My Drive/CN_Breakout/ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying jamesbond.bin from /content/drive/My Drive/CN_Breakout/ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying adventure.bin from /content/drive/My Drive/CN_Breakout/ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying pacman.bin from /content/drive/My Drive/CN_Breakout/ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying keystone_kapers.bin from /content/drive/My Drive/CN_Breakout/ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from /content/drive/My Drive/CN_Breakout/ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from /content/drive/My Drive/CN_Breakout/ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying sir_lancelot.bin from /content/drive/My Drive/CN_Breakout/ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n"
          ],
          "name": "stdout"
        }
      ]
=======
        "id": "l1hqnsP3c8uA"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXbGMPQxbxk7",
<<<<<<< HEAD
        "outputId": "1043052b-a4ad-474a-84dc-cb283397f378"
=======
        "outputId": "10688f17-c089-498a-83a9-635302b2d2cf"
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\n",
        "#https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import skimage as skimage\n",
        "from skimage import transform, color, exposure\n",
        "from skimage.transform import rotate\n",
        "from skimage.viewer import ImageViewer\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import json\n",
        "from tensorflow.keras.initializers import identity\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD , Adam\n",
        "import tensorflow as tf\n",
<<<<<<< HEAD
        "#import agent\n",
=======
        "import agent\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "# Import the gym module\n",
        "import gym\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
<<<<<<< HEAD
        "\n",
=======
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "GAME = 'atari' # the name of the game being played for log files\n",
        "CONFIG = 'nothreshold'\n",
        "ACTIONS = 4 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
<<<<<<< HEAD
        "OBSERVATION = 100. # timesteps to observe before training. de cada 3200 frames, vamos ao nosso buffer e selecionamos de forma aleatoria um batch size. Neste caso, 32 frames. em numpy arrays\n",
        "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.01 # starting value of epsilon EPSILON é para ver o exploration vs exploitation\n",
=======
        "OBSERVATION = 3200. # timesteps to observe before training. de cada 3200 frames, vamos ao nosso buffer e selecionamos de forma aleatoria um batch size. Neste caso, 32 frames. em numpy arrays\n",
        "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.1 # starting value of epsilon EPSILON é para ver o exploration vs exploitation\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "#MAX_STEPS_PER_EPISODE = 1000\n",
        "EPISODES = 10000\n",
<<<<<<< HEAD
        "q_max_list = []\n",
        "loss_list = []\n",
        "reward_list = []\n",
=======
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "\n",
        "img_rows, img_cols = 84, 84\n",
        "#Convert image into Black and white\n",
        "img_channels = 4 #We stack 4 frames\n",
        "\n",
<<<<<<< HEAD
        "def buildmodel():\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = tf.keras.layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = Conv2D(32, 8, strides=4, activation=\"relu\", padding = 'same')(inputs)\n",
        "    layer2 = Conv2D(64, 4, strides=2, activation=\"relu\", padding = 'same')(layer1)\n",
        "    layer3 = Conv2D(64, 3, strides=1, activation=\"relu\", padding = 'same')(layer2)\n",
        "\n",
        "    layer4 = Flatten()(layer3)\n",
        "\n",
        "    layer5 = Dense(512, activation=\"relu\")(layer4)\n",
        "    action = Dense(4, activation=\"linear\")(layer5)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "def buildmodel2():\n",
=======
        "\n",
        "\n",
        "def buildmodel():\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "    print(\"Now we build the model\")\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters = 32, kernel_size = (8, 8), strides = (4, 4), padding = 'same', input_shape = (img_rows, img_cols, img_channels)))  #80*80*4\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (4, 4), strides = (2, 2), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
<<<<<<< HEAD
        "    model.add(Dense(4))\n",
=======
        "    model.add(Dense(2))\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "\n",
        "    adam = Adam(learning_rate = LEARNING_RATE)\n",
        "    model.compile(loss='mse', optimizer = adam)\n",
        "    print(\"We finish building the model\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n",
        "def trainNetwork(model,args):\n",
        "    # open up a game state to communicate with emulator\n",
        "    env = wrap_env(gym.make('BreakoutDeterministic-v4'))\n",
        "    env.reset()\n",
        "    # store the previous observations in replay memory\n",
<<<<<<< HEAD
        "    \n",
        "    #----------------------------------------\n",
        "    #PARA OBTER O SIGNIFICADO DAS AÇÕES POSSíVEIS\n",
        "    #print(env.unwrapped.get_action_meanings())\n",
=======
        "    D = deque()\n",
        "    \n",
        "    #----------------------------------------\n",
        "    #PARA OBTER O SIGNIFICADO DAS AÇÕES POSSíVEIS\n",
        "    print(env.unwrapped.get_action_meanings())\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "    #----------------------------------------\n",
        "    \n",
        "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        \n",
        "    x_t, r_0, terminal, info = env.step(1) #COMEÇAR O JOGO COM A AÇÃO \"FIRE\"\n",
        "    env.render()\n",
        "\n",
<<<<<<< HEAD
        "    D = args['D']\n",
        "\n",
=======
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "    x_t = skimage.color.rgb2gray(x_t)\n",
        "    x_t = skimage.transform.resize(x_t, (84,84))\n",
        "    x_t = skimage.exposure.rescale_intensity(x_t, out_range = (0,255))\n",
        "\n",
        "    x_t = x_t / 255.0\n",
        "\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2) #colocar a sequência de frames. 4 frames sequenciais, que vamos aplicar à nossa lista. Para conseguir a estabilidade de imagens sequenciais\n",
        "    \n",
        "    #print (s_t.shape)\n",
        "\n",
        "    #In Keras, need to reshape\n",
        "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
        "\n",
<<<<<<< HEAD
        "    t = args['t']\n",
=======
        "\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "\n",
        "    if args['mode'] == 'Run':\n",
        "        OBSERVE = 999999999\t#We keep observe, never train\n",
        "        epsilon = FINAL_EPSILON # higher epsilon, more timestamps?\n",
        "        print (\"Now we load weight\")\n",
<<<<<<< HEAD
        "        model.load_weights(\"/content/drive/My Drive/CN_Breakout/models/model_v1.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "\n",
        "    elif args['mode'] == 'CTrain': #Continue previous train\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = 0.07823368810419994 #0.08811709480229288\n",
        "        epsilon = args['epsilon']\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/My Drive/CN_Breakout/models/model.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        \n",
        "\n",
        "    else:\t\t\t\t\t   #We go to training mode -> -m \"Train\"\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = INITIAL_EPSILON #o EPSILON é o que divide a parte de exploration vs exploitation. se for abaixo de um dado valor é exploration. Caso contrário é exploitation\n",
        "        epsilon = args['epsilon']\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
=======
        "        model.load_weights(\"model_v1.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        t = 0\n",
        "\n",
        "    elif args['mode'] == 'CTrain': #Continue previous train\n",
        "        OBSERVE = OBSERVATION\n",
        "        epsilon = 0.07823368810419994 #0.08811709480229288\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"model.h5\")\n",
        "        adam = Adam(lr = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        t = 0 #360045\n",
        "\n",
        "    else:\t\t\t\t\t   #We go to training mode -> -m \"Train\"\n",
        "        OBSERVE = OBSERVATION\n",
        "        epsilon = INITIAL_EPSILON #o EPSILON é o que divide a parte de exploration vs exploitation. se for abaixo de um dado valor é exploration. Caso contrário é exploitation\n",
        "        t = 0\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "\n",
        "    while (not terminal):\n",
        "    #for i in range(MAX_STEPS_PER_EPISODE):\n",
        "        loss = 0\n",
        "        Q_sa = 0 # Q(s, a) representing the maximum discounted future reward when we perform action a in state s.\n",
        "        action_index = 0\n",
        "        r_t = 0 #reward\n",
        "        a_t = np.zeros([ACTIONS]) #action\n",
        "        #choose an action epsilon greedy\n",
        "        if t % FRAME_PER_ACTION == 0:\n",
        "            if random.random() <= epsilon:\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
<<<<<<< HEAD
        "                while action_index == 1:\n",
        "                  action_index = random.randrange(ACTIONS)\n",
=======
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "                a_t[action_index] = 1\n",
        "                \n",
        "            else:\n",
        "                q = model.predict(s_t)\t   #input a stack of 4 images, get the prediction\n",
        "                max_Q = np.argmax(q)\n",
        "                action_index = max_Q\n",
        "                a_t[max_Q] = 1\n",
        "\n",
        "        #We reduce the epsilon gradually\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        #run the selected action and observed next state and reward. DEPOIS DE UM \"STEP\" correr sempre o \"RENDER\"\n",
        "        x_t1_colored, r_t, terminal, info = env.step(np.where(a_t == 1)[0][0]) #FUNÇÂO \"WHERE\" para obter o índice do valor do array que está a 1\n",
        "        env.render()\n",
        "        \n",
        "        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n",
        "        x_t1 = skimage.transform.resize(x_t1, (84, 84))\n",
        "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range = (0, 255))\n",
        "        \n",
        "        x_t1 = x_t1 / 255.0\n",
        "\n",
        "\n",
        "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis = 3)\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        #only train if done observing\n",
        "        if t > OBSERVE: #train ou update da nossa rede. de quantas em quantas frames vamos precisar para fazer um treino. se replay_mem começar a ficar mto cheio retira a última entrada. e fazemos append das novas decisoes que foram sendo adquiridas\n",
        "            #sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "\n",
        "            #Now we do the experience replay\n",
        "            state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "            state_t = np.concatenate(state_t)\n",
        "            state_t1 = np.concatenate(state_t1)\n",
        "            targets = model.predict(state_t)\n",
        "            Q_sa = model.predict(state_t1)\n",
        "            targets[range(BATCH), action_t] = reward_t + GAMMA * np.max(Q_sa, axis = 1) * np.invert(terminal) #qual o target associado\n",
<<<<<<< HEAD
        "            \n",
=======
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "\n",
        "            loss += model.train_on_batch(state_t, targets) #quanto mais proximo de zero, mais proximo está de convergir para conseguir estimar o key value de acordo com o par (estado, ação)\n",
        "            \n",
        "        s_t = s_t1\n",
        "        t = t + 1\n",
        "\n",
        "        # save progress every 1000 iterations\n",
        "        if t % 10 == 0:\n",
        "            print(\"Now we save model\")\n",
<<<<<<< HEAD
        "            model.save_weights(\"/content/drive/My Drive/CN_Breakout/models/model.h5\", overwrite = True)\n",
=======
        "            model.save_weights(\"model.h5\", overwrite = True)\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "            with open(\"model.json\", \"w\") as outfile:\n",
        "                json.dump(model.to_json(), outfile)\n",
        "\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
<<<<<<< HEAD
=======
        "        print(\"LOSS\", loss)\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
<<<<<<< HEAD
        "        \n",
        "        q_max_list.append(np.max(Q_sa))\n",
        "        loss_list.append(loss)\n",
        "        reward_list.append(r_t)\n",
        "\n",
        "    env.close()\n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")\n",
        "    return t, epsilon, D\n",
        "\n",
        "def playGame(args):\n",
        "    model = buildmodel()\n",
        "    t, epsilon, D = trainNetwork(model,args)\n",
        "    return t, epsilon, D\n",
=======
        "    env.close()\n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")\n",
        "\n",
        "def playGame(args):\n",
        "    model = buildmodel()\n",
        "    trainNetwork(model,args)\n",
        "\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description = 'Description of your program')\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True)\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True) adicionar o argumento de número de episódios\n",
        "    #args = vars(parser.parse_args())\n",
<<<<<<< HEAD
        "    t = 0\n",
        "    epsilon = INITIAL_EPSILON\n",
        "    D = deque()\n",
        "    for i in range(EPISODES):\n",
        "        print(\"EPISODE\", i)\n",
        "        tp, epsilonp, Dp = playGame({'mode': 'CTrain', 't': t, 'epsilon': epsilon, 'D': D})\n",
        "        #show_video()\n",
        "        t = tp\n",
        "        epsilon = epsilonp\n",
        "        D = Dp\n",
=======
        "    for i in range(EPISODES):\n",
        "        print(\"EPISODE\", i)\n",
        "        playGame({'mode': 'Train'})\n",
        "        #show_video()\n",
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Viewer requires Qt\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "EPISODE 3798\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3900 / STATE explore / EPSILON 0.009987463299997373 / ACTION 2 / REWARD 0.0 / Q_MAX  5133.206 / Loss  1313.6915283203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3799\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3901 / STATE explore / EPSILON 0.009987459999997372 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  598.776611328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3800\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3902 / STATE explore / EPSILON 0.009987456699997371 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  586.0070190429688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3801\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3903 / STATE explore / EPSILON 0.00998745339999737 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  568.1279296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3802\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3904 / STATE explore / EPSILON 0.00998745009999737 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  565.9852294921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3803\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3905 / STATE explore / EPSILON 0.00998744679999737 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  555.16650390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3804\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3906 / STATE explore / EPSILON 0.009987443499997369 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  566.24658203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3805\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3907 / STATE explore / EPSILON 0.009987440199997368 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  572.6533203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3806\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3908 / STATE explore / EPSILON 0.009987436899997367 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  591.22802734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3807\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3909 / STATE explore / EPSILON 0.009987433599997366 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  578.9485473632812\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3808\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3910 / STATE explore / EPSILON 0.009987430299997366 / ACTION 3 / REWARD 0.0 / Q_MAX  5198.495 / Loss  591.506591796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3809\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3911 / STATE explore / EPSILON 0.009987426999997365 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1432.929931640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3810\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3912 / STATE explore / EPSILON 0.009987423699997364 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1700.693115234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3811\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3913 / STATE explore / EPSILON 0.009987420399997364 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1327.2918701171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3812\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3914 / STATE explore / EPSILON 0.009987417099997363 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1435.86962890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3813\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3915 / STATE explore / EPSILON 0.009987413799997362 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1753.8543701171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3814\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3916 / STATE explore / EPSILON 0.009987410499997362 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1440.578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3815\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3917 / STATE explore / EPSILON 0.009987407199997361 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1135.52001953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3816\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3918 / STATE explore / EPSILON 0.00998740389999736 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1537.5555419921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3817\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3919 / STATE explore / EPSILON 0.00998740059999736 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1178.4134521484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3818\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3920 / STATE explore / EPSILON 0.009987397299997359 / ACTION 2 / REWARD 0.0 / Q_MAX  5072.9424 / Loss  1288.049072265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3819\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3921 / STATE explore / EPSILON 0.009987393999997358 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  639.5966186523438\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3820\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3922 / STATE explore / EPSILON 0.009987390699997357 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  640.1337280273438\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3821\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3923 / STATE explore / EPSILON 0.009987387399997357 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  655.9594116210938\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3822\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3924 / STATE explore / EPSILON 0.009987384099997356 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  630.4061279296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3823\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3925 / STATE explore / EPSILON 0.009987380799997355 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  635.1207885742188\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3824\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3926 / STATE explore / EPSILON 0.009987377499997355 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  629.6222534179688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3825\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3927 / STATE explore / EPSILON 0.009987374199997354 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  657.4646606445312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3826\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3928 / STATE explore / EPSILON 0.009987370899997353 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  651.829345703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3827\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3929 / STATE explore / EPSILON 0.009987367599997353 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  652.6417846679688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3828\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3930 / STATE explore / EPSILON 0.009987364299997352 / ACTION 3 / REWARD 0.0 / Q_MAX  5182.006 / Loss  650.3717651367188\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3829\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3931 / STATE explore / EPSILON 0.009987360999997351 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  2085.33642578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3830\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3932 / STATE explore / EPSILON 0.00998735769999735 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1505.39453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3831\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3933 / STATE explore / EPSILON 0.00998735439999735 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1794.376953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3832\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3934 / STATE explore / EPSILON 0.00998735109999735 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1372.203857421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3833\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3935 / STATE explore / EPSILON 0.009987347799997348 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1851.4044189453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3834\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3936 / STATE explore / EPSILON 0.009987344499997348 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1602.8770751953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3835\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3937 / STATE explore / EPSILON 0.009987341199997347 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1775.8819580078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3836\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3938 / STATE explore / EPSILON 0.009987337899997346 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1599.6553955078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3837\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3939 / STATE explore / EPSILON 0.009987334599997346 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1599.6553955078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3838\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3940 / STATE explore / EPSILON 0.009987331299997345 / ACTION 2 / REWARD 0.0 / Q_MAX  5316.4336 / Loss  1429.8720703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3839\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3941 / STATE explore / EPSILON 0.009987327999997344 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  694.013671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3840\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3942 / STATE explore / EPSILON 0.009987324699997344 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  669.3665771484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3841\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3943 / STATE explore / EPSILON 0.009987321399997343 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  639.128173828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3842\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3944 / STATE explore / EPSILON 0.009987318099997342 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  635.2999877929688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3843\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3945 / STATE explore / EPSILON 0.009987314799997342 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  630.6751708984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3844\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3946 / STATE explore / EPSILON 0.00998731149999734 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  648.253662109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3845\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3947 / STATE explore / EPSILON 0.00998730819999734 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  657.5032348632812\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3846\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3948 / STATE explore / EPSILON 0.00998730489999734 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  673.8231201171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3847\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3949 / STATE explore / EPSILON 0.009987301599997339 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  694.318603515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3848\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3950 / STATE explore / EPSILON 0.009987298299997338 / ACTION 3 / REWARD 0.0 / Q_MAX  5431.3604 / Loss  690.3033447265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3849\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3951 / STATE explore / EPSILON 0.009987294999997337 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1429.517578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3850\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3952 / STATE explore / EPSILON 0.009987291699997337 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1287.7646484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3851\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3953 / STATE explore / EPSILON 0.009987288399997336 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1433.210205078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3852\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3954 / STATE explore / EPSILON 0.009987285099997335 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1638.667724609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3853\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3955 / STATE explore / EPSILON 0.009987281799997335 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1365.7333984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3854\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3956 / STATE explore / EPSILON 0.009987278499997334 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1462.857421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3855\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3957 / STATE explore / EPSILON 0.009987275199997333 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1428.9442138671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3856\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3958 / STATE explore / EPSILON 0.009987271899997333 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1448.6624755859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3857\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3959 / STATE explore / EPSILON 0.009987268599997332 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1601.988037109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3858\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3960 / STATE explore / EPSILON 0.009987265299997331 / ACTION 2 / REWARD 0.0 / Q_MAX  5294.6816 / Loss  1158.4234619140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3859\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3961 / STATE explore / EPSILON 0.00998726199999733 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  815.4472045898438\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3860\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3962 / STATE explore / EPSILON 0.00998725869999733 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  780.8778076171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3861\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3963 / STATE explore / EPSILON 0.009987255399997329 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  772.2354736328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3862\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3964 / STATE explore / EPSILON 0.009987252099997328 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  831.7437744140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3863\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3965 / STATE explore / EPSILON 0.009987248799997328 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  780.171630859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3864\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3966 / STATE explore / EPSILON 0.009987245499997327 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  800.4520874023438\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3865\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3967 / STATE explore / EPSILON 0.009987242199997326 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  800.41796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3866\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3968 / STATE explore / EPSILON 0.009987238899997326 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  822.5106201171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3867\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3969 / STATE explore / EPSILON 0.009987235599997325 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  809.6586303710938\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3868\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3970 / STATE explore / EPSILON 0.009987232299997324 / ACTION 3 / REWARD 0.0 / Q_MAX  5242.9897 / Loss  860.9749755859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3869\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3971 / STATE explore / EPSILON 0.009987228999997324 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1138.9267578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3870\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3972 / STATE explore / EPSILON 0.009987225699997323 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1135.629638671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3871\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3973 / STATE explore / EPSILON 0.009987222399997322 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1014.9212646484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3872\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3974 / STATE explore / EPSILON 0.009987219099997321 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1158.5819091796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3873\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3975 / STATE explore / EPSILON 0.00998721579999732 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1002.1305541992188\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3874\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3976 / STATE explore / EPSILON 0.00998721249999732 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1069.526123046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3875\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3977 / STATE explore / EPSILON 0.00998720919999732 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1089.72509765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3876\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3978 / STATE explore / EPSILON 0.009987205899997319 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1090.26904296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3877\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3979 / STATE explore / EPSILON 0.009987202599997318 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  1115.880859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3878\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3980 / STATE explore / EPSILON 0.009987199299997317 / ACTION 2 / REWARD 0.0 / Q_MAX  5410.531 / Loss  998.6751708984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3879\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3981 / STATE explore / EPSILON 0.009987195999997317 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  930.8699340820312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3880\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3982 / STATE explore / EPSILON 0.009987192699997316 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  973.7542724609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3881\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3983 / STATE explore / EPSILON 0.009987189399997315 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  991.6123657226562\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3882\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3984 / STATE explore / EPSILON 0.009987186099997315 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  973.4437255859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3883\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3985 / STATE explore / EPSILON 0.009987182799997314 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  1100.35888671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3884\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3986 / STATE explore / EPSILON 0.009987179499997313 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  909.6837768554688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3885\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3987 / STATE explore / EPSILON 0.009987176199997313 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  994.9403686523438\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3886\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3988 / STATE explore / EPSILON 0.009987172899997312 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  952.5681762695312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3887\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3989 / STATE explore / EPSILON 0.009987169599997311 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  1056.9195556640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3888\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 3990 / STATE explore / EPSILON 0.00998716629999731 / ACTION 3 / REWARD 0.0 / Q_MAX  5276.336 / Loss  908.711181640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3889\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3991 / STATE explore / EPSILON 0.00998716299999731 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  1171.6407470703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3890\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3992 / STATE explore / EPSILON 0.009987159699997309 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  1207.50830078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3891\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3993 / STATE explore / EPSILON 0.009987156399997308 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  910.5269775390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3892\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3994 / STATE explore / EPSILON 0.009987153099997308 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  1170.0771484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3893\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3995 / STATE explore / EPSILON 0.009987149799997307 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  907.6798706054688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3894\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3996 / STATE explore / EPSILON 0.009987146499997306 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  1088.783935546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3895\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3997 / STATE explore / EPSILON 0.009987143199997306 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  1098.9605712890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3896\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3998 / STATE explore / EPSILON 0.009987139899997305 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  1224.31787109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3897\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 3999 / STATE explore / EPSILON 0.009987136599997304 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  1125.8009033203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3898\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4000 / STATE explore / EPSILON 0.009987133299997304 / ACTION 2 / REWARD 0.0 / Q_MAX  5172.6934 / Loss  1089.55615234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3899\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4001 / STATE explore / EPSILON 0.009987129999997303 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  906.7379150390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3900\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4002 / STATE explore / EPSILON 0.009987126699997302 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  827.9329833984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3901\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4003 / STATE explore / EPSILON 0.009987123399997301 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  870.2621459960938\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3902\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4004 / STATE explore / EPSILON 0.0099871200999973 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  818.0391845703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3903\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4005 / STATE explore / EPSILON 0.0099871167999973 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  786.5968627929688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3904\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4006 / STATE explore / EPSILON 0.0099871134999973 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  807.3663330078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3905\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4007 / STATE explore / EPSILON 0.009987110199997299 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  817.6832275390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3906\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4008 / STATE explore / EPSILON 0.009987106899997298 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  885.2572021484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3907\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4009 / STATE explore / EPSILON 0.009987103599997297 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  843.390869140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3908\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4010 / STATE explore / EPSILON 0.009987100299997297 / ACTION 3 / REWARD 0.0 / Q_MAX  5312.7515 / Loss  844.3801879882812\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3909\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4011 / STATE explore / EPSILON 0.009987096999997296 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1416.0927734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3910\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4012 / STATE explore / EPSILON 0.009987093699997295 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1149.50537109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3911\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4013 / STATE explore / EPSILON 0.009987090399997295 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1259.62353515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3912\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4014 / STATE explore / EPSILON 0.009987087099997294 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1093.0689697265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3913\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4015 / STATE explore / EPSILON 0.009987083799997293 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1204.0513916015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3914\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4016 / STATE explore / EPSILON 0.009987080499997292 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1198.245849609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3915\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4017 / STATE explore / EPSILON 0.009987077199997292 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1005.5115966796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3916\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4018 / STATE explore / EPSILON 0.009987073899997291 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1254.4794921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3917\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4019 / STATE explore / EPSILON 0.00998707059999729 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1099.0772705078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3918\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4020 / STATE explore / EPSILON 0.00998706729999729 / ACTION 2 / REWARD 0.0 / Q_MAX  5465.328 / Loss  1347.8424072265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3919\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4021 / STATE explore / EPSILON 0.009987063999997289 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  748.4652099609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3920\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4022 / STATE explore / EPSILON 0.009987060699997288 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  730.5657958984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3921\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4023 / STATE explore / EPSILON 0.009987057399997288 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  699.651123046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3922\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4024 / STATE explore / EPSILON 0.009987054099997287 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  754.4152221679688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3923\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4025 / STATE explore / EPSILON 0.009987050799997286 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  752.1224975585938\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3924\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4026 / STATE explore / EPSILON 0.009987047499997286 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  729.5469970703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3925\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4027 / STATE explore / EPSILON 0.009987044199997285 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  727.2950439453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3926\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4028 / STATE explore / EPSILON 0.009987040899997284 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  739.15576171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3927\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4029 / STATE explore / EPSILON 0.009987037599997283 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  762.8505859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3928\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4030 / STATE explore / EPSILON 0.009987034299997283 / ACTION 3 / REWARD 0.0 / Q_MAX  5620.4883 / Loss  732.8558959960938\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3929\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4031 / STATE explore / EPSILON 0.009987030999997282 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1686.257080078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3930\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4032 / STATE explore / EPSILON 0.009987027699997281 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1544.4854736328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3931\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4033 / STATE explore / EPSILON 0.00998702439999728 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1452.47509765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3932\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4034 / STATE explore / EPSILON 0.00998702109999728 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1803.998779296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3933\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4035 / STATE explore / EPSILON 0.00998701779999728 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1517.7698974609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3934\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4036 / STATE explore / EPSILON 0.009987014499997279 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1920.0390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3935\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4037 / STATE explore / EPSILON 0.009987011199997278 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1800.28857421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3936\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4038 / STATE explore / EPSILON 0.009987007899997277 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1510.920654296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3937\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4039 / STATE explore / EPSILON 0.009987004599997277 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1593.65869140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3938\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4040 / STATE explore / EPSILON 0.009987001299997276 / ACTION 2 / REWARD 0.0 / Q_MAX  5515.1284 / Loss  1785.73095703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3939\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4041 / STATE explore / EPSILON 0.009986997999997275 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  692.226806640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3940\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4042 / STATE explore / EPSILON 0.009986994699997274 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  654.2322387695312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3941\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4043 / STATE explore / EPSILON 0.009986991399997274 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  612.2615356445312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3942\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4044 / STATE explore / EPSILON 0.009986988099997273 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  670.8160400390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3943\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4045 / STATE explore / EPSILON 0.009986984799997272 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  634.2081298828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3944\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4046 / STATE explore / EPSILON 0.009986981499997272 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  657.6299438476562\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3945\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4047 / STATE explore / EPSILON 0.009986978199997271 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  611.6832885742188\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3946\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4048 / STATE explore / EPSILON 0.00998697489999727 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  593.5595703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3947\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4049 / STATE explore / EPSILON 0.00998697159999727 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  692.5164794921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3948\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4050 / STATE explore / EPSILON 0.009986968299997269 / ACTION 3 / REWARD 0.0 / Q_MAX  5644.839 / Loss  647.3941650390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3949\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4051 / STATE explore / EPSILON 0.009986964999997268 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1403.9658203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3950\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4052 / STATE explore / EPSILON 0.009986961699997268 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1628.603271484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3951\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4053 / STATE explore / EPSILON 0.009986958399997267 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1789.20361328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3952\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4054 / STATE explore / EPSILON 0.009986955099997266 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1465.797119140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3953\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4055 / STATE explore / EPSILON 0.009986951799997265 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1839.0205078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3954\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4056 / STATE explore / EPSILON 0.009986948499997265 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1959.062744140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3955\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4057 / STATE explore / EPSILON 0.009986945199997264 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1414.975341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3956\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4058 / STATE explore / EPSILON 0.009986941899997263 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1808.404541015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3957\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4059 / STATE explore / EPSILON 0.009986938599997263 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1497.629638671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3958\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4060 / STATE explore / EPSILON 0.009986935299997262 / ACTION 2 / REWARD 0.0 / Q_MAX  5513.8096 / Loss  1830.77099609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3959\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4061 / STATE explore / EPSILON 0.009986931999997261 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  499.782958984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3960\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4062 / STATE explore / EPSILON 0.00998692869999726 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  545.891357421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3961\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4063 / STATE explore / EPSILON 0.00998692539999726 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  579.2529296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3962\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4064 / STATE explore / EPSILON 0.00998692209999726 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  552.283447265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3963\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4065 / STATE explore / EPSILON 0.009986918799997259 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  519.711669921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3964\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4066 / STATE explore / EPSILON 0.009986915499997258 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  569.9063720703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3965\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4067 / STATE explore / EPSILON 0.009986912199997257 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  552.25537109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3966\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4068 / STATE explore / EPSILON 0.009986908899997256 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  537.8555297851562\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3967\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4069 / STATE explore / EPSILON 0.009986905599997256 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  542.38720703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3968\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4070 / STATE explore / EPSILON 0.009986902299997255 / ACTION 3 / REWARD 0.0 / Q_MAX  5650.66 / Loss  596.3924560546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3969\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4071 / STATE explore / EPSILON 0.009986898999997254 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1710.475341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3970\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4072 / STATE explore / EPSILON 0.009986895699997254 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1485.4727783203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3971\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4073 / STATE explore / EPSILON 0.009986892399997253 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1992.7655029296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3972\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4074 / STATE explore / EPSILON 0.009986889099997252 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1592.4462890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3973\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4075 / STATE explore / EPSILON 0.009986885799997252 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1879.51416015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3974\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4076 / STATE explore / EPSILON 0.009986882499997251 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1875.07177734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3975\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4077 / STATE explore / EPSILON 0.00998687919999725 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1653.68212890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3976\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4078 / STATE explore / EPSILON 0.00998687589999725 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1752.939453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3977\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4079 / STATE explore / EPSILON 0.009986872599997249 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  2105.346435546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3978\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4080 / STATE explore / EPSILON 0.009986869299997248 / ACTION 2 / REWARD 0.0 / Q_MAX  5512.302 / Loss  1746.083251953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3979\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4081 / STATE explore / EPSILON 0.009986865999997247 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  546.6800537109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3980\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4082 / STATE explore / EPSILON 0.009986862699997247 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  569.65625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3981\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4083 / STATE explore / EPSILON 0.009986859399997246 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  559.9188232421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3982\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4084 / STATE explore / EPSILON 0.009986856099997245 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  588.2266235351562\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3983\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4085 / STATE explore / EPSILON 0.009986852799997245 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  635.0960693359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3984\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4086 / STATE explore / EPSILON 0.009986849499997244 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  609.695556640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3985\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4087 / STATE explore / EPSILON 0.009986846199997243 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  547.757080078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3986\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4088 / STATE explore / EPSILON 0.009986842899997243 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  543.5017700195312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3987\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4089 / STATE explore / EPSILON 0.009986839599997242 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  612.5675048828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3988\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4090 / STATE explore / EPSILON 0.009986836299997241 / ACTION 3 / REWARD 0.0 / Q_MAX  5645.997 / Loss  586.6539306640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3989\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4091 / STATE explore / EPSILON 0.00998683299999724 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1644.119140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3990\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4092 / STATE explore / EPSILON 0.00998682969999724 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1477.644775390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3991\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4093 / STATE explore / EPSILON 0.00998682639999724 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1653.672119140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3992\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4094 / STATE explore / EPSILON 0.009986823099997238 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1907.4793701171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3993\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4095 / STATE explore / EPSILON 0.009986819799997238 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1920.143798828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3994\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4096 / STATE explore / EPSILON 0.009986816499997237 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1613.01123046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3995\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4097 / STATE explore / EPSILON 0.009986813199997236 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1735.674560546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3996\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4098 / STATE explore / EPSILON 0.009986809899997236 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1530.6925048828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3997\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4099 / STATE explore / EPSILON 0.009986806599997235 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1674.184814453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3998\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4100 / STATE explore / EPSILON 0.009986803299997234 / ACTION 2 / REWARD 0.0 / Q_MAX  5517.581 / Loss  1715.1617431640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3999\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4101 / STATE explore / EPSILON 0.009986799999997234 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  588.02001953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4000\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4102 / STATE explore / EPSILON 0.009986796699997233 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  573.0545654296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4001\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4103 / STATE explore / EPSILON 0.009986793399997232 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  590.5869140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4002\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4104 / STATE explore / EPSILON 0.009986790099997232 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  642.2840576171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4003\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4105 / STATE explore / EPSILON 0.009986786799997231 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  640.9998779296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4004\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4106 / STATE explore / EPSILON 0.00998678349999723 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  601.1052856445312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4005\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4107 / STATE explore / EPSILON 0.00998678019999723 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  599.5445556640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4006\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4108 / STATE explore / EPSILON 0.009986776899997229 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  576.38134765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4007\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4109 / STATE explore / EPSILON 0.009986773599997228 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  601.2650146484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4008\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4110 / STATE explore / EPSILON 0.009986770299997227 / ACTION 3 / REWARD 0.0 / Q_MAX  5646.007 / Loss  654.1259765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4009\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4111 / STATE explore / EPSILON 0.009986766999997227 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1638.4859619140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4010\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4112 / STATE explore / EPSILON 0.009986763699997226 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1324.607421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4011\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4113 / STATE explore / EPSILON 0.009986760399997225 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1510.7144775390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4012\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4114 / STATE explore / EPSILON 0.009986757099997225 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1650.328369140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4013\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4115 / STATE explore / EPSILON 0.009986753799997224 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1719.966796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4014\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4116 / STATE explore / EPSILON 0.009986750499997223 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1543.18408203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4015\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4117 / STATE explore / EPSILON 0.009986747199997223 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1542.792236328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4016\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4118 / STATE explore / EPSILON 0.009986743899997222 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1250.76806640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4017\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4119 / STATE explore / EPSILON 0.009986740599997221 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1853.7608642578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4018\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4120 / STATE explore / EPSILON 0.00998673729999722 / ACTION 2 / REWARD 0.0 / Q_MAX  5507.891 / Loss  1438.166015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4019\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4121 / STATE explore / EPSILON 0.00998673399999722 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  671.0013427734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4020\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4122 / STATE explore / EPSILON 0.009986730699997219 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  684.1563720703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4021\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4123 / STATE explore / EPSILON 0.009986727399997218 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  587.0703735351562\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4022\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4124 / STATE explore / EPSILON 0.009986724099997218 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  672.2905883789062\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4023\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4125 / STATE explore / EPSILON 0.009986720799997217 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  661.2239990234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4024\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4126 / STATE explore / EPSILON 0.009986717499997216 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  679.8388671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4025\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4127 / STATE explore / EPSILON 0.009986714199997216 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  690.2794799804688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4026\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4128 / STATE explore / EPSILON 0.009986710899997215 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  690.2880859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4027\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4129 / STATE explore / EPSILON 0.009986707599997214 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  641.2787475585938\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4028\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4130 / STATE explore / EPSILON 0.009986704299997214 / ACTION 3 / REWARD 0.0 / Q_MAX  5636.456 / Loss  666.035400390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4029\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4131 / STATE explore / EPSILON 0.009986700999997213 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  2153.783203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4030\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4132 / STATE explore / EPSILON 0.009986697699997212 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  2337.54052734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4031\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4133 / STATE explore / EPSILON 0.009986694399997212 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  2025.9295654296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4032\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4134 / STATE explore / EPSILON 0.00998669109999721 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  2407.92333984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4033\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4135 / STATE explore / EPSILON 0.00998668779999721 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  1585.25830078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4034\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4136 / STATE explore / EPSILON 0.00998668449999721 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  1756.7381591796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4035\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4137 / STATE explore / EPSILON 0.009986681199997209 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  1810.5528564453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4036\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4138 / STATE explore / EPSILON 0.009986677899997208 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  1973.7999267578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4037\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4139 / STATE explore / EPSILON 0.009986674599997207 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  2152.09814453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4038\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4140 / STATE explore / EPSILON 0.009986671299997207 / ACTION 2 / REWARD 0.0 / Q_MAX  5748.011 / Loss  2172.632568359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4039\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4141 / STATE explore / EPSILON 0.009986667999997206 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  486.2716064453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4040\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4142 / STATE explore / EPSILON 0.009986664699997205 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  592.9683837890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4041\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4143 / STATE explore / EPSILON 0.009986661399997205 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  508.1336364746094\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4042\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4144 / STATE explore / EPSILON 0.009986658099997204 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  484.7774963378906\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4043\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4145 / STATE explore / EPSILON 0.009986654799997203 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  479.8043212890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4044\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4146 / STATE explore / EPSILON 0.009986651499997203 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  507.1792907714844\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4045\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4147 / STATE explore / EPSILON 0.009986648199997202 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  479.097900390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4046\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4148 / STATE explore / EPSILON 0.009986644899997201 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  486.2716064453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4047\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4149 / STATE explore / EPSILON 0.0099866415999972 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  596.9404296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4048\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4150 / STATE explore / EPSILON 0.0099866382999972 / ACTION 3 / REWARD 0.0 / Q_MAX  5883.4814 / Loss  530.0919799804688\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4049\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4151 / STATE explore / EPSILON 0.009986634999997199 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  1805.135498046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4050\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4152 / STATE explore / EPSILON 0.009986631699997198 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  2015.0316162109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4051\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4153 / STATE explore / EPSILON 0.009986628399997198 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  2530.130859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4052\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4154 / STATE explore / EPSILON 0.009986625099997197 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  2226.77587890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4053\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4155 / STATE explore / EPSILON 0.009986621799997196 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  2140.919921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4054\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4156 / STATE explore / EPSILON 0.009986618499997196 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  1907.393310546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4055\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4157 / STATE explore / EPSILON 0.009986615199997195 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  2194.346435546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4056\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4158 / STATE explore / EPSILON 0.009986611899997194 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  2169.930908203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4057\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4159 / STATE explore / EPSILON 0.009986608599997194 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  2123.275390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4058\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4160 / STATE explore / EPSILON 0.009986605299997193 / ACTION 2 / REWARD 0.0 / Q_MAX  5752.833 / Loss  2093.42529296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4059\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4161 / STATE explore / EPSILON 0.009986601999997192 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  546.0486450195312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4060\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4162 / STATE explore / EPSILON 0.009986598699997191 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  585.163330078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4061\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4163 / STATE explore / EPSILON 0.00998659539999719 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  568.0986328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4062\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4164 / STATE explore / EPSILON 0.00998659209999719 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  503.7498779296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4063\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4165 / STATE explore / EPSILON 0.00998658879999719 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  542.3236083984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4064\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4166 / STATE explore / EPSILON 0.009986585499997189 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  558.7445068359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4065\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4167 / STATE explore / EPSILON 0.009986582199997188 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  503.0440979003906\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4066\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4168 / STATE explore / EPSILON 0.009986578899997187 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  562.4236450195312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4067\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4169 / STATE explore / EPSILON 0.009986575599997187 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  390.8270263671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4068\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4170 / STATE explore / EPSILON 0.009986572299997186 / ACTION 3 / REWARD 0.0 / Q_MAX  5882.076 / Loss  612.0491333007812\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4069\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4171 / STATE explore / EPSILON 0.009986568999997185 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1731.3956298828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4070\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4172 / STATE explore / EPSILON 0.009986565699997185 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1358.964599609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4071\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4173 / STATE explore / EPSILON 0.009986562399997184 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1337.403564453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4072\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4174 / STATE explore / EPSILON 0.009986559099997183 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1597.9349365234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4073\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4175 / STATE explore / EPSILON 0.009986555799997182 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1555.80908203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4074\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4176 / STATE explore / EPSILON 0.009986552499997182 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1269.8780517578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4075\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4177 / STATE explore / EPSILON 0.009986549199997181 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1623.698974609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4076\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4178 / STATE explore / EPSILON 0.00998654589999718 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1554.23681640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4077\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4179 / STATE explore / EPSILON 0.00998654259999718 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1535.090087890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4078\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4180 / STATE explore / EPSILON 0.009986539299997179 / ACTION 2 / REWARD 0.0 / Q_MAX  5724.539 / Loss  1380.559814453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4079\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4181 / STATE explore / EPSILON 0.009986535999997178 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  944.634033203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4080\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4182 / STATE explore / EPSILON 0.009986532699997178 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  950.8264770507812\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4081\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4183 / STATE explore / EPSILON 0.009986529399997177 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  952.287841796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4082\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4184 / STATE explore / EPSILON 0.009986526099997176 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  967.8275146484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4083\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4185 / STATE explore / EPSILON 0.009986522799997176 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  969.851318359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4084\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "----------Random Action----------\n",
            "TIMESTEP 4186 / STATE explore / EPSILON 0.009986519499997175 / ACTION 2 / REWARD 0.0 / Q_MAX  5832.064 / Loss  1004.489990234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4085\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4187 / STATE explore / EPSILON 0.009986516199997174 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  983.97998046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4086\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4188 / STATE explore / EPSILON 0.009986512899997173 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  1014.6961059570312\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4087\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4189 / STATE explore / EPSILON 0.009986509599997173 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  922.1981201171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4088\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4190 / STATE explore / EPSILON 0.009986506299997172 / ACTION 3 / REWARD 0.0 / Q_MAX  5832.064 / Loss  971.1474609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4089\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4191 / STATE explore / EPSILON 0.009986502999997171 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1202.8076171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4090\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4192 / STATE explore / EPSILON 0.00998649969999717 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1440.2532958984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4091\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4193 / STATE explore / EPSILON 0.00998649639999717 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1529.2401123046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4092\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4194 / STATE explore / EPSILON 0.00998649309999717 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1545.8240966796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4093\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4195 / STATE explore / EPSILON 0.009986489799997169 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1308.991455078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4094\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4196 / STATE explore / EPSILON 0.009986486499997168 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1441.0902099609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4095\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4197 / STATE explore / EPSILON 0.009986483199997167 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1615.1051025390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4096\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4198 / STATE explore / EPSILON 0.009986479899997167 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1465.19091796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4097\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4199 / STATE explore / EPSILON 0.009986476599997166 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1509.460205078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4098\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4200 / STATE explore / EPSILON 0.009986473299997165 / ACTION 2 / REWARD 0.0 / Q_MAX  5723.466 / Loss  1502.90380859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4099\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4201 / STATE explore / EPSILON 0.009986469999997164 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  779.243896484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4100\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "----------Random Action----------\n",
            "TIMESTEP 4202 / STATE explore / EPSILON 0.009986466699997164 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  765.6459350585938\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4101\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4203 / STATE explore / EPSILON 0.009986463399997163 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  793.64013671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4102\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4204 / STATE explore / EPSILON 0.009986460099997162 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  786.4420166015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4103\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4205 / STATE explore / EPSILON 0.009986456799997162 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  818.1309814453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4104\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4206 / STATE explore / EPSILON 0.009986453499997161 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  812.7744140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4105\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4207 / STATE explore / EPSILON 0.00998645019999716 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  775.735595703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4106\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4208 / STATE explore / EPSILON 0.00998644689999716 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  786.1762084960938\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4107\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4209 / STATE explore / EPSILON 0.009986443599997159 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  828.83740234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4108\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4210 / STATE explore / EPSILON 0.009986440299997158 / ACTION 3 / REWARD 0.0 / Q_MAX  5870.032 / Loss  832.4364624023438\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4109\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4211 / STATE explore / EPSILON 0.009986436999997158 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1362.123046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4110\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4212 / STATE explore / EPSILON 0.009986433699997157 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1412.211181640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4111\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4213 / STATE explore / EPSILON 0.009986430399997156 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1382.791015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4112\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4214 / STATE explore / EPSILON 0.009986427099997155 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1410.272216796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4113\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4215 / STATE explore / EPSILON 0.009986423799997155 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1493.9302978515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4114\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4216 / STATE explore / EPSILON 0.009986420499997154 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1493.846923828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4115\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4217 / STATE explore / EPSILON 0.009986417199997153 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1124.076416015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4116\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4218 / STATE explore / EPSILON 0.009986413899997153 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1268.8017578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4117\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4219 / STATE explore / EPSILON 0.009986410599997152 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1456.12060546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4118\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4220 / STATE explore / EPSILON 0.009986407299997151 / ACTION 2 / REWARD 0.0 / Q_MAX  5713.8237 / Loss  1302.691162109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4119\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4221 / STATE explore / EPSILON 0.00998640399999715 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1074.1187744140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4120\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4222 / STATE explore / EPSILON 0.00998640069999715 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1095.093994140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4121\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4223 / STATE explore / EPSILON 0.00998639739999715 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1088.795166015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4122\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4224 / STATE explore / EPSILON 0.009986394099997149 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1047.044921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4123\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4225 / STATE explore / EPSILON 0.009986390799997148 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1027.8018798828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4124\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4226 / STATE explore / EPSILON 0.009986387499997147 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1001.607421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4125\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4227 / STATE explore / EPSILON 0.009986384199997147 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1118.527099609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4126\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4228 / STATE explore / EPSILON 0.009986380899997146 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1119.536376953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4127\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4229 / STATE explore / EPSILON 0.009986377599997145 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  1104.655029296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4128\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4230 / STATE explore / EPSILON 0.009986374299997144 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.3906 / Loss  997.541259765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4129\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4231 / STATE explore / EPSILON 0.009986370999997144 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1086.77490234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4130\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4232 / STATE explore / EPSILON 0.009986367699997143 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1251.67822265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4131\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4233 / STATE explore / EPSILON 0.009986364399997142 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1202.69189453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4132\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4234 / STATE explore / EPSILON 0.009986361099997142 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1251.67822265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4133\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4235 / STATE explore / EPSILON 0.009986357799997141 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1099.943359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4134\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4236 / STATE explore / EPSILON 0.00998635449999714 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1137.632568359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4135\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4237 / STATE explore / EPSILON 0.00998635119999714 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1201.1890869140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4136\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4238 / STATE explore / EPSILON 0.009986347899997139 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1252.046630859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4137\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4239 / STATE explore / EPSILON 0.009986344599997138 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1125.95068359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4138\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4240 / STATE explore / EPSILON 0.009986341299997138 / ACTION 2 / REWARD 0.0 / Q_MAX  5680.7266 / Loss  1226.43359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4139\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4241 / STATE explore / EPSILON 0.009986337999997137 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1097.4642333984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4140\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4242 / STATE explore / EPSILON 0.009986334699997136 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  991.3369140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4141\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4243 / STATE explore / EPSILON 0.009986331399997135 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1087.21484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4142\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4244 / STATE explore / EPSILON 0.009986328099997135 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1174.330078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4143\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4245 / STATE explore / EPSILON 0.009986324799997134 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1128.232421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4144\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4246 / STATE explore / EPSILON 0.009986321499997133 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1072.86474609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4145\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4247 / STATE explore / EPSILON 0.009986318199997133 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1143.861083984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4146\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4248 / STATE explore / EPSILON 0.009986314899997132 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1085.93310546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4147\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4249 / STATE explore / EPSILON 0.009986311599997131 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1097.464111328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4148\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4250 / STATE explore / EPSILON 0.00998630829999713 / ACTION 3 / REWARD 0.0 / Q_MAX  5834.625 / Loss  1127.202392578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4149\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4251 / STATE explore / EPSILON 0.00998630499999713 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1315.40234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4150\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4252 / STATE explore / EPSILON 0.00998630169999713 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1200.42236328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4151\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4253 / STATE explore / EPSILON 0.009986298399997129 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1334.46826171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4152\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4254 / STATE explore / EPSILON 0.009986295099997128 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1370.9991455078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4153\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4255 / STATE explore / EPSILON 0.009986291799997127 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1270.88037109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4154\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4256 / STATE explore / EPSILON 0.009986288499997126 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1208.0328369140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4155\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4257 / STATE explore / EPSILON 0.009986285199997126 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1246.504638671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4156\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4258 / STATE explore / EPSILON 0.009986281899997125 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1165.642578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4157\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4259 / STATE explore / EPSILON 0.009986278599997124 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1311.86083984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4158\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4260 / STATE explore / EPSILON 0.009986275299997124 / ACTION 2 / REWARD 0.0 / Q_MAX  6001.409 / Loss  1219.494384765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4159\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4261 / STATE explore / EPSILON 0.009986271999997123 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1212.282470703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4160\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4262 / STATE explore / EPSILON 0.009986268699997122 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1285.898681640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4161\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4263 / STATE explore / EPSILON 0.009986265399997122 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1149.1253662109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4162\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4264 / STATE explore / EPSILON 0.009986262099997121 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1161.72021484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4163\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4265 / STATE explore / EPSILON 0.00998625879999712 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1253.959228515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4164\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4266 / STATE explore / EPSILON 0.00998625549999712 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1285.898681640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4165\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4267 / STATE explore / EPSILON 0.009986252199997119 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1340.46484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4166\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4268 / STATE explore / EPSILON 0.009986248899997118 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1136.736083984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4167\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4269 / STATE explore / EPSILON 0.009986245599997117 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1198.61572265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4168\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4270 / STATE explore / EPSILON 0.009986242299997117 / ACTION 3 / REWARD 0.0 / Q_MAX  6164.5166 / Loss  1235.5113525390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4169\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4271 / STATE explore / EPSILON 0.009986238999997116 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1434.0748291015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4170\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4272 / STATE explore / EPSILON 0.009986235699997115 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1347.1905517578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4171\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4273 / STATE explore / EPSILON 0.009986232399997115 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1493.31689453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4172\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4274 / STATE explore / EPSILON 0.009986229099997114 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1376.5201416015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4173\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4275 / STATE explore / EPSILON 0.009986225799997113 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1412.96044921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4174\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4276 / STATE explore / EPSILON 0.009986222499997113 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1354.64404296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4175\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4277 / STATE explore / EPSILON 0.009986219199997112 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1375.7584228515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4176\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4278 / STATE explore / EPSILON 0.009986215899997111 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1383.17333984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4177\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4279 / STATE explore / EPSILON 0.00998621259999711 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1450.90087890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4178\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4280 / STATE explore / EPSILON 0.00998620929999711 / ACTION 2 / REWARD 0.0 / Q_MAX  6326.4795 / Loss  1335.353759765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4179\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4281 / STATE explore / EPSILON 0.00998620599999711 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1400.0394287109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4180\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4282 / STATE explore / EPSILON 0.009986202699997108 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1370.0908203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4181\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4283 / STATE explore / EPSILON 0.009986199399997108 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1321.94140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4182\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4284 / STATE explore / EPSILON 0.009986196099997107 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1245.3958740234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4183\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4285 / STATE explore / EPSILON 0.009986192799997106 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1347.1107177734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4184\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4286 / STATE explore / EPSILON 0.009986189499997106 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1296.3251953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4185\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4287 / STATE explore / EPSILON 0.009986186199997105 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1347.1107177734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4186\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4288 / STATE explore / EPSILON 0.009986182899997104 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1271.2252197265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4187\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4289 / STATE explore / EPSILON 0.009986179599997104 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1421.831787109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4188\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4290 / STATE explore / EPSILON 0.009986176299997103 / ACTION 3 / REWARD 0.0 / Q_MAX  6215.2666 / Loss  1321.94140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4189\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4291 / STATE explore / EPSILON 0.009986172999997102 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1552.449462890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4190\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4292 / STATE explore / EPSILON 0.009986169699997102 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1339.70556640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4191\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4293 / STATE explore / EPSILON 0.0099861663999971 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1376.6513671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4192\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4294 / STATE explore / EPSILON 0.0099861630999971 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1523.14501953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4193\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "----------Random Action----------\n",
            "TIMESTEP 4295 / STATE explore / EPSILON 0.0099861597999971 / ACTION 0 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1400.561767578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4194\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4296 / STATE explore / EPSILON 0.009986156499997099 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1550.82763671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4195\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4297 / STATE explore / EPSILON 0.009986153199997098 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1519.091552734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4196\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4298 / STATE explore / EPSILON 0.009986149899997097 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1453.71533203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4197\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4299 / STATE explore / EPSILON 0.009986146599997097 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1424.43798828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4198\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4300 / STATE explore / EPSILON 0.009986143299997096 / ACTION 2 / REWARD 0.0 / Q_MAX  6297.951 / Loss  1433.62744140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4199\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4301 / STATE explore / EPSILON 0.009986139999997095 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1198.5380859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4200\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4302 / STATE explore / EPSILON 0.009986136699997095 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1281.13134765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4201\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4303 / STATE explore / EPSILON 0.009986133399997094 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1216.061767578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4202\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4304 / STATE explore / EPSILON 0.009986130099997093 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1235.145751953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4203\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4305 / STATE explore / EPSILON 0.009986126799997093 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1272.7701416015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4204\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4306 / STATE explore / EPSILON 0.009986123499997092 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1324.937744140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4205\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4307 / STATE explore / EPSILON 0.009986120199997091 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1272.595947265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4206\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4308 / STATE explore / EPSILON 0.00998611689999709 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1278.7080078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4207\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4309 / STATE explore / EPSILON 0.00998611359999709 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1265.576904296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4208\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4310 / STATE explore / EPSILON 0.009986110299997089 / ACTION 3 / REWARD 0.0 / Q_MAX  6463.1396 / Loss  1287.3162841796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4209\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4311 / STATE explore / EPSILON 0.009986106999997088 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1547.636474609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4210\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4312 / STATE explore / EPSILON 0.009986103699997088 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1488.7996826171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4211\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4313 / STATE explore / EPSILON 0.009986100399997087 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1748.318359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4212\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4314 / STATE explore / EPSILON 0.009986097099997086 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1566.571533203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4213\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4315 / STATE explore / EPSILON 0.009986093799997086 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1478.717041015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4214\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4316 / STATE explore / EPSILON 0.009986090499997085 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1507.7347412109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4215\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4317 / STATE explore / EPSILON 0.009986087199997084 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1536.75244140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4216\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4318 / STATE explore / EPSILON 0.009986083899997084 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1600.220947265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4217\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4319 / STATE explore / EPSILON 0.009986080599997083 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1547.636474609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4218\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4320 / STATE explore / EPSILON 0.009986077299997082 / ACTION 2 / REWARD 0.0 / Q_MAX  6638.707 / Loss  1489.6395263671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4219\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4321 / STATE explore / EPSILON 0.009986073999997081 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1468.12744140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4220\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4322 / STATE explore / EPSILON 0.00998607069999708 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1642.038330078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4221\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4323 / STATE explore / EPSILON 0.00998606739999708 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1585.982666015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4222\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4324 / STATE explore / EPSILON 0.00998606409999708 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1696.6336669921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4223\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4325 / STATE explore / EPSILON 0.009986060799997079 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1660.138427734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4224\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4326 / STATE explore / EPSILON 0.009986057499997078 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1531.38720703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4225\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4327 / STATE explore / EPSILON 0.009986054199997077 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1689.693359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4226\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4328 / STATE explore / EPSILON 0.009986050899997077 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1606.470458984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4227\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "----------Random Action----------\n",
            "TIMESTEP 4329 / STATE explore / EPSILON 0.009986047599997076 / ACTION 2 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1495.425048828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4228\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4330 / STATE explore / EPSILON 0.009986044299997075 / ACTION 3 / REWARD 0.0 / Q_MAX  6746.4375 / Loss  1427.8868408203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4229\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4331 / STATE explore / EPSILON 0.009986040999997075 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1572.7352294921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4230\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4332 / STATE explore / EPSILON 0.009986037699997074 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1601.9674072265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4231\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4333 / STATE explore / EPSILON 0.009986034399997073 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1486.555908203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4232\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4334 / STATE explore / EPSILON 0.009986031099997073 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1598.736328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4233\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4335 / STATE explore / EPSILON 0.009986027799997072 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1662.16845703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4234\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4336 / STATE explore / EPSILON 0.009986024499997071 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1631.628662109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4235\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4337 / STATE explore / EPSILON 0.00998602119999707 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1573.091064453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4236\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4338 / STATE explore / EPSILON 0.00998601789999707 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1632.61181640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4237\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4339 / STATE explore / EPSILON 0.009986014599997069 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1458.832763671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4238\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4340 / STATE explore / EPSILON 0.009986011299997068 / ACTION 2 / REWARD 0.0 / Q_MAX  6616.5117 / Loss  1572.1015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4239\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4341 / STATE explore / EPSILON 0.009986007999997068 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1354.8328857421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4240\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4342 / STATE explore / EPSILON 0.009986004699997067 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1468.7545166015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4241\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4343 / STATE explore / EPSILON 0.009986001399997066 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1449.189453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4242\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4344 / STATE explore / EPSILON 0.009985998099997066 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1510.305419921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4243\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4345 / STATE explore / EPSILON 0.009985994799997065 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1546.732177734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4244\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4346 / STATE explore / EPSILON 0.009985991499997064 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1469.04931640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4245\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4347 / STATE explore / EPSILON 0.009985988199997064 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1338.03466796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4246\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4348 / STATE explore / EPSILON 0.009985984899997063 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1488.405029296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4247\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4349 / STATE explore / EPSILON 0.009985981599997062 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1439.736572265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4248\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4350 / STATE explore / EPSILON 0.009985978299997061 / ACTION 3 / REWARD 0.0 / Q_MAX  6774.787 / Loss  1468.7545166015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4249\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4351 / STATE explore / EPSILON 0.00998597499999706 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1702.1995849609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4250\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4352 / STATE explore / EPSILON 0.00998597169999706 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1694.033203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4251\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4353 / STATE explore / EPSILON 0.00998596839999706 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1704.873779296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4252\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4354 / STATE explore / EPSILON 0.009985965099997059 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1727.5806884765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4253\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4355 / STATE explore / EPSILON 0.009985961799997058 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1784.759033203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4254\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4356 / STATE explore / EPSILON 0.009985958499997057 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1745.7103271484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4255\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4357 / STATE explore / EPSILON 0.009985955199997057 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1639.361083984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4256\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4358 / STATE explore / EPSILON 0.009985951899997056 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1542.592529296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4257\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4359 / STATE explore / EPSILON 0.009985948599997055 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1757.063720703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4258\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4360 / STATE explore / EPSILON 0.009985945299997055 / ACTION 2 / REWARD 0.0 / Q_MAX  6950.072 / Loss  1535.45654296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4259\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4361 / STATE explore / EPSILON 0.009985941999997054 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1560.2724609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4260\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4362 / STATE explore / EPSILON 0.009985938699997053 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1634.1025390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4261\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4363 / STATE explore / EPSILON 0.009985935399997052 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1676.772216796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4262\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4364 / STATE explore / EPSILON 0.009985932099997052 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1601.7755126953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4263\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4365 / STATE explore / EPSILON 0.009985928799997051 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1642.7293701171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4264\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4366 / STATE explore / EPSILON 0.00998592549999705 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1649.021240234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4265\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4367 / STATE explore / EPSILON 0.00998592219999705 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1636.4373779296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4266\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4368 / STATE explore / EPSILON 0.009985918899997049 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1703.9925537109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4267\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4369 / STATE explore / EPSILON 0.009985915599997048 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1615.24365234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4268\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4370 / STATE explore / EPSILON 0.009985912299997048 / ACTION 3 / REWARD 0.0 / Q_MAX  7097.359 / Loss  1703.992431640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4269\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4371 / STATE explore / EPSILON 0.009985908999997047 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1609.713623046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4270\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4372 / STATE explore / EPSILON 0.009985905699997046 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1597.507568359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4271\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4373 / STATE explore / EPSILON 0.009985902399997046 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1712.372802734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4272\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4374 / STATE explore / EPSILON 0.009985899099997045 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1635.297607421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4273\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4375 / STATE explore / EPSILON 0.009985895799997044 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1514.3853759765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4274\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4376 / STATE explore / EPSILON 0.009985892499997043 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1650.017578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4275\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4377 / STATE explore / EPSILON 0.009985889199997043 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1535.4869384765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4276\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4378 / STATE explore / EPSILON 0.009985885899997042 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1677.4951171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4277\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4379 / STATE explore / EPSILON 0.009985882599997041 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1690.3216552734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4278\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4380 / STATE explore / EPSILON 0.00998587929999704 / ACTION 2 / REWARD 0.0 / Q_MAX  7271.629 / Loss  1629.8656005859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4279\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4381 / STATE explore / EPSILON 0.00998587599999704 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  2049.564453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4280\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4382 / STATE explore / EPSILON 0.00998587269999704 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  2327.56787109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4281\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4383 / STATE explore / EPSILON 0.009985869399997039 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  2208.575927734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4282\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4384 / STATE explore / EPSILON 0.009985866099997038 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  2140.21484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4283\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4385 / STATE explore / EPSILON 0.009985862799997037 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  1943.7850341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4284\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4386 / STATE explore / EPSILON 0.009985859499997037 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  2021.488037109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4285\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4387 / STATE explore / EPSILON 0.009985856199997036 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  1902.2308349609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4286\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4388 / STATE explore / EPSILON 0.009985852899997035 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  1979.3189697265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4287\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4389 / STATE explore / EPSILON 0.009985849599997034 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  2011.375732421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4288\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4390 / STATE explore / EPSILON 0.009985846299997034 / ACTION 3 / REWARD 0.0 / Q_MAX  7432.839 / Loss  2100.55078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4289\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4391 / STATE explore / EPSILON 0.009985842999997033 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1496.7454833984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4290\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4392 / STATE explore / EPSILON 0.009985839699997032 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1459.606689453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4291\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4393 / STATE explore / EPSILON 0.009985836399997032 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1475.787353515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4292\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4394 / STATE explore / EPSILON 0.009985833099997031 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1477.5655517578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4293\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4395 / STATE explore / EPSILON 0.00998582979999703 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1466.89794921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4294\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4396 / STATE explore / EPSILON 0.00998582649999703 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1699.3726806640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4295\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4397 / STATE explore / EPSILON 0.009985823199997029 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1469.4427490234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4296\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4398 / STATE explore / EPSILON 0.009985819899997028 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1454.7225341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4297\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4399 / STATE explore / EPSILON 0.009985816599997028 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1722.87548828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4298\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4400 / STATE explore / EPSILON 0.009985813299997027 / ACTION 2 / REWARD 0.0 / Q_MAX  7618.3057 / Loss  1703.5428466796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4299\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4401 / STATE explore / EPSILON 0.009985809999997026 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2492.515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4300\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4402 / STATE explore / EPSILON 0.009985806699997025 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2324.29052734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4301\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4403 / STATE explore / EPSILON 0.009985803399997025 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2552.13232421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4302\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4404 / STATE explore / EPSILON 0.009985800099997024 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2324.47314453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4303\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4405 / STATE explore / EPSILON 0.009985796799997023 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2265.325927734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4304\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4406 / STATE explore / EPSILON 0.009985793499997023 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2610.510986328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4305\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4407 / STATE explore / EPSILON 0.009985790199997022 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2550.24169921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4306\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4408 / STATE explore / EPSILON 0.009985786899997021 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2267.802734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4307\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4409 / STATE explore / EPSILON 0.00998578359999702 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2266.564208984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4308\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4410 / STATE explore / EPSILON 0.00998578029999702 / ACTION 3 / REWARD 0.0 / Q_MAX  7816.523 / Loss  2210.015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4309\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4411 / STATE explore / EPSILON 0.00998577699999702 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1670.560791015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4310\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4412 / STATE explore / EPSILON 0.009985773699997019 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1599.405517578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4311\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4413 / STATE explore / EPSILON 0.009985770399997018 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1572.095703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4312\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4414 / STATE explore / EPSILON 0.009985767099997017 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1610.284912109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4313\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4415 / STATE explore / EPSILON 0.009985763799997016 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1578.75341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4314\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4416 / STATE explore / EPSILON 0.009985760499997016 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1627.7708740234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4315\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4417 / STATE explore / EPSILON 0.009985757199997015 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1544.1923828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4316\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4418 / STATE explore / EPSILON 0.009985753899997014 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1677.3994140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4317\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4419 / STATE explore / EPSILON 0.009985750599997014 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1600.4609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4318\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4420 / STATE explore / EPSILON 0.009985747299997013 / ACTION 2 / REWARD 0.0 / Q_MAX  7643.357 / Loss  1585.78759765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4319\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4421 / STATE explore / EPSILON 0.009985743999997012 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2469.14208984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4320\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4422 / STATE explore / EPSILON 0.009985740699997012 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2586.40673828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4321\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4423 / STATE explore / EPSILON 0.009985737399997011 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2568.6591796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4322\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4424 / STATE explore / EPSILON 0.00998573409999701 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2290.9150390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4323\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4425 / STATE explore / EPSILON 0.00998573079999701 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2410.383056640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4324\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4426 / STATE explore / EPSILON 0.009985727499997009 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2765.5703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4325\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4427 / STATE explore / EPSILON 0.009985724199997008 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2292.00341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4326\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4428 / STATE explore / EPSILON 0.009985720899997007 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2411.6162109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4327\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4429 / STATE explore / EPSILON 0.009985717599997007 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2763.8232421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4328\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4430 / STATE explore / EPSILON 0.009985714299997006 / ACTION 3 / REWARD 0.0 / Q_MAX  7802.6836 / Loss  2234.89599609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4329\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4431 / STATE explore / EPSILON 0.009985710999997005 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1555.6988525390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4330\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4432 / STATE explore / EPSILON 0.009985707699997005 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1619.6827392578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4331\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4433 / STATE explore / EPSILON 0.009985704399997004 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1538.266845703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4332\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4434 / STATE explore / EPSILON 0.009985701099997003 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1755.871337890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4333\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4435 / STATE explore / EPSILON 0.009985697799997003 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1559.70654296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4334\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4436 / STATE explore / EPSILON 0.009985694499997002 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1585.5377197265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4335\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4437 / STATE explore / EPSILON 0.009985691199997001 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1583.2491455078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4336\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4438 / STATE explore / EPSILON 0.009985687899997 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1567.20361328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4337\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4439 / STATE explore / EPSILON 0.009985684599997 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1582.518310546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4338\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4440 / STATE explore / EPSILON 0.009985681299997 / ACTION 2 / REWARD 0.0 / Q_MAX  7630.2817 / Loss  1552.20947265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4339\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4441 / STATE explore / EPSILON 0.009985677999996998 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2362.4970703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4340\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4442 / STATE explore / EPSILON 0.009985674699996998 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2667.990234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4341\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4443 / STATE explore / EPSILON 0.009985671399996997 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2398.7216796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4342\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4444 / STATE explore / EPSILON 0.009985668099996996 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2417.703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4343\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4445 / STATE explore / EPSILON 0.009985664799996996 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2328.341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4344\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4446 / STATE explore / EPSILON 0.009985661499996995 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2272.329833984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4345\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4447 / STATE explore / EPSILON 0.009985658199996994 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2395.8173828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4346\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4448 / STATE explore / EPSILON 0.009985654899996994 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2441.55810546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4347\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4449 / STATE explore / EPSILON 0.009985651599996993 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2416.8154296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4348\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4450 / STATE explore / EPSILON 0.009985648299996992 / ACTION 3 / REWARD 0.0 / Q_MAX  7529.094 / Loss  2158.82763671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4349\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4451 / STATE explore / EPSILON 0.009985644999996992 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1548.905517578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4350\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4452 / STATE explore / EPSILON 0.009985641699996991 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1677.217041015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4351\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4453 / STATE explore / EPSILON 0.00998563839999699 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1548.905517578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4352\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4454 / STATE explore / EPSILON 0.00998563509999699 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1568.2034912109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4353\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4455 / STATE explore / EPSILON 0.009985631799996989 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1887.512451171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4354\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4456 / STATE explore / EPSILON 0.009985628499996988 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1526.4735107421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4355\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4457 / STATE explore / EPSILON 0.009985625199996987 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1675.9481201171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4356\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4458 / STATE explore / EPSILON 0.009985621899996987 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1697.73583984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4357\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4459 / STATE explore / EPSILON 0.009985618599996986 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1531.00146484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4358\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4460 / STATE explore / EPSILON 0.009985615299996985 / ACTION 2 / REWARD 0.0 / Q_MAX  7561.0254 / Loss  1675.9481201171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4359\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4461 / STATE explore / EPSILON 0.009985611999996985 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2608.07080078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4360\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4462 / STATE explore / EPSILON 0.009985608699996984 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2492.12255859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4361\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4463 / STATE explore / EPSILON 0.009985605399996983 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2557.9150390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4362\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4464 / STATE explore / EPSILON 0.009985602099996983 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2188.423095703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4363\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4465 / STATE explore / EPSILON 0.009985598799996982 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2211.288818359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4364\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4466 / STATE explore / EPSILON 0.009985595499996981 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2490.90380859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4365\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4467 / STATE explore / EPSILON 0.00998559219999698 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2603.279296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4366\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4468 / STATE explore / EPSILON 0.00998558889999698 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2612.1298828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4367\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4469 / STATE explore / EPSILON 0.009985585599996979 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2260.22607421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4368\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4470 / STATE explore / EPSILON 0.009985582299996978 / ACTION 3 / REWARD 0.0 / Q_MAX  7747.5005 / Loss  2501.15966796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4369\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4471 / STATE explore / EPSILON 0.009985578999996978 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1640.6142578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4370\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4472 / STATE explore / EPSILON 0.009985575699996977 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1480.9876708984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4371\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4473 / STATE explore / EPSILON 0.009985572399996976 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1430.1329345703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4372\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4474 / STATE explore / EPSILON 0.009985569099996976 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1458.231201171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4373\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4475 / STATE explore / EPSILON 0.009985565799996975 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1452.32470703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4374\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4476 / STATE explore / EPSILON 0.009985562499996974 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1377.053466796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4375\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4477 / STATE explore / EPSILON 0.009985559199996974 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1393.869140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4376\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4478 / STATE explore / EPSILON 0.009985555899996973 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1623.021484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4377\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4479 / STATE explore / EPSILON 0.009985552599996972 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1589.141357421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4378\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4480 / STATE explore / EPSILON 0.009985549299996972 / ACTION 2 / REWARD 0.0 / Q_MAX  7932.7227 / Loss  1368.571044921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4379\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4481 / STATE explore / EPSILON 0.00998554599999697 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  2843.795166015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4380\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4482 / STATE explore / EPSILON 0.00998554269999697 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  3011.2412109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4381\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4483 / STATE explore / EPSILON 0.00998553939999697 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  2430.62255859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4382\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4484 / STATE explore / EPSILON 0.009985536099996969 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  2636.5341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4383\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4485 / STATE explore / EPSILON 0.009985532799996968 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  2571.08740234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4384\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4486 / STATE explore / EPSILON 0.009985529499996967 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  2869.01953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4385\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4487 / STATE explore / EPSILON 0.009985526199996967 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  3207.67724609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4386\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4488 / STATE explore / EPSILON 0.009985522899996966 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  2923.470947265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4387\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4489 / STATE explore / EPSILON 0.009985519599996965 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  2649.821044921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4388\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4490 / STATE explore / EPSILON 0.009985516299996965 / ACTION 3 / REWARD 0.0 / Q_MAX  8158.5996 / Loss  2578.0361328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4389\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4491 / STATE explore / EPSILON 0.009985512999996964 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  1907.6719970703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4390\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4492 / STATE explore / EPSILON 0.009985509699996963 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  1695.079345703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4391\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4493 / STATE explore / EPSILON 0.009985506399996963 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  1636.340576171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4392\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4494 / STATE explore / EPSILON 0.009985503099996962 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  1680.0836181640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4393\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4495 / STATE explore / EPSILON 0.009985499799996961 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  2165.687255859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4394\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4496 / STATE explore / EPSILON 0.00998549649999696 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  1655.771240234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4395\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4497 / STATE explore / EPSILON 0.00998549319999696 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  1961.97607421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4396\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4498 / STATE explore / EPSILON 0.009985489899996959 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  2232.62158203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4397\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4499 / STATE explore / EPSILON 0.009985486599996958 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  1710.0750732421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4398\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4500 / STATE explore / EPSILON 0.009985483299996958 / ACTION 2 / REWARD 0.0 / Q_MAX  7983.5703 / Loss  1639.3917236328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4399\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4501 / STATE explore / EPSILON 0.009985479999996957 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2622.41796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4400\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4502 / STATE explore / EPSILON 0.009985476699996956 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  3300.90283203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4401\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4503 / STATE explore / EPSILON 0.009985473399996956 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2417.61767578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4402\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4504 / STATE explore / EPSILON 0.009985470099996955 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2553.251220703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4403\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4505 / STATE explore / EPSILON 0.009985466799996954 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2812.7470703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4404\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4506 / STATE explore / EPSILON 0.009985463499996954 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2755.949951171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4405\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4507 / STATE explore / EPSILON 0.009985460199996953 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2916.995361328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4406\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4508 / STATE explore / EPSILON 0.009985456899996952 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2758.0517578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4407\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4509 / STATE explore / EPSILON 0.009985453599996951 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2488.1279296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4408\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4510 / STATE explore / EPSILON 0.00998545029999695 / ACTION 3 / REWARD 0.0 / Q_MAX  8164.368 / Loss  2690.23486328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4409\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4511 / STATE explore / EPSILON 0.00998544699999695 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  1604.998291015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4410\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4512 / STATE explore / EPSILON 0.00998544369999695 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  1582.236572265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4411\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4513 / STATE explore / EPSILON 0.009985440399996949 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  1597.4111328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4412\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4514 / STATE explore / EPSILON 0.009985437099996948 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  1577.5419921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4413\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4515 / STATE explore / EPSILON 0.009985433799996947 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  1559.4749755859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4414\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4516 / STATE explore / EPSILON 0.009985430499996947 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  2259.598388671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4415\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4517 / STATE explore / EPSILON 0.009985427199996946 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  1742.572509765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4416\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4518 / STATE explore / EPSILON 0.009985423899996945 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  1780.5087890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4417\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4519 / STATE explore / EPSILON 0.009985420599996945 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  2414.932861328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4418\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4520 / STATE explore / EPSILON 0.009985417299996944 / ACTION 2 / REWARD 0.0 / Q_MAX  8337.837 / Loss  1827.0361328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4419\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4521 / STATE explore / EPSILON 0.009985413999996943 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  3631.23583984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4420\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4522 / STATE explore / EPSILON 0.009985410699996942 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  3086.333740234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4421\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4523 / STATE explore / EPSILON 0.009985407399996942 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  2760.84033203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4422\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4524 / STATE explore / EPSILON 0.009985404099996941 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  2822.026611328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4423\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4525 / STATE explore / EPSILON 0.00998540079999694 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  3403.07568359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4424\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4526 / STATE explore / EPSILON 0.00998539749999694 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  3024.13232421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4425\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4527 / STATE explore / EPSILON 0.009985394199996939 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  2822.0263671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4426\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4528 / STATE explore / EPSILON 0.009985390899996938 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  2822.026611328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4427\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4529 / STATE explore / EPSILON 0.009985387599996938 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  2393.720703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4428\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4530 / STATE explore / EPSILON 0.009985384299996937 / ACTION 3 / REWARD 0.0 / Q_MAX  8569.197 / Loss  3283.677490234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4429\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4531 / STATE explore / EPSILON 0.009985380999996936 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1802.210205078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4430\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4532 / STATE explore / EPSILON 0.009985377699996936 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1800.0948486328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4431\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4533 / STATE explore / EPSILON 0.009985374399996935 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1806.10791015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4432\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4534 / STATE explore / EPSILON 0.009985371099996934 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1797.6993408203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4433\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4535 / STATE explore / EPSILON 0.009985367799996933 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  2091.39697265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4434\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4536 / STATE explore / EPSILON 0.009985364499996933 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1828.0947265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4435\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4537 / STATE explore / EPSILON 0.009985361199996932 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1799.1341552734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4436\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4538 / STATE explore / EPSILON 0.009985357899996931 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1788.765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4437\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4539 / STATE explore / EPSILON 0.00998535459999693 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1785.0625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4438\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4540 / STATE explore / EPSILON 0.00998535129999693 / ACTION 2 / REWARD 0.0 / Q_MAX  8388.708 / Loss  1800.094970703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4439\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4541 / STATE explore / EPSILON 0.00998534799999693 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2501.98974609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4440\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4542 / STATE explore / EPSILON 0.009985344699996929 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2500.919921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4441\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4543 / STATE explore / EPSILON 0.009985341399996928 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2552.958740234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4442\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4544 / STATE explore / EPSILON 0.009985338099996927 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2935.46826171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4443\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4545 / STATE explore / EPSILON 0.009985334799996927 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2423.83837890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4444\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4546 / STATE explore / EPSILON 0.009985331499996926 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2466.87841796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4445\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4547 / STATE explore / EPSILON 0.009985328199996925 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  3149.22265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4446\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4548 / STATE explore / EPSILON 0.009985324899996924 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2640.08154296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4447\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4549 / STATE explore / EPSILON 0.009985321599996924 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2423.3740234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4448\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4550 / STATE explore / EPSILON 0.009985318299996923 / ACTION 3 / REWARD 0.0 / Q_MAX  8618.438 / Loss  2639.0390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4449\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4551 / STATE explore / EPSILON 0.009985314999996922 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  1914.8538818359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4450\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4552 / STATE explore / EPSILON 0.009985311699996922 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  2059.018310546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4451\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4553 / STATE explore / EPSILON 0.009985308399996921 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  1905.758544921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4452\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4554 / STATE explore / EPSILON 0.00998530509999692 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  2059.0185546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4453\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4555 / STATE explore / EPSILON 0.00998530179999692 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  1911.1719970703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4454\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4556 / STATE explore / EPSILON 0.009985298499996919 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  1913.2318115234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4455\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4557 / STATE explore / EPSILON 0.009985295199996918 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  1907.6268310546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4456\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4558 / STATE explore / EPSILON 0.009985291899996918 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  1910.18310546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4457\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4559 / STATE explore / EPSILON 0.009985288599996917 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  1911.7255859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4458\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4560 / STATE explore / EPSILON 0.009985285299996916 / ACTION 2 / REWARD 0.0 / Q_MAX  8828.453 / Loss  2065.3115234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4459\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4561 / STATE explore / EPSILON 0.009985281999996916 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  2980.26611328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4460\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4562 / STATE explore / EPSILON 0.009985278699996915 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  3339.736328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4461\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4563 / STATE explore / EPSILON 0.009985275399996914 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  3330.12451171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4462\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4564 / STATE explore / EPSILON 0.009985272099996913 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  2552.9052734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4463\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4565 / STATE explore / EPSILON 0.009985268799996913 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  2882.84814453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4464\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4566 / STATE explore / EPSILON 0.009985265499996912 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  2834.80029296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4465\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4567 / STATE explore / EPSILON 0.009985262199996911 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  2496.65283203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4466\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4568 / STATE explore / EPSILON 0.00998525889999691 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  3254.100830078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4467\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4569 / STATE explore / EPSILON 0.00998525559999691 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  3644.24755859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4468\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4570 / STATE explore / EPSILON 0.00998525229999691 / ACTION 3 / REWARD 0.0 / Q_MAX  8639.769 / Loss  3384.759521484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4469\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4571 / STATE explore / EPSILON 0.009985248999996909 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1837.822265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4470\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4572 / STATE explore / EPSILON 0.009985245699996908 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  2384.20458984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4471\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4573 / STATE explore / EPSILON 0.009985242399996907 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1888.39501953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4472\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4574 / STATE explore / EPSILON 0.009985239099996907 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1868.02978515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4473\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4575 / STATE explore / EPSILON 0.009985235799996906 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1855.904052734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4474\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4576 / STATE explore / EPSILON 0.009985232499996905 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1852.3492431640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4475\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4577 / STATE explore / EPSILON 0.009985229199996904 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1918.8240966796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4476\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4578 / STATE explore / EPSILON 0.009985225899996904 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1832.4622802734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4477\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4579 / STATE explore / EPSILON 0.009985222599996903 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1862.043212890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4478\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4580 / STATE explore / EPSILON 0.009985219299996902 / ACTION 2 / REWARD 0.0 / Q_MAX  8830.644 / Loss  1839.296142578125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4479\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4581 / STATE explore / EPSILON 0.009985215999996902 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  2705.62353515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4480\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4582 / STATE explore / EPSILON 0.009985212699996901 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  4595.490234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4481\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "----------Random Action----------\n",
            "TIMESTEP 4583 / STATE explore / EPSILON 0.0099852093999969 / ACTION 0 / REWARD 0.0 / Q_MAX  9067.901 / Loss  3055.7509765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4482\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4584 / STATE explore / EPSILON 0.0099852060999969 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  2997.396484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4483\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4585 / STATE explore / EPSILON 0.009985202799996899 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  3621.37939453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4484\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4586 / STATE explore / EPSILON 0.009985199499996898 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  2588.91455078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4485\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4587 / STATE explore / EPSILON 0.009985196199996898 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  3230.814697265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4486\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4588 / STATE explore / EPSILON 0.009985192899996897 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  3854.7978515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4487\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4589 / STATE explore / EPSILON 0.009985189599996896 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  3795.713134765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4488\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4590 / STATE explore / EPSILON 0.009985186299996895 / ACTION 3 / REWARD 0.0 / Q_MAX  9067.901 / Loss  3229.15087890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4489\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4591 / STATE explore / EPSILON 0.009985182999996895 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  1664.48876953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4490\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4592 / STATE explore / EPSILON 0.009985179699996894 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  1712.29345703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4491\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4593 / STATE explore / EPSILON 0.009985176399996893 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  2886.0322265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4492\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4594 / STATE explore / EPSILON 0.009985173099996893 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  2101.53857421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4493\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4595 / STATE explore / EPSILON 0.009985169799996892 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  3884.821044921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4494\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4596 / STATE explore / EPSILON 0.009985166499996891 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  1905.223876953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4495\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4597 / STATE explore / EPSILON 0.00998516319999689 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  1713.9853515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4496\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4598 / STATE explore / EPSILON 0.00998515989999689 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  1687.5450439453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4497\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4599 / STATE explore / EPSILON 0.00998515659999689 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  3817.32421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4498\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4600 / STATE explore / EPSILON 0.009985153299996889 / ACTION 2 / REWARD 0.0 / Q_MAX  9287.26 / Loss  1802.5758056640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4499\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4601 / STATE explore / EPSILON 0.009985149999996888 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  3841.28173828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4500\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4602 / STATE explore / EPSILON 0.009985146699996887 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  3065.572265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4501\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4603 / STATE explore / EPSILON 0.009985143399996886 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  3065.572265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4502\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4604 / STATE explore / EPSILON 0.009985140099996886 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  3350.126708984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4503\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4605 / STATE explore / EPSILON 0.009985136799996885 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  3063.906982421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4504\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4606 / STATE explore / EPSILON 0.009985133499996884 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  3196.26171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4505\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4607 / STATE explore / EPSILON 0.009985130199996884 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  3567.44775390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4506\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4608 / STATE explore / EPSILON 0.009985126899996883 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  4245.3662109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4507\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4609 / STATE explore / EPSILON 0.009985123599996882 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  3939.956298828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4508\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4610 / STATE explore / EPSILON 0.009985120299996882 / ACTION 3 / REWARD 0.0 / Q_MAX  9075.921 / Loss  2533.58154296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4509\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4611 / STATE explore / EPSILON 0.009985116999996881 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  2180.962158203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4510\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4612 / STATE explore / EPSILON 0.00998511369999688 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  2158.51416015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4511\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4613 / STATE explore / EPSILON 0.00998511039999688 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  1676.69140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4512\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4614 / STATE explore / EPSILON 0.009985107099996879 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  2260.169677734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4513\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4615 / STATE explore / EPSILON 0.009985103799996878 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  1725.6099853515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4514\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4616 / STATE explore / EPSILON 0.009985100499996877 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  1618.763427734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4515\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4617 / STATE explore / EPSILON 0.009985097199996877 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  1624.26904296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4516\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4618 / STATE explore / EPSILON 0.009985093899996876 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  1692.855224609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4517\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4619 / STATE explore / EPSILON 0.009985090599996875 / ACTION 2 / REWARD 0.0 / Q_MAX  8820.152 / Loss  2270.054931640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4518\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "TIMESTEP 4620 / STATE explore / EPSILON 0.009985087299996875 / ACTION 0 / REWARD 0.0 / Q_MAX  8820.152 / Loss  2107.68603515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4519\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4621 / STATE explore / EPSILON 0.009985083999996874 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  3900.083984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4520\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4622 / STATE explore / EPSILON 0.009985080699996873 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  3454.493896484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4521\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4623 / STATE explore / EPSILON 0.009985077399996873 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  3148.859619140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4522\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4624 / STATE explore / EPSILON 0.009985074099996872 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  3233.00146484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4523\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4625 / STATE explore / EPSILON 0.009985070799996871 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  3309.603271484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4524\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4626 / STATE explore / EPSILON 0.00998506749999687 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  3748.51904296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4525\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4627 / STATE explore / EPSILON 0.00998506419999687 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  3375.04931640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4526\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4628 / STATE explore / EPSILON 0.00998506089999687 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  2723.06494140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4527\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4629 / STATE explore / EPSILON 0.009985057599996868 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  3016.496826171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4528\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4630 / STATE explore / EPSILON 0.009985054299996868 / ACTION 3 / REWARD 0.0 / Q_MAX  9068.408 / Loss  2896.353515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4529\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4631 / STATE explore / EPSILON 0.009985050999996867 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1889.3861083984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4530\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4632 / STATE explore / EPSILON 0.009985047699996866 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1845.66943359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4531\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4633 / STATE explore / EPSILON 0.009985044399996866 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1867.888916015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4532\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4634 / STATE explore / EPSILON 0.009985041099996865 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1845.841552734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4533\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4635 / STATE explore / EPSILON 0.009985037799996864 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1967.539794921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4534\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4636 / STATE explore / EPSILON 0.009985034499996864 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1844.71630859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4535\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4637 / STATE explore / EPSILON 0.009985031199996863 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1838.109130859375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4536\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4638 / STATE explore / EPSILON 0.009985027899996862 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1890.139404296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4537\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4639 / STATE explore / EPSILON 0.009985024599996862 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1879.0047607421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4538\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4640 / STATE explore / EPSILON 0.00998502129999686 / ACTION 2 / REWARD 0.0 / Q_MAX  8850.96 / Loss  1869.8834228515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4539\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4641 / STATE explore / EPSILON 0.00998501799999686 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  2673.114013671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4540\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4642 / STATE explore / EPSILON 0.00998501469999686 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  2776.59033203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4541\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4643 / STATE explore / EPSILON 0.009985011399996859 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  2347.91650390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4542\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4644 / STATE explore / EPSILON 0.009985008099996858 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  3769.894775390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4543\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4645 / STATE explore / EPSILON 0.009985004799996857 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  3132.02001953125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4544\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4646 / STATE explore / EPSILON 0.009985001499996857 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  3080.60888671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4545\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4647 / STATE explore / EPSILON 0.009984998199996856 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  2917.85009765625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4546\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4648 / STATE explore / EPSILON 0.009984994899996855 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  3232.11962890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4547\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4649 / STATE explore / EPSILON 0.009984991599996855 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  2492.22412109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4548\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4650 / STATE explore / EPSILON 0.009984988299996854 / ACTION 3 / REWARD 0.0 / Q_MAX  8664.448 / Loss  2778.06396484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4549\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4651 / STATE explore / EPSILON 0.009984984999996853 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  1905.290283203125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4550\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4652 / STATE explore / EPSILON 0.009984981699996853 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  1910.31982421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4551\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4653 / STATE explore / EPSILON 0.009984978399996852 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  2310.801513671875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4552\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4654 / STATE explore / EPSILON 0.009984975099996851 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  1915.03466796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4553\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4655 / STATE explore / EPSILON 0.00998497179999685 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  1918.717041015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4554\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4656 / STATE explore / EPSILON 0.00998496849999685 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  1912.1826171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4555\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4657 / STATE explore / EPSILON 0.009984965199996849 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  1914.4444580078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4556\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4658 / STATE explore / EPSILON 0.009984961899996848 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  2316.695068359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4557\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4659 / STATE explore / EPSILON 0.009984958599996848 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  2315.616455078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4558\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4660 / STATE explore / EPSILON 0.009984955299996847 / ACTION 2 / REWARD 0.0 / Q_MAX  8839.3955 / Loss  2308.26416015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4559\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4661 / STATE explore / EPSILON 0.009984951999996846 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2781.191650390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4560\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4662 / STATE explore / EPSILON 0.009984948699996846 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2690.484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4561\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4663 / STATE explore / EPSILON 0.009984945399996845 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2867.9482421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4562\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4664 / STATE explore / EPSILON 0.009984942099996844 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2852.867431640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4563\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4665 / STATE explore / EPSILON 0.009984938799996844 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2632.7080078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4564\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4666 / STATE explore / EPSILON 0.009984935499996843 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2734.850341796875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4565\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4667 / STATE explore / EPSILON 0.009984932199996842 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2612.66162109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4566\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4668 / STATE explore / EPSILON 0.009984928899996841 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  3279.536865234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4567\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4669 / STATE explore / EPSILON 0.00998492559999684 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2601.75244140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4568\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4670 / STATE explore / EPSILON 0.00998492229999684 / ACTION 3 / REWARD 0.0 / Q_MAX  9080.23 / Loss  2982.3857421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4569\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4671 / STATE explore / EPSILON 0.00998491899999684 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  1926.4212646484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4570\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4672 / STATE explore / EPSILON 0.009984915699996839 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  1919.8916015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4571\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4673 / STATE explore / EPSILON 0.009984912399996838 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  1926.746826171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4572\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4674 / STATE explore / EPSILON 0.009984909099996837 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  1932.2237548828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4573\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4675 / STATE explore / EPSILON 0.009984905799996837 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  1935.02587890625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4574\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4676 / STATE explore / EPSILON 0.009984902499996836 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  2113.217529296875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4575\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4677 / STATE explore / EPSILON 0.009984899199996835 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  1927.39794921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4576\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4678 / STATE explore / EPSILON 0.009984895899996835 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  1938.097412109375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4577\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4679 / STATE explore / EPSILON 0.009984892599996834 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  2112.89208984375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4578\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4680 / STATE explore / EPSILON 0.009984889299996833 / ACTION 2 / REWARD 0.0 / Q_MAX  8840.443 / Loss  2300.299560546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4579\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4681 / STATE explore / EPSILON 0.009984885999996833 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  3227.887939453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4580\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4682 / STATE explore / EPSILON 0.009984882699996832 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  2900.87939453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4581\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4683 / STATE explore / EPSILON 0.009984879399996831 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  3126.16845703125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4582\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4684 / STATE explore / EPSILON 0.00998487609999683 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  2966.68798828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4583\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4685 / STATE explore / EPSILON 0.00998487279999683 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  3162.486328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4584\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4686 / STATE explore / EPSILON 0.009984869499996829 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  3016.9794921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4585\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4687 / STATE explore / EPSILON 0.009984866199996828 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  3237.4150390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4586\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4688 / STATE explore / EPSILON 0.009984862899996828 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  2899.2275390625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4587\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4689 / STATE explore / EPSILON 0.009984859599996827 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  3096.3515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4588\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4690 / STATE explore / EPSILON 0.009984856299996826 / ACTION 3 / REWARD 0.0 / Q_MAX  9039.289 / Loss  3097.08447265625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4589\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4691 / STATE explore / EPSILON 0.009984852999996826 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2155.6123046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4590\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4692 / STATE explore / EPSILON 0.009984849699996825 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  1983.4979248046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4591\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4693 / STATE explore / EPSILON 0.009984846399996824 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2150.7744140625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4592\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4694 / STATE explore / EPSILON 0.009984843099996824 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2150.9736328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4593\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4695 / STATE explore / EPSILON 0.009984839799996823 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2039.6463623046875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4594\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4696 / STATE explore / EPSILON 0.009984836499996822 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2133.26171875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4595\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4697 / STATE explore / EPSILON 0.009984833199996821 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2009.943115234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4596\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4698 / STATE explore / EPSILON 0.00998482989999682 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2218.62939453125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4597\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4699 / STATE explore / EPSILON 0.00998482659999682 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2004.052734375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4598\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4700 / STATE explore / EPSILON 0.00998482329999682 / ACTION 2 / REWARD 0.0 / Q_MAX  9231.232 / Loss  2091.0205078125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4599\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4701 / STATE explore / EPSILON 0.009984819999996819 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3357.130615234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4600\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4702 / STATE explore / EPSILON 0.009984816699996818 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3675.6318359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4601\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4703 / STATE explore / EPSILON 0.009984813399996817 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3369.401611328125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4602\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "----------Random Action----------\n",
            "TIMESTEP 4704 / STATE explore / EPSILON 0.009984810099996817 / ACTION 0 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3142.933349609375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4603\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4705 / STATE explore / EPSILON 0.009984806799996816 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3455.62353515625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4604\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4706 / STATE explore / EPSILON 0.009984803499996815 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  4024.289306640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4605\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4707 / STATE explore / EPSILON 0.009984800199996815 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3282.25048828125\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4606\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4708 / STATE explore / EPSILON 0.009984796899996814 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3189.8193359375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4607\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4709 / STATE explore / EPSILON 0.009984793599996813 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3201.27294921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4608\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "Now we save model\n",
            "TIMESTEP 4710 / STATE explore / EPSILON 0.009984790299996812 / ACTION 3 / REWARD 0.0 / Q_MAX  9127.391 / Loss  3968.884521484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4609\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4711 / STATE explore / EPSILON 0.009984786999996812 / ACTION 2 / REWARD 0.0 / Q_MAX  9304.804 / Loss  1700.8411865234375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4610\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4712 / STATE explore / EPSILON 0.009984783699996811 / ACTION 2 / REWARD 0.0 / Q_MAX  9304.804 / Loss  1467.6165771484375\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4611\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4713 / STATE explore / EPSILON 0.00998478039999681 / ACTION 2 / REWARD 0.0 / Q_MAX  9304.804 / Loss  1495.3258056640625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4612\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4714 / STATE explore / EPSILON 0.00998477709999681 / ACTION 2 / REWARD 0.0 / Q_MAX  9304.804 / Loss  1757.18310546875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4613\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4715 / STATE explore / EPSILON 0.009984773799996809 / ACTION 2 / REWARD 0.0 / Q_MAX  9304.804 / Loss  1704.141357421875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4614\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4716 / STATE explore / EPSILON 0.009984770499996808 / ACTION 2 / REWARD 0.0 / Q_MAX  9304.804 / Loss  1652.498291015625\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4615\n",
            "Now we load weight\n",
            "Weight load successfully\n",
            "TIMESTEP 4717 / STATE explore / EPSILON 0.009984767199996808 / ACTION 2 / REWARD 0.0 / Q_MAX  9304.804 / Loss  1616.0633544921875\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4616\n",
            "Now we load weight\n",
            "Weight load successfully\n"
=======
            "EPISODE 0\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 1\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 240 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 241 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 242 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 243 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 244 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 245 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 246 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 247 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 248 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 249 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 250 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 251 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 252 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 253 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 254 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 255 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 256 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 257 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 258 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 259 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 260 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 261 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 262 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 263 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 264 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 265 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 266 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 267 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 268 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 269 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 270 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 271 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 272 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 273 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 274 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 275 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 276 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 277 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 278 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 279 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 280 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 281 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 282 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 283 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 284 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 285 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 286 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 287 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 288 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 289 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 290 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 291 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 292 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 293 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 294 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 295 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 296 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 297 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 298 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 299 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 300 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 301 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 302 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 303 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 304 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 305 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 306 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 307 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 308 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 309 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 310 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 311 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 312 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 313 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 314 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 315 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 316 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 317 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 318 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 319 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 320 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 321 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 322 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 323 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 324 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 325 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 326 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 327 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 328 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 329 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 330 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 331 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 332 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 333 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 334 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 335 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 336 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 337 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 338 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 339 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 340 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 341 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 342 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 343 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 344 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 345 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 346 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 347 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 348 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 349 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 350 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 351 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 352 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 353 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 354 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 355 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 356 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 357 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 358 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 359 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 360 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 361 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 362 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 363 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 364 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 365 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 366 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 367 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 368 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 369 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 370 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 371 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 372 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 373 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 374 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 375 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 376 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 377 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 378 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 379 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 380 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 381 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 382 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 383 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 384 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 385 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 2\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 3\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 4\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 240 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 241 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 242 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 243 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 244 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 245 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 246 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 247 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 248 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 249 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 250 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 251 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 252 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 253 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 254 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 255 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 256 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 257 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 258 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 259 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 260 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 261 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 5\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 6\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Episode finished!\n",
            "************************\n",
            "EPISODE 7\n",
            "Now we build the model\n",
            "We finish building the model\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
            "LOSS 0\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 240 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 241 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 242 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 243 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 244 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 245 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 246 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 247 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 248 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 249 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 250 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 251 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 252 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 253 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 254 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 255 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 256 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 257 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 258 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 259 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 260 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 261 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 262 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 263 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 264 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 265 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 266 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 267 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 268 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 269 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 270 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 271 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 272 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 273 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 274 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 275 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 276 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 277 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 278 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "LOSS 0\n",
            "TIMESTEP 279 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "Now we save model\n",
            "LOSS 0\n",
            "TIMESTEP 280 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 281 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 282 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
            "LOSS 0\n",
            "TIMESTEP 283 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
>>>>>>> 7cb2c6c0799b23abab23aea6d1eb3758d2c69d4e
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}