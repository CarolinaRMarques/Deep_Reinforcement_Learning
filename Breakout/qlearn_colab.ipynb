{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qlearn_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdYve4Htb0_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8089456e-1041-4bc1-a1e5-185d36f6c2fe"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!pip install atari_py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari_py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari_py) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH6yAoJDzo4Z",
        "outputId": "5b539574-542f-4afa-fb5f-d9fec0501347"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1hqnsP3c8uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2715fc68-45f9-4d6e-eb44-7e2ed6a55b1b"
      },
      "source": [
        "!python -m atari_py.import_roms '/content/drive/My Drive/CN_Breakout/ROMS'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pXbGMPQxbxk7",
        "outputId": "89ac333d-d7bd-4683-bdb6-4d1249baf5b0"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\n",
        "#https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import skimage as skimage\n",
        "from skimage import transform, color, exposure\n",
        "from skimage.transform import rotate\n",
        "from skimage.viewer import ImageViewer\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import json\n",
        "from tensorflow.keras.initializers import identity\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD , Adam\n",
        "import tensorflow as tf\n",
        "#import agent\n",
        "# Import the gym module\n",
        "import gym\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\n",
        "GAME = 'atari' # the name of the game being played for log files\n",
        "CONFIG = 'nothreshold'\n",
        "ACTIONS = 3 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVATION = 200. # timesteps to observe before training. de cada 3200 frames, vamos ao nosso buffer e selecionamos de forma aleatoria um batch size. Neste caso, 32 frames. em numpy arrays\n",
        "EXPLORE = 3000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.01 # starting value of epsilon EPSILON é para ver o exploration vs exploitation\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "#MAX_STEPS_PER_EPISODE = 1000\n",
        "EPISODES = 10000\n",
        "q_max_list = []\n",
        "loss_list = []\n",
        "reward_list = []\n",
        "\n",
        "img_rows, img_cols = 84, 84\n",
        "#Convert image into Black and white\n",
        "img_channels = 4 #We stack 4 frames\n",
        "\n",
        "def buildmodel():\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = tf.keras.layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = Conv2D(32, 8, strides=4, activation=\"relu\", padding = 'same')(inputs)\n",
        "    layer2 = Conv2D(64, 4, strides=2, activation=\"relu\", padding = 'same')(layer1)\n",
        "    layer3 = Conv2D(64, 3, strides=1, activation=\"relu\", padding = 'same')(layer2)\n",
        "\n",
        "    layer4 = Flatten()(layer3)\n",
        "\n",
        "    layer5 = Dense(512, activation=\"relu\")(layer4)\n",
        "    action = Dense(ACTIONS, activation=\"linear\")(layer5)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "def buildmodel2():\n",
        "    print(\"Now we build the model\")\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters = 32, kernel_size = (8, 8), strides = (4, 4), padding = 'same', input_shape = (img_rows, img_cols, img_channels)))  #80*80*4\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (4, 4), strides = (2, 2), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(ACTIONS))\n",
        "\n",
        "    adam = Adam(learning_rate = LEARNING_RATE)\n",
        "    model.compile(loss='mse', optimizer = adam)\n",
        "    print(\"We finish building the model\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n",
        "def trainNetwork(model,args):\n",
        "    # open up a game state to communicate with emulator\n",
        "    env = wrap_env(gym.make('BreakoutDeterministic-v4'))\n",
        "    env.reset()\n",
        "    # store the previous observations in replay memory\n",
        "    \n",
        "    #----------------------------------------\n",
        "    #PARA OBTER O SIGNIFICADO DAS AÇÕES POSSíVEIS\n",
        "    #print(env.unwrapped.get_action_meanings())\n",
        "    #----------------------------------------\n",
        "    \n",
        "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        \n",
        "    x_t, r_0, terminal, info = env.step(1) #COMEÇAR O JOGO COM A AÇÃO \"FIRE\"\n",
        "    \n",
        "    env.render()\n",
        "\n",
        "    D = args['D']\n",
        "\n",
        "    x_t = skimage.color.rgb2gray(x_t)\n",
        "    x_t = skimage.transform.resize(x_t, (84,84))\n",
        "    x_t = skimage.exposure.rescale_intensity(x_t, out_range = (0,255))\n",
        "\n",
        "    x_t = x_t / 255.0\n",
        "\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2) #colocar a sequência de frames. 4 frames sequenciais, que vamos aplicar à nossa lista. Para conseguir a estabilidade de imagens sequenciais\n",
        "    \n",
        "    #print (s_t.shape)\n",
        "\n",
        "    #In Keras, need to reshape\n",
        "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
        "\n",
        "    t = args['t']\n",
        "\n",
        "    if args['mode'] == 'Run':\n",
        "        OBSERVE = 999999999\t#We keep observe, never train\n",
        "        epsilon = FINAL_EPSILON # higher epsilon, more timestamps?\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/My Drive/CN_Breakout/models/model_v1.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "\n",
        "    elif args['mode'] == 'CTrain': #Continue previous train\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = 0.07823368810419994 #0.08811709480229288\n",
        "        epsilon = args['epsilon']\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"/content/drive/My Drive/CN_Breakout/models/model.h5\")\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "        print (\"Weight load successfully\")\n",
        "        \n",
        "\n",
        "    else:\t\t\t\t\t   #We go to training mode -> -m \"Train\"\n",
        "        OBSERVE = OBSERVATION\n",
        "        #epsilon = INITIAL_EPSILON #o EPSILON é o que divide a parte de exploration vs exploitation. se for abaixo de um dado valor é exploration. Caso contrário é exploitation\n",
        "        epsilon = args['epsilon']\n",
        "        adam = Adam(learning_rate = LEARNING_RATE)\n",
        "        model.compile(loss = 'mse', optimizer = adam)\n",
        "\n",
        "    lives = 5\n",
        "    r_total = 0\n",
        "    while (lives > 0):\n",
        "    #for i in range(MAX_STEPS_PER_EPISODE):\n",
        "        loss = 0\n",
        "        Q_sa = 0 # Q(s, a) representing the maximum discounted future reward when we perform action a in state s.\n",
        "        action_index = 0\n",
        "        r_t = 0 #reward\n",
        "        a_t = np.zeros([ACTIONS]) #action\n",
        "      \n",
        "        #choose an action epsilon greedy\n",
        "        if t % FRAME_PER_ACTION == 0:\n",
        "            if random.random() <= epsilon:\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
        "                a_t[action_index] = 1\n",
        "                \n",
        "            else:\n",
        "                q = model.predict(s_t)\t   #input a stack of 4 images, get the prediction\n",
        "                max_Q = np.argmax(q)\n",
        "               # print(max_Q, q, a_t)\n",
        "                action_index = max_Q\n",
        "                a_t[max_Q] = 1\n",
        "\n",
        "        #We reduce the epsilon gradually\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        #run the selected action and observed next state and reward. DEPOIS DE UM \"STEP\" correr sempre o \"RENDER\"\n",
        "        x_t1_colored, r_t, terminal, info = env.step(list(a_t).index(1) + 1) #FUNÇÂO \"WHERE\" para obter o índice do valor do array que está a 1\n",
        "        print(\"INFO\", info)\n",
        "        print(\"Terminal\", terminal)\n",
        "        if info['ale.lives'] < lives:\n",
        "          lives -= 1\n",
        "          r_t = -1.0\n",
        "        env.render()\n",
        "        \n",
        "        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n",
        "        x_t1 = skimage.transform.resize(x_t1, (84, 84))\n",
        "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range = (0, 255))\n",
        "        \n",
        "        x_t1 = x_t1 / 255.0\n",
        "\n",
        "\n",
        "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis = 3)\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        #only train if done observing\n",
        "        if t > OBSERVE: #train ou update da nossa rede. de quantas em quantas frames vamos precisar para fazer um treino. se replay_mem começar a ficar mto cheio retira a última entrada. e fazemos append das novas decisoes que foram sendo adquiridas\n",
        "            #sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "\n",
        "            #Now we do the experience replay\n",
        "            state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "            state_t = np.concatenate(state_t)\n",
        "            state_t1 = np.concatenate(state_t1)\n",
        "            targets = model.predict(state_t)\n",
        "            Q_sa = model.predict(state_t1)\n",
        "            #r_total += reward_t\n",
        "            targets[range(BATCH), action_t] = reward_t + GAMMA * np.max(Q_sa, axis = 1) * np.invert(terminal) #qual o target associado\n",
        "            \n",
        "            \n",
        "            loss += model.train_on_batch(state_t, targets) #quanto mais proximo de zero, mais proximo está de convergir para conseguir estimar o key value de acordo com o par (estado, ação)\n",
        "            \n",
        "        s_t = s_t1\n",
        "        t = t + 1\n",
        "\n",
        "        # save progress every 1000 iterations\n",
        "        if t % 100 == 0:\n",
        "            print(\"Now we save model\")\n",
        "            model.save_weights(\"/content/drive/My Drive/CN_Breakout/models/model.h5\", overwrite = True)\n",
        "            with open(\"model.json\", \"w\") as outfile:\n",
        "                json.dump(model.to_json(), outfile)\n",
        "\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_total, \\\n",
        "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
        "        \n",
        "        q_max_list.append(np.max(Q_sa))\n",
        "        loss_list.append(loss)\n",
        "        reward_list.append(r_t)\n",
        "\n",
        "    env.close()\n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")\n",
        "    return t, epsilon, D\n",
        "\n",
        "def playGame(args):\n",
        "    model = buildmodel()\n",
        "    t, epsilon, D = trainNetwork(model,args)\n",
        "    return t, epsilon, D\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser(description = 'Description of your program')\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True)\n",
        "    #parser.add_argument('-m','--mode', help = 'Train / CTrain / Run', required=True) adicionar o argumento de número de episódios\n",
        "    #args = vars(parser.parse_args())\n",
        "    t = 0\n",
        "    epsilon = INITIAL_EPSILON\n",
        "    D = deque()\n",
        "    for i in range(EPISODES):\n",
        "        print(\"EPISODE\", i)\n",
        "        tp, epsilonp, Dp = playGame({'mode': 'Train', 't': t, 'epsilon': epsilon, 'D': D})\n",
        "        #show_video()\n",
        "        t = tp\n",
        "        epsilon = epsilonp\n",
        "        D = Dp\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPISODE 0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.01 / ACTION 1 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 5}\n",
            "Terminal False\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "Now we save model\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 101 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 102 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 103 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 104 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 105 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 106 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 107 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 108 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 109 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 110 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 111 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 112 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 113 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 114 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 115 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 116 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 117 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 118 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 119 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 120 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 121 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 122 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 123 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 124 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 125 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 126 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 127 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 128 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 129 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 130 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 131 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 132 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 133 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 134 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 135 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 136 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 137 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 138 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 139 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 140 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 141 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 142 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 143 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 144 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 145 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 146 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 147 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 148 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 149 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 150 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 151 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 152 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 153 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 154 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 155 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 156 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 157 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 158 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 159 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 160 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 161 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 162 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 163 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 164 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 165 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 166 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 167 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 168 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 169 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 170 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 171 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 172 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 173 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 174 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 175 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 176 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 177 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 178 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 179 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 180 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 181 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 182 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 183 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 184 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 185 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 186 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 187 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 188 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 189 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 190 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 191 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 192 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 193 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 194 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 195 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 196 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 197 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 198 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 199 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "Now we save model\n",
            "TIMESTEP 200 / STATE observe / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n",
            "TIMESTEP 201 / STATE explore / EPSILON 0.01 / ACTION 2 / REWARD 0 / Q_MAX  0 / Loss  0\n",
            "INFO {'ale.lives': 4}\n",
            "Terminal False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2559\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'range' has no attr named '_read_only_resource_inputs'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/auto_control_deps_utils.py\u001b[0m in \u001b[0;36mget_read_write_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mread_only_input_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAD_ONLY_RESOURCE_INPUTS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2563\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2565\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Operation 'range' has no attr named '_read_only_resource_inputs'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-775d961c68ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-775d961c68ac>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPISODE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilonp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;31m#show_video()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-775d961c68ac>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-775d961c68ac>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mstate_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mQ_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m#r_total += reward_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1704\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1923\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1924\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1925\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1926\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4485\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4486\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4487\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4488\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4489\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m     \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3712\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m       \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m       \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3133\u001b[0m     \"\"\"\n\u001b[1;32m   3134\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3135\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3136\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3098\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3100\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3101\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         if x is not None)\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    409\u001b[0m       \u001b[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m       \u001b[0;31m# and last_write_to_resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m   \u001b[0;34m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m   \u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_read_write_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m   \u001b[0msaturated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msaturated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/auto_control_deps_utils.py\u001b[0m in \u001b[0;36mget_read_write_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2391\u001b[0m       self._inputs_val = tuple(\n\u001b[1;32m   2392\u001b[0m           map(self.graph._get_tensor_by_tf_output,\n\u001b[0;32m-> 2393\u001b[0;31m               pywrap_tf_session.GetOperationInputs(self._c_op)))\n\u001b[0m\u001b[1;32m   2394\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2395\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_tensor_by_tf_output\u001b[0;34m(self, tf_output)\u001b[0m\n\u001b[1;32m   3945\u001b[0m     \"\"\"\n\u001b[1;32m   3946\u001b[0m     \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_operation_by_tf_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3947\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3949\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}